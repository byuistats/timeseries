[
  {
    "objectID": "sample_code_chunks/callouts_demos.html",
    "href": "sample_code_chunks/callouts_demos.html",
    "title": "Callouts",
    "section": "",
    "text": "Caution\n\n\n\nText goes here\n\n\n\n\n\n\n\n\nWarning\n\n\n\nText goes here\n\n\n\n\n\n\n\n\nImportant\n\n\n\nText goes here\n\n\n\n\n\n\n\n\nNote\n\n\n\nSome important information….\n\n\n\n\n\n\n\n\nTip\n\n\n\nText goes here"
  },
  {
    "objectID": "resource_blog.html",
    "href": "resource_blog.html",
    "title": "Resources",
    "section": "",
    "text": "Tidyverse Conversion\n\n\nSupplement to Introductory Time Series with R\n\n\n\n\n\n\n\n\nCommon Packages\n\n\n\n\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(tidyverse, \n               tsibble, fable,\n               feasts, tsibbledata,\n               fable.prophet,\n               patchwork,\n               lubridate, \n               rio \n)\n\nFull list of libraries used throughout the lessons. If you are having issues with a function not getting called correctly, read through the comments and see if anything matches, if not pull packages out of the pacman and individually load them in through library(). Some of the most likely error causeing libraries are tagged in the comments with a note, so start with those unless you have a better idea of what package might be causing the issue.\n\npacman::p_load(\n  # Interactive plots\n  plotly, # Interactive visualizations, loaded before tidyverse so it overwrite dplyr::select(). Note: High conflict Potential\n    \n  # Core packages\n  MASS, # MVNorm, loaded before tidyverse so it doesn't overwrite dplyr::select()\n  tidyverse, # This will also load the dependencies; dplyr, readr, stringr, tibble, tidyr, purrr, forcats, gglot2, & lubridate\n  \n  # Data manipulation\n  tsibble, # Tidyverse Temporal data\n  tsibbledata, # Sample Tsibble datasets\n\n  \n  # Statistical modeling (GLS - Chpt 6-7) \n  nlme, # loaded before feasts to avoid ACF() conflict\n  tidymodels, # for GLS, This will also load the dependencies; broom, rsample, dials, tune, infer, workflows, modeldata, workflowsets, parsnip, yardstick, & recipies. Note: High conflict Potential\n  multilevelmod, # for GLS\n  broom.mixed, # for GLS\n  \n  # TS modeling and forecasting\n  fable,# Forecasting Models for Tidy Time Series, Note: High conflict Potential\n  feasts, # collection of features, decomposition methods, statistical summaries and graphics for tsibble data, Loaded after nlme to avoid ACF() conflict\n  fable.prophet, # Converts prophet (forecasting) package for fable workflow\n  \n  # Data exploration & visualization\n  patchwork, # Multiple plot outputs\n  ggthemes, # Plot styling\n  see,  # okabeito color scheme\n  ggokabeito,  # colorblind palette\n\n  \n  # Reporting & output\n  kableExtra, # Create nice-looking tables from data.frames\n  rio, # Easy import/export of data between R and other software\n  gt, # Grammar of Tables for advanced table creation\n  quarto, # For generating reports in LaTeX format\n\n  # Additional packages\n  tidyquant # Quantitative analysis tools using tidyverse principles, This will also load the dependencies; PerformanceAnalytics, xts, & zoo. Important Masks: ‘package:base’: as.Date, as.Date.numeric. Note: High conflict Potential\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nBase-R\n\n\n\n\n\n\n\n\n\n\n\n\n\nData Visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\nLubridate\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuarto\n\n\n\n\n\n\n\n\n\n\n\n\n\nR Studio\n\n\n\n\n\n\n\n\n\n\n\n\n\nTidyverts\n\n\n\n\n\n\n\n\n\n\n\n\n\nTimeseries\n\n\n\n\n\n\n\n\n\n\n\n\n\ntidyr\n\n\n\n\n\n\n\n\n\n\n\n\n\ntsbox\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "index.html#course-outcomes",
    "href": "index.html#course-outcomes",
    "title": "Applied Time Series Analysis",
    "section": "Course Outcomes",
    "text": "Course Outcomes\n\n\n\n\n\n\nChapter 1\n\n\n\n\n\n\nLesson 1\n\n\nIntroduce the course structure and syllabus\n\n\nGet to know each other\nDescribe key concepts in time series analysis\nExplore an example time series interactively\n\n\n\n\nLesson 2\n\n\nUse technical language to describe the main features of time series data\n\n\nDefine time series analysis\nDefine time series\nDefine sampling interval\nDefine serial dependence or autocorrelation\nDefine a time series trend\nDefine seasonal variation\nDefine cycle\nDifferentiate between deterministic and stochastic trends\n\n\n\n\nPlot time series data to visualize trends, seasonal patterns, and potential outliers\n\n\nPlot a “ts” object\nPlot the estimated trend of a time series by computing the mean across one full period\n\n\n\n\nLesson 3\n\n\nDecompose time series into trends, seasonal variation, and residuals\n\n\nDefine smoothing\nCompute the centered moving average for a time series\nEstimate the trend component using moving averages\n\n\n\n\nPlot time series data to visualize trends, seasonal patterns, and potential outliers\n\n\nPlot the estimated trend of a time series using a moving average\nMake box plots to examine seasonality\nInterpret the trend and seasonal pattern observed in a time series\n\n\n\n\nLesson 4\n\n\nUse R to describe key features of time series data\n\n\nImport CSV data and convert to tsibble format\n\n\n\n\nDecompose time series into trends, seasonal variation, and residuals\n\n\nImplement additive decomposition\nExplain how to remove seasonal variation using an estimate for seasonal component of a time series\nCompute the estimators of seasonal variation for an additive model\nCalculate the random component for an additive model\nCompute a seasonally-adjusted time series based on an additive model\n\n\n\n\nLesson 5\n\n\nDecompose time series into trends, seasonal variation, and residuals\n\n\nExplain the differences between additive and multiplicative models\nImplement multiplicative decomposition\nCompute the estimators of seasonal variation for a multiplicative model\nCalculate the random component for a multiplicative model\nCompute a seasonally-adjusted time series based on a multiplicative model\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 2\n\n\n\n\n\n\nLesson 1\n\n\nCompute the key statistics used to describe the linear relationship between two variables\n\n\nCompute the sample mean\nCompute the sample variance\nCompute the sample standard deviation\nCompute the sample covariance\nCompute the sample correlation coefficient\nExplain sample covariance using a scatter plot\n\n\n\n\nInterpret the key statistics used to describe sample data\n\n\nInterpret the sample mean\nInterpret the sample variance\nInterpret the sample standard deviation\nInterpret the sample covariance\nInterpret the sample correlation coefficient\n\n\n\n\nLesson 2\n\n\nDefine key terms in time series analysis\n\n\nDefine the ensemble of a time series\nDefine the expected value (or mean function) of a time series model\nDefine the sample estimate of the population mean of a time series\nDefine the variance function of a time series model\nState the constant variance estimator for a time series model\nExplain the stationarity assumption\nExplain the stationary variance assumption\nDefine lag\nDefine autocorrelation\nDefine the second-order stationary time series\nExplain the autocovariance function in Equation (2.11)\nExplain the lag k autocorrelation function in Equation (2.12)\nDefine the autocovariance function, acvf\nDefine the sample autocorrelation function, acf\n\n\n\n\nCalculate sample estimates of autocovariance and autocorrelation functions from time series data\n\n\nDefine the sample autocovariance function, c_k\nDefine the sample autocorrelation function, r_k\n\n\n\n\nLesson 3\n\n\nExplain the theoretical implications of autocorrelation for the estimation of time series statistics\n\n\nExplain how positive autocorrelation leads to underestimation of variance in short time series\nExplain how negative autocorrelation can improve efficiency of sample mean estimate\n\n\n\n\nInterpret correlograms to identify significant lags, correlations, trends, and seasonality\n\n\nCreate a correlogram\nInterpret a correlogram\nDefine a sampling distribution\nState the sampling distribution of rk\nExplain the concept of a confidence interval\nConduct a single hypothesis test using a correlogram\nDescribe the problems associated with multiple hypothesis testing in a correlogram\nDifferentiate statistical and practical significance\nDiagnose non-stationarity using a correlogram\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 3\n\n\n\n\n\n\nLesson 1\n\n\nExplain the purpose and limitations of forecasting\n\n\nDefine lead time\nDefine forecasting\nDifferentiate causation from correlation\n\n\n\n\nExplain why there is not one correct model to describe a time series\n\n\nExplain why there can be several suitable models for a given time series\n\n\n\n\nUse cross-correlation analysis to quantify lead/lag relationships\n\n\nExplain forecasting by leading indicators\nDefine the population k-lag ccvf\nDefine the population k-lag ccf\nDefine the sample k-lag ccvf\nDefine the sample k-lag ccf\nEstimate an ccf for two time series\nInterpret whether a variable is a leading indicator using a cross-correlogram\n\n\n\n\nEvaluate the limitations of forecasting models based on past trends\n\n\nExplain how unexpected future events may invalidate forecast trends\nAvoid over-extrapolation of fitted trends beyond reasonable time horizons\n\n\n\n\nLesson 2\n\n\nImplement simple exponential smoothing to estimate local mean levels\n\n\nExplain forecasting by extrapolation\nState the assumptions of exponential smoothing\nDefine exponential weighted moving average (EWMA)\nState the exponential smoothing forecasting equation\nState the EWMA in geometric series form (in terms of x_t only Eq 3.18)\nExplain the EWMA intuitively\nDefine the one-step-ahead prediction error (1PE)\nState the SS1PE used to estimate the smoothing parameter of a EWMA\nIndicate when the EWMA smoothing parameter is optimally set as 1/n\n\n\n\n\nLesson 3\n\n\nImplement the Holt-Winter method to forecast time series\n\n\nJustify the need for the Holt-Winters method\nDescribe how to obtain initial parameters for the Holt-Winters algorithm\nExplain the Holt-Winters update equations for additive decomposition models\nExplain the purpose of the parameters \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\)\nInterpret the coefficient estimates \\(a_t\\), \\(b_t\\), and \\(s_t\\) of the Holt-Winters algorithm\nExplain the Holt-Winters forecasting equation for additive decomposition models, Equation (3.22)\n\n\n\n\nLesson 4\n\n\nImplement the Holt-Winter method to forecast time series\n\n\nCompute the Holt-Winters estimate by hand\nUse HoltWinters() to forecast additive model time series\nPlot the Holt-Winters decomposition of a time series (see Fig 3.10)\nPlot the Holt-Winters fitted values versus the original time series (see Fig 3.11)\nSuperimpose plots of the Holt-Winters predictions with the time series realizations (see Fig 3.13)\n\n\n\n\nLesson 5\n\n\nImplement the Holt-Winter method to forecast time series\n\n\nExplain the Holt-Winters method equations for multiplicative decomposition models\nExplain the purpose of the paramters \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\)\nInterpret the coefficient estimates \\(a_t\\), \\(b_t\\), and \\(s_t\\) of the Holt-Winters smoothing algorithm\nExplain the Holt-Winters forecasting equation for multiplicative decomposition models, Equation (3.23)\nUse HoltWinters() to forecast multiplicative model time series\nPlot the Holt-Winters decomposition of a TS (see Fig 3.10)\nPlot the Holt-Winters fitted values versus the original time series (see Fig 3.11)\nSuperimpose plots of the Holt-Winters predictions with the time series realizations (see Fig 3.13)\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 4\n\n\n\n\n\n\nLesson 1\n\n\nCharacterize the properties of discrete white noise\n\n\nDefine Residual error\nDefine discrete white noise (DWN)\nDefine Gaussian white noise\nSimulate Gaussian white noise with R\nPlot DWN simulation results\nState DWN second order properties\nExplain how to estimate (or fit) a DWN process\nState the assumptions needed to categorize residual error series as white noise\n\n\n\n\nCharacterize the properties of a random walk\n\n\nDefine a random walk\n\n\n\n\nSimulate realizations from basic time series models in R\n\n\nSimulate a random walk\nPlot a random walk\n\n\n\n\nLesson 2\n\n\nCharacterize the properties of a random walk\n\n\nDefine the second order properties of a random walk\nDefine the backward shift operator\nUse the backward shift operator to state a random walk as a sequence of white noise realizations\nDefine a random walk with drift\n\n\n\n\nSimulate realizations from basic time series models in R\n\n\nSimulate a random walk\nPlot a random walk\n\n\n\n\nFit time series models to data and interpret fitted parameters\n\n\nMotive the need for differencing in time series analysis\nDefine the difference operator\nExplain the relationship between the difference operator and the backward shift operator\nTest whether a series is a random walk using first differences\nExplain how to estimate a random walk with increasing slope using Holt-Winters\nEstimate the drift parameter of a random walk\n\n\n\n\nLesson 3\n\n\nCharacterize the properties of an \\(AR(p)\\) stochastic process\n\n\nDefine an \\(AR(p)\\) stochastic process\nExpress an \\(AR(p)\\) process using the backward shift operator\nState an \\(AR(p)\\) forecast (or prediction) function\nIdentify stationarity of an \\(AR(p)\\) process using the backward shift operator\nDetermine the stationarity of an \\(AR(p)\\) process using a characteristic equation\n\n\n\n\nCheck model adequacy using diagnostic plots like correlograms of residuals\n\n\nCharacterize a random walk’s second order characteristics using a correlogram\nDefine partial autocorrelations\nExplain how to use a partial correlogram to decide what model would be suitable to estimate an \\(AR(p)\\) process\nDemonstrate the use of partial correlogram via simulation\n\n\n\n\nLesson 4\n\n\nFit time series models to data and interpret fitted parameters\n\n\nFit an \\(AR(p)\\) model to simulated data\nExplain the difference between parameters of the data generating process and estimates\nCalculate confidence intervals for AR coefficient estimates\nInterpret AR coefficient estimates in the context of the source and nature of historical data\n\n\n\n\nCheck model adequacy using diagnostic plots like correlograms of residuals\n\n\nCompare AR fitted models to an underlying data generating process\nExplain the limitations of stochastic model fitting as evidence in favor or against real world arguments.\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 5\n\n\n\n\n\n\nLesson 1\n\n\nExplain the difference between stochastic and deterministic trends in time series\n\n\nDescribe deterministic trends as smooth, predictable changes over time\nDefine stochastic trends as random, unpredictable fluctuations\nExplain the different treatment of stochastic and deterministic trends when forecasting\n\n\n\n\nFit linear regression models to time series data\n\n\nDefine a linear time series model\nExplain why ordinary linear regression systematically underestimates of the standard error of parameter estimates when the error terms are autocorrelated\nApply generalized least squares GLS in R to estimate linear regression model parameters\nExplain how to estimate the autocorrelation input for the GLS algorithm\nCompare GLS and OLS standard error estimates to evaluate autocorrelation bias\nIdentify an appropriate function to model the trend in a given time series\nRepresent seasonal factors in a regression model using indicator variables\nFit a linear model for a simulated time series with linear time trend and \\(AR(p)\\) error\nUse acf and pacf to test for autocorrelation in the residuals\nEstimate a seasonal indicator model using GLS\nForecast using a fitted GLS model with seasonal indicator variables\n\n\n\n\nApply differencing to nonstationary time series\n\n\nTransform a non-stationary linear to a stationary process using differencing\nState how to remove a polynomial trend of order \\(m\\)\n\n\n\n\nSimulate time series\n\n\nSimulate a time series with a linear time trend and a \\(AR(p)\\) error\n\n\n\n\nLesson 2\n\n\nFit linear regression models to time series data\n\n\nDescribe a Fourier series\nExplain how a few terms in a Fourier series can be used to fit a seasonal component\nMotivate the use of the harmonic seasonal model\nRepresent seasonal factors using harmonic seasonal terms\n\n\n\n\nLesson 3\n\n\nFit linear regression models to time series data\n\n\nState the additive model with harmonic seasonal component\nSimulate a time series with harmonic seasonal components\nIdentify an appropriate function to model the trend in a given time series\nIdentify a parsimonious set of harmonic terms for use in a regression model\nFit the additive model with harmonic seasonal component to real-world data\nEvaluate residuals using a correlogram and partial correlogram to ensure they meet the assumptions\n\n\n\n\nApply model selection criteria\n\n\nUse AIC to aid in model selection\n\n\n\n\nLesson 4\n\n\nApply logarithmic transformations to time series\n\n\nExplain when to use a log-transformation\nEstimate a harmonic seasonal model using GLS with a log-transformed series\nExplain how to use logarithms to linearize certain non-linear trends\n\n\n\n\nApply non-linear models to time series\n\n\nExplain when to use non-linear models\nSimulate a time series with an exponential trend\nFit a time series model with an exponential trend\n\n\n\n\n\nLesson 5\n\n\nApply logarithmic transformations to time series\n\n\nApply a log-transformation to a multiplicative time series\n\n\n\n\nApply the bias correction factor for inverse transformations\n\n\nState the bias correction procedure for log-transform estimates \nExplain when to use the bias correction factor\nUse the bias correction factor for a log-transform model\nForecast using the inverse-transform and bias correction of a log-transformed model\n\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 6\n\n\n\n\n\n\nLesson 1\n\n\nCharacterize the properties of moving average (MA) models\n\n\nDefine a moving average (MA) process\nWrite an MA(q) model in terms of the backward shift operator\nState the mean and variance of an MA(q) process\nExplain the autocorrelation function of an MA(q) process\nDefine an invertible MA process\n\n\n\n\nFit time series models to data and interpret fitted parameters\n\n\nDetermine an appropriate MA(q) model to fit to a time series based on the ACF plot\nFit an MA(q) model to data in R using the arima() function\nAssess model fit by examining residual diagnostic plots\nInterpret the fitted MA coefficients\n\n\n\n\nLesson 2\n\n\nDefine autoregressive moving average (ARMA) models\n\n\nWrite the equation for an ARMA(p,q) model\nExpress an ARMA model in terms of the backward shift operators for the AR and MA components\nState facts about ARMA processes related to stationarity, invertibility, special cases, parsimony, and parameter redundancy\nUse ACF and PACF plots to determine if an AR, MA or ARMA model is appropriate for a time series\n\n\n\n\nApply an iterative time series modeling process\n\n\nFit a regression model to capture trend and seasonality\nExamine residual diagnostic plots to assess autocorrelation\nFit an ARMA model to the residuals if needed\nCheck the residuals of the ARMA model for white noise\nForecast the original series by combining the regression and ARMA model forecasts\n\n\n\n\n\n\n\n\n\n\n\n\nChapter 7\n\n\n\n\n\n\nLesson 1\n\n\nExplain the concept of non-stationarity in time series\n\n\nDefine non-stationarity and its implications for time series analysis\nIdentify non-stationary behavior in time series plots\n\n\n\n\nApply differencing to remove non-stationarity\n\n\nExplain the concept of differencing and its role in removing non-stationarity\nUse differencing to transform non-stationary time series into stationary ones\nInterpret the results of differencing on time series plots and ACF/PACF\n\n\n\n\nIdentify integrated models and ARIMA notation\n\n\nDefine integrated models and their relationship to differencing\nUnderstand the ARIMA notation and its components (p, d, q)\nRecognize the role of the ‘d’ parameter in ARIMA models\n\n\n\n\nLesson 2\n\n\nIdentify seasonal ARIMA models\n\n\nDefine seasonal ARIMA models and their notation (p, d, q)(P, D, Q)[m]\nIdentify the need for seasonal ARIMA models in time series with seasonal patterns\n\n\n\n\nApply the fitting procedure for seasonal ARIMA models\n\n\nDescribe the steps involved in fitting seasonal ARIMA models\nDetermine the appropriate order of differencing (d and D) based on ACF/PACF plots\nSelect the order of AR and MA terms (p, q, P, Q) using ACF/PACF plots and model selection criteria\n\n\n\n\nFit seasonal ARIMA models to time series data using R\n\n\nUse R to fit seasonal ARIMA models\nInterpret the output for the ARIMA models, including coefficients and model diagnostics\nForecast future values using the fitted seasonal ARIMA model\n\n\n\n\nLesson 3\n\n\nExplain the concept of volatility in time series\n\n\nDefine volatility and its importance in financial and climate time series\nIdentify patterns of volatility in time series plots\n\n\n\n\nInterpret ARCH and GARCH models\n\n\nDefine ARCH models and their extensions, including GARCH models\nUnderstand the role of ARCH/GARCH models in capturing time-varying volatility\n\n\n\n\nSimulate and fit GARCH models using R\n\n\nSimulate GARCH processes using R\nFit GARCH models to time series data using R\nInterpret the R output for GARCH models, including coefficients and model diagnostics\n\n\n\n\nApply GARCH modeling to real-world time series\n\n\nUse GARCH models to analyze volatility in financial time series\nApply GARCH models to climate time series to understand changing variability\nIncorporate GARCH models into forecasts and simulations for improved accuracy"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_2_class_activity.html",
    "href": "class_activities/chapter_2_lesson_2_class_activity.html",
    "title": "Chapter 2: Correlation",
    "section": "",
    "text": "Define covariance, autocovariance, autocorrelation\nUnderstand autocorrelation in time series\n\n\n\n\n\n\n\n\nMotivate studying autocorrelation\n\n\n\n\n\n\n\n\n\nFormally define autocovariance, autocorrelation\nExplain autocorrelation in time series context\n\n\n# Set random seed for reproducibility\nset.seed(123) \n\nn &lt;- 1000\nsigma &lt;- 0.1\na &lt;- 1\nx &lt;- rep(0,n)\ne &lt;- rnorm(n, 0, sigma)\n\nfor (i in 2:n) {\n  x[i] &lt;- a * x[i-1] + e[i]\n}\ntime &lt;- seq(1,n)\n\n# Create time variable  \ntime &lt;- 1:n  \n\n# Plot  \nplot(x, type=\"l\", \n     main=\"Simulated Stationary Time Series\",\n     xlab=\"Time\", ylab=\"Value\")\nlines(time, x)\n\n\n\n\n\n\n\n\n\n# Set random seed for reproducibility\nset.seed(123) \n\nn &lt;- 1000\nrealizations &lt;- 5\nsigma &lt;- 0.1\na &lt;- 1\nx &lt;- rep(0,n)\ne &lt;- rnorm(n, 0, sigma)\n\ndf1 &lt;- data.frame()\n\nfor (i in 2:n) {\n  x[i] &lt;- a * x[i-1] + e[i]\n}\ntime &lt;- seq(1,n)\n\n# Create time variable  \ntime &lt;- 1:n  \n\n# Plot  \nplot(x, type=\"l\", \n     main=\"Simulated Stationary Time Series\",\n     xlab=\"Time\", ylab=\"Value\")\nlines(time, x)\n\n\n\n\n\n\n\n\n\n# Define previous variables\nn &lt;- 100\nrealizations &lt;- 5\nsigma &lt;- 0.1\nm &lt;- 0.0005 * 0 # No drift\na &lt;- 1\n\n# Create data frames  \ndf1 &lt;- tibble(rep = 1:realizations)\ndf2 &lt;- tibble(x = 0, \n              i = 1:n)\n\n# Join \ndf &lt;- full_join(df1, df2, by = character()) %&gt;% \n  mutate(e = rnorm(n = nrow(.), mean = 0, sd = sigma)) %&gt;% \n  arrange(rep, i) %&gt;%\n  group_by(rep) %&gt;%\n  # mutate(x = ifelse(i == 1, 0, a*lag(x, 1) + e)) %&gt;%\n  # ungroup()\n  ungroup()\n\nWarning: Using `by = character()` to perform a cross join was deprecated in dplyr 1.1.0.\nℹ Please use `cross_join()` instead.\n\nfor (row_num in 1:nrow(df)) {\n  if (df$i[row_num] &gt; 1) {\n    df$x[row_num] &lt;- m * df$i[row_num] + a * df$x[row_num - 1] + df$e[row_num]\n  }\n}\n\nggplot(df, aes(x = i, y = x, color = factor(rep))) + \n  geom_line() +\n  scale_color_discrete() +\n  labs(title = \"Autocorrelated Data without Drift\",  \n       x = \"Time\",\n       y = \"x\") +\n  theme_bw() + \n  theme(plot.title = element_text(hjust = 0.5)) +\n  guides(color = guide_legend(title = \"Realization\"))\n\n\n\n\n\n\n\n\n\n# Define previous variables\nn &lt;- 100\nrealizations &lt;- 1\nsigma &lt;- 0.1 * 5\nm &lt;- 0.02 * 0 # No drift\na &lt;- 1 * 0\n\n# Create data frames  \ndf1 &lt;- tibble(rep = 1:realizations)\ndf2 &lt;- tibble(x = 0, \n              i = 1:n)\n\n# Join \ndf &lt;- full_join(df1, df2, by = character()) %&gt;% \n  mutate(e = rnorm(n = nrow(.), mean = 0, sd = sigma)) %&gt;% \n  arrange(rep, i) %&gt;%\n  group_by(rep) %&gt;%\n  # mutate(x = ifelse(i == 1, 0, a*lag(x, 1) + e)) %&gt;%\n  # ungroup()\n  ungroup()\n\nfor (row_num in 1:nrow(df)) {\n  if (df$i[row_num] &gt; 1) {\n    df$x[row_num] &lt;- m * df$i[row_num] + a * df$x[row_num - 1] + df$e[row_num]\n  }\n}\n\nggplot(df, aes(x = i, y = x, color = factor(rep))) + \n  geom_line() +\n  scale_color_discrete() +\n  labs(title = \"Independent Data without Drift\",  \n       x = \"Time\",\n       y = \"x\") +\n  theme_bw() + \n  theme(plot.title = element_text(hjust = 0.5)) +\n  guides(color = guide_legend(title = \"Realization\"))\n\n\n\n\n\n\n\n\n\nb &lt;- 0.5\nx &lt;- 1:100 * b + arima.sim(n=100-1,list(ar=0.1,ma=0.2,order=c(1,1,1)))\nplot(x)\n\n\n\n\n\n\n\ne &lt;- rnorm(100,0,2.5)\nb &lt;- 0.5\ny &lt;- 1:100 * b + e\n# Create time series\nlibrary(zoo) \n\n\nAttaching package: 'zoo'\n\n\nThe following object is masked from 'package:tsibble':\n\n    index\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nz &lt;- zoo(y, order.by = 1:100)\n\n# Plot\nplot(z) # , main=\"Simulated Time Series\", ylab=\"X\", xlab=\"Time\")\n\n\n\n\n\n\n\n\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\n\n# Simulate data \n# set.seed(101)\nx &lt;- 1:100 * 0.5 + arima.sim(n=100-1, list(ar=0.1, ma=0.2, order=c(1,1,1)))  \n\n# Simulate z\ne &lt;- rnorm(100,0,2.5) \ny &lt;- 1:100*0.5 + e\nz &lt;- zoo(y, order.by = 1:100)\n\n# Plot both series\nplot(x, type=\"l\", col=\"blue\", main=\"X vs Z Series\",\n     ylab=\"Values\", xlab=\"Time\") \nlines(z, col=\"red\")  \nlegend(\"topleft\", legend=c(\"X\", \"Z\"),\n       col=c(\"blue\", \"red\"), lty=1)\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tidyverse)\nlibrary(zoo) \n\n# Generate data\nset.seed(101) \nx &lt;- 1:100*0.5 + arima.sim(n=100-1, list(ar=0.1, ma=0.2, order=c(1,1,1)))\ne &lt;- rnorm(100, 0, 2.5)\ny &lt;- 1:100*0.5 + e  \nz &lt;- zoo(y, order.by = 1:100)\n\n# Create data frame  \ndf &lt;- data.frame(\n  time = 1:100,\n  x = x,\n  z = coredata(z)\n)\n\n# Plot  \nggplot(df, aes(x = time)) +\n  geom_line(aes(y = x, color = \"X\")) + \n  geom_line(aes(y = z, color = \"Z\")) +\n  labs(title=\"X vs Z Series\", \n       x=\"Time\",\n       y=\"Values\",\n       color=\"Series\") +\n  theme_bw()\n\nDon't know how to automatically pick scale for object of type &lt;ts&gt;. Defaulting\nto continuous.\n\n\n\n\n\n\n\n\n\n\n\n\nWalk through sample autocorrelation analysis\n\n\n\n\nReview objectives and key concepts\nClarify questions on autocorrelation\n\n\n\n\n\nWrite definitions of autocovariance and autocorrelation"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_2_class_activity.html#objectives",
    "href": "class_activities/chapter_2_lesson_2_class_activity.html#objectives",
    "title": "Chapter 2: Correlation",
    "section": "",
    "text": "Define covariance, autocovariance, autocorrelation\nUnderstand autocorrelation in time series"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_2_class_activity.html#introduction-5-mins",
    "href": "class_activities/chapter_2_lesson_2_class_activity.html#introduction-5-mins",
    "title": "Chapter 2: Correlation",
    "section": "",
    "text": "Motivate studying autocorrelation"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_2_class_activity.html#lecture-20-mins",
    "href": "class_activities/chapter_2_lesson_2_class_activity.html#lecture-20-mins",
    "title": "Chapter 2: Correlation",
    "section": "",
    "text": "Formally define autocovariance, autocorrelation\nExplain autocorrelation in time series context\n\n\n# Set random seed for reproducibility\nset.seed(123) \n\nn &lt;- 1000\nsigma &lt;- 0.1\na &lt;- 1\nx &lt;- rep(0,n)\ne &lt;- rnorm(n, 0, sigma)\n\nfor (i in 2:n) {\n  x[i] &lt;- a * x[i-1] + e[i]\n}\ntime &lt;- seq(1,n)\n\n# Create time variable  \ntime &lt;- 1:n  \n\n# Plot  \nplot(x, type=\"l\", \n     main=\"Simulated Stationary Time Series\",\n     xlab=\"Time\", ylab=\"Value\")\nlines(time, x)\n\n\n\n\n\n\n\n\n\n# Set random seed for reproducibility\nset.seed(123) \n\nn &lt;- 1000\nrealizations &lt;- 5\nsigma &lt;- 0.1\na &lt;- 1\nx &lt;- rep(0,n)\ne &lt;- rnorm(n, 0, sigma)\n\ndf1 &lt;- data.frame()\n\nfor (i in 2:n) {\n  x[i] &lt;- a * x[i-1] + e[i]\n}\ntime &lt;- seq(1,n)\n\n# Create time variable  \ntime &lt;- 1:n  \n\n# Plot  \nplot(x, type=\"l\", \n     main=\"Simulated Stationary Time Series\",\n     xlab=\"Time\", ylab=\"Value\")\nlines(time, x)\n\n\n\n\n\n\n\n\n\n# Define previous variables\nn &lt;- 100\nrealizations &lt;- 5\nsigma &lt;- 0.1\nm &lt;- 0.0005 * 0 # No drift\na &lt;- 1\n\n# Create data frames  \ndf1 &lt;- tibble(rep = 1:realizations)\ndf2 &lt;- tibble(x = 0, \n              i = 1:n)\n\n# Join \ndf &lt;- full_join(df1, df2, by = character()) %&gt;% \n  mutate(e = rnorm(n = nrow(.), mean = 0, sd = sigma)) %&gt;% \n  arrange(rep, i) %&gt;%\n  group_by(rep) %&gt;%\n  # mutate(x = ifelse(i == 1, 0, a*lag(x, 1) + e)) %&gt;%\n  # ungroup()\n  ungroup()\n\nWarning: Using `by = character()` to perform a cross join was deprecated in dplyr 1.1.0.\nℹ Please use `cross_join()` instead.\n\nfor (row_num in 1:nrow(df)) {\n  if (df$i[row_num] &gt; 1) {\n    df$x[row_num] &lt;- m * df$i[row_num] + a * df$x[row_num - 1] + df$e[row_num]\n  }\n}\n\nggplot(df, aes(x = i, y = x, color = factor(rep))) + \n  geom_line() +\n  scale_color_discrete() +\n  labs(title = \"Autocorrelated Data without Drift\",  \n       x = \"Time\",\n       y = \"x\") +\n  theme_bw() + \n  theme(plot.title = element_text(hjust = 0.5)) +\n  guides(color = guide_legend(title = \"Realization\"))\n\n\n\n\n\n\n\n\n\n# Define previous variables\nn &lt;- 100\nrealizations &lt;- 1\nsigma &lt;- 0.1 * 5\nm &lt;- 0.02 * 0 # No drift\na &lt;- 1 * 0\n\n# Create data frames  \ndf1 &lt;- tibble(rep = 1:realizations)\ndf2 &lt;- tibble(x = 0, \n              i = 1:n)\n\n# Join \ndf &lt;- full_join(df1, df2, by = character()) %&gt;% \n  mutate(e = rnorm(n = nrow(.), mean = 0, sd = sigma)) %&gt;% \n  arrange(rep, i) %&gt;%\n  group_by(rep) %&gt;%\n  # mutate(x = ifelse(i == 1, 0, a*lag(x, 1) + e)) %&gt;%\n  # ungroup()\n  ungroup()\n\nfor (row_num in 1:nrow(df)) {\n  if (df$i[row_num] &gt; 1) {\n    df$x[row_num] &lt;- m * df$i[row_num] + a * df$x[row_num - 1] + df$e[row_num]\n  }\n}\n\nggplot(df, aes(x = i, y = x, color = factor(rep))) + \n  geom_line() +\n  scale_color_discrete() +\n  labs(title = \"Independent Data without Drift\",  \n       x = \"Time\",\n       y = \"x\") +\n  theme_bw() + \n  theme(plot.title = element_text(hjust = 0.5)) +\n  guides(color = guide_legend(title = \"Realization\"))\n\n\n\n\n\n\n\n\n\nb &lt;- 0.5\nx &lt;- 1:100 * b + arima.sim(n=100-1,list(ar=0.1,ma=0.2,order=c(1,1,1)))\nplot(x)\n\n\n\n\n\n\n\ne &lt;- rnorm(100,0,2.5)\nb &lt;- 0.5\ny &lt;- 1:100 * b + e\n# Create time series\nlibrary(zoo) \n\n\nAttaching package: 'zoo'\n\n\nThe following object is masked from 'package:tsibble':\n\n    index\n\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\nz &lt;- zoo(y, order.by = 1:100)\n\n# Plot\nplot(z) # , main=\"Simulated Time Series\", ylab=\"X\", xlab=\"Time\")\n\n\n\n\n\n\n\n\n\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\\n\n\n# Simulate data \n# set.seed(101)\nx &lt;- 1:100 * 0.5 + arima.sim(n=100-1, list(ar=0.1, ma=0.2, order=c(1,1,1)))  \n\n# Simulate z\ne &lt;- rnorm(100,0,2.5) \ny &lt;- 1:100*0.5 + e\nz &lt;- zoo(y, order.by = 1:100)\n\n# Plot both series\nplot(x, type=\"l\", col=\"blue\", main=\"X vs Z Series\",\n     ylab=\"Values\", xlab=\"Time\") \nlines(z, col=\"red\")  \nlegend(\"topleft\", legend=c(\"X\", \"Z\"),\n       col=c(\"blue\", \"red\"), lty=1)"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_2_class_activity.html#tyson-please-build-a-shiny-app-in-which-students-can-change-the-autocorrelation-coefficient-slider-for-autocorrelation-from-0-to-1-and-watch-the-graph-update-in-real-time",
    "href": "class_activities/chapter_2_lesson_2_class_activity.html#tyson-please-build-a-shiny-app-in-which-students-can-change-the-autocorrelation-coefficient-slider-for-autocorrelation-from-0-to-1-and-watch-the-graph-update-in-real-time",
    "title": "Chapter 2: Correlation",
    "section": "",
    "text": "library(tidyverse)\nlibrary(zoo) \n\n# Generate data\nset.seed(101) \nx &lt;- 1:100*0.5 + arima.sim(n=100-1, list(ar=0.1, ma=0.2, order=c(1,1,1)))\ne &lt;- rnorm(100, 0, 2.5)\ny &lt;- 1:100*0.5 + e  \nz &lt;- zoo(y, order.by = 1:100)\n\n# Create data frame  \ndf &lt;- data.frame(\n  time = 1:100,\n  x = x,\n  z = coredata(z)\n)\n\n# Plot  \nggplot(df, aes(x = time)) +\n  geom_line(aes(y = x, color = \"X\")) + \n  geom_line(aes(y = z, color = \"Z\")) +\n  labs(title=\"X vs Z Series\", \n       x=\"Time\",\n       y=\"Values\",\n       color=\"Series\") +\n  theme_bw()\n\nDon't know how to automatically pick scale for object of type &lt;ts&gt;. Defaulting\nto continuous."
  },
  {
    "objectID": "class_activities/chapter_2_lesson_2_class_activity.html#example-analysis-15-mins",
    "href": "class_activities/chapter_2_lesson_2_class_activity.html#example-analysis-15-mins",
    "title": "Chapter 2: Correlation",
    "section": "",
    "text": "Walk through sample autocorrelation analysis"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_2_class_activity.html#recap-20-mins",
    "href": "class_activities/chapter_2_lesson_2_class_activity.html#recap-20-mins",
    "title": "Chapter 2: Correlation",
    "section": "",
    "text": "Review objectives and key concepts\nClarify questions on autocorrelation"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_2_class_activity.html#assessment",
    "href": "class_activities/chapter_2_lesson_2_class_activity.html#assessment",
    "title": "Chapter 2: Correlation",
    "section": "",
    "text": "Write definitions of autocovariance and autocorrelation"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_2_class_activity.html#objectives-1",
    "href": "class_activities/chapter_2_lesson_2_class_activity.html#objectives-1",
    "title": "Chapter 2: Correlation",
    "section": "Objectives:",
    "text": "Objectives:\n\nCompute sample autocorrelations\nMake and interpret correlogram plots"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_2_class_activity.html#agenda-1",
    "href": "class_activities/chapter_2_lesson_2_class_activity.html#agenda-1",
    "title": "Chapter 2: Correlation",
    "section": "Agenda:",
    "text": "Agenda:"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_2_class_activity.html#introduction-5-mins-1",
    "href": "class_activities/chapter_2_lesson_2_class_activity.html#introduction-5-mins-1",
    "title": "Chapter 2: Correlation",
    "section": "Introduction (5 mins)",
    "text": "Introduction (5 mins)\n\nIntroduce correlogram plots"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_2_class_activity.html#lecture-15-mins",
    "href": "class_activities/chapter_2_lesson_2_class_activity.html#lecture-15-mins",
    "title": "Chapter 2: Correlation",
    "section": "Lecture (15 mins)",
    "text": "Lecture (15 mins)\n\nDemonstrate computing acf/pacf in R\nExplain interpreting correlogram plots"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_2_class_activity.html#hands-on-exercise-25-mins",
    "href": "class_activities/chapter_2_lesson_2_class_activity.html#hands-on-exercise-25-mins",
    "title": "Chapter 2: Correlation",
    "section": "Hands-on Exercise (25 mins)",
    "text": "Hands-on Exercise (25 mins)\n\nCompute acf/pacf on sample data\nMake correlogram plots in R\nIdentify significant lags/correlations"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_2_class_activity.html#recap-15-mins",
    "href": "class_activities/chapter_2_lesson_2_class_activity.html#recap-15-mins",
    "title": "Chapter 2: Correlation",
    "section": "Recap (15 mins)",
    "text": "Recap (15 mins)\n\nReview objectives and key points\nClarify questions on correlograms"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_2_class_activity.html#assessment-1",
    "href": "class_activities/chapter_2_lesson_2_class_activity.html#assessment-1",
    "title": "Chapter 2: Correlation",
    "section": "Assessment:",
    "text": "Assessment:\n\nCompute and interpret acf/pacf on data\n\n\n\n\n\n\n\nNote\n\n\n\nSome important information….\n\n\n\n\n\n\n\n\nWarning\n\n\n\nSome important information….\n\n\n\n\n\n\n\n\nImportant\n\n\n\nSome important information….\n\n\n\n\n\n\n\n\nTip\n\n\n\nSome important information….\n\n\n\n\n\n\n\n\nCaution\n\n\n\nSome important information…."
  },
  {
    "objectID": "class_activities/chapter_2_lesson_1_class--OLD-MAYBE.html",
    "href": "class_activities/chapter_2_lesson_1_class--OLD-MAYBE.html",
    "title": "Chapter 2: Correlation",
    "section": "",
    "text": "Important\n\n\n\nPlease change the colors to be the okabeito scheme….\npalette(“okabeito”)\nokabeito_colors_list &lt;- c( orange = “#E69F00”, light blue = “#56B4E9”, green = “#009E73”, yellow = “#F0E442”, blue = “#0072B2”, red = “#D55E00”, purple = “#CC79A7”, grey = “#999999”, black = “#000000”, sky blue = “#56B4E9”, bluish green = “#009E73”, vermillion = “#D55E00”, reddish purple = “#CC79A7”, dark yellow = “#F5C710”, amber = “#F5C710” )"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_1_class--OLD-MAYBE.html#objectives",
    "href": "class_activities/chapter_2_lesson_1_class--OLD-MAYBE.html#objectives",
    "title": "Chapter 2: Correlation",
    "section": "Objectives:",
    "text": "Objectives:\n\nDefine covariance, correlation\nCompute covariance and correlation"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_1_class--OLD-MAYBE.html#agenda",
    "href": "class_activities/chapter_2_lesson_1_class--OLD-MAYBE.html#agenda",
    "title": "Chapter 2: Correlation",
    "section": "Agenda:",
    "text": "Agenda:\n\nIntroduction (5 mins)\nMotivate covariance and correlation"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_1_class--OLD-MAYBE.html#class-activity-variance-and-standard-deviation-15-mins",
    "href": "class_activities/chapter_2_lesson_1_class--OLD-MAYBE.html#class-activity-variance-and-standard-deviation-15-mins",
    "title": "Chapter 2: Correlation",
    "section": "Class Activity: Variance and Standard Deviation (15 mins)",
    "text": "Class Activity: Variance and Standard Deviation (15 mins)\nWe will explore the variance and standard deviation in this section.\n\nWhat do the standard deviation and the variance measure?\n\nThe following code simulates observations of a random variable. We will use these data to explore the variance and standard deviation.\n\n# Set random seed\nset.seed(2412)\n\n# Specify means and standard deviation\nn &lt;- 5        # number of points\nmu &lt;- 10      # mean\nsigma &lt;- 3    # standard deviation\n\n# Simulate normal data\nsim_data &lt;- data.frame(x = round(rnorm(n, mu, sigma), 1)) %&gt;% \n  arrange(x)\n\nThe data simulated by this process are:\n\n6.9, 7.7, 8.1, 10.8, 13.5\n\n\nFind the mean of these numbers. \nWhat are some ways to interpret the mean?\n\nThe variance and standard deviation are individual numbers that summarize how far the data are from the mean. We first compute the deviations from the mean, \\(x - \\bar x\\). This is the directed distance from the mean to each data point.\n\n\nWarning in geom_text(aes(x = upper, y = -1, label = \"x\"), size = 4, hjust = -1, : All aesthetics have length 1, but the data has 5 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = mean_x, xend = mean_x, y = -0.75, yend = n + : All aesthetics have length 1, but the data has 5 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_text(aes(x = mean_x, y = -3), label = paste0(\"Mean = \", : All aesthetics have length 1, but the data has 5 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\nWe can summarize this information in a table:\n\n\n\n\n\nx\nx - mean(x)\nWorkspace for your use\n\n\n\n\n6.9\n-2.5\n\n\n\n7.7\n-1.7\n\n\n\n8.1\n-1.3\n\n\n\n10.8\n1.4\n\n\n\n13.5\n4.1\n\n\n\n\n\n\n\n\nHow can we obtain one number that summarizes how spread out the data are from the mean? We may try averaging the deviations from the mean.\n\nWhat is the average deviation from the mean?\nWill we get the same value with other data sets, or is this just a coincidence?\nWhat could you do to prevent this from happening?\nApply your idea. Compute the resulting value that summarizes the spread. What do you get?\nWhat is the relationship between the sample variance and the sample standard deviation?\nUse the table above to verify that the sample variance is 7.4.\nShow that the sample standard deviation is 2.7203."
  },
  {
    "objectID": "class_activities/chapter_2_lesson_1_class--OLD-MAYBE.html#class-activity-covariance-and-correlation-20-mins",
    "href": "class_activities/chapter_2_lesson_1_class--OLD-MAYBE.html#class-activity-covariance-and-correlation-20-mins",
    "title": "Chapter 2: Correlation",
    "section": "Class Activity: Covariance and Correlation (20 mins)",
    "text": "Class Activity: Covariance and Correlation (20 mins)\n\n\n\n\n\n\nImportant\n\n\n\nCould we make this into a shiny app? Students could use a slider or something to choose the values of the following:\n\n# n &lt;- 50             # number of points\n# mu &lt;- c(1.25, 2.5)  # mean vector (mu_x, mu_y)\n# sigma_x &lt;- 1        # standard deviation x\n# sigma_y &lt;- 3        # standard deviation y\n# rho &lt;- 0.8          # correlation coefficient\n\nThe table and scatterplot would update to match the values input into the app.\n\n\nThe following code simulates \\((x,y)\\)-pairs of random variables.\n\n# Unset random seed\nset.seed(Sys.time())\n\n# Specify means and correlation coefficient\nn &lt;- 50             # number of points\nmu &lt;- c(1.25, 2.5)  # mean vector (mu_x, mu_y)\nsigma_x &lt;- 1        # standard deviation x\nsigma_y &lt;- 3        # standard deviation y\nrho &lt;- 0.8          # correlation coefficient\n\n# Define variance-covariance matrix\nsigma &lt;- matrix(\n  c(sigma_x^2,\n    rho*sigma_x*sigma_y,\n    rho*sigma_x*sigma_y,\n    sigma_y^2),\n  nrow = 2)\n\n# Simulate bivariate normal data\nmvn_data &lt;- MASS::mvrnorm(n, mu, sigma) %&gt;% \n  data.frame() %&gt;% \n  rename(x = X1, y = X2)\n\nThe following table illustrates some of the simulated values. The mean of the \\(x\\) values is \\(\\bar x = 0.9\\). The mean of the \\(y\\) values is \\(\\bar y =1.849\\). We will soon use the values \\((x-\\bar x)\\), \\((x-\\bar x)^2\\), \\((y-\\bar y)\\), \\((y-\\bar y)^2\\), and \\((x-\\bar x)(y-\\bar y)\\). For convenience, they are included in the table below.\n\n\n\n\n\ni\nx\ny\nx-mean(x)\n(x-mean(x))^2\ny-mean(y)\n(y-mean(y))^2\n(x-mean(x))(y-mean(y))\n\n\n\n\n1\n-0.613\n-1.992\n-1.513\n2.29\n-3.84\n14.748\n5.811\n\n\n2\n3.104\n4.143\n2.204\n4.857\n2.295\n5.266\n5.058\n\n\n3\n1.285\n4.175\n0.384\n0.148\n2.326\n5.41\n0.894\n\n\n4\n1.498\n2.154\n0.598\n0.358\n0.306\n0.094\n0.183\n\n\n5\n1.961\n2.55\n1.061\n1.125\n0.702\n0.492\n0.744\n\n\n6\n2.181\n4.962\n1.281\n1.64\n3.113\n9.691\n3.987\n\n\n7\n0.355\n-1.247\n-0.545\n0.297\n-3.096\n9.585\n1.687\n\n\n8\n3.662\n4.036\n2.762\n7.63\n2.187\n4.784\n6.042\n\n\n9\n0.157\n-0.357\n-0.743\n0.552\n-2.205\n4.862\n1.638\n\n\n10\n-1.28\n-0.368\n-2.18\n4.751\n-2.217\n4.914\n4.832\n\n\n11\n2.302\n6.637\n1.402\n1.967\n4.788\n22.929\n6.715\n\n\n12\n0.468\n-1.192\n-0.432\n0.187\n-3.04\n9.243\n1.313\n\n\n13\n0.105\n-2.041\n-0.795\n0.632\n-3.89\n15.132\n3.092\n\n\n14\n1\n5.104\n0.1\n0.01\n3.255\n10.597\n0.325\n\n\n15\n1.493\n1.776\n0.593\n0.352\n-0.072\n0.005\n-0.043\n\n\n:\n:\n:\n:\n:\n:\n:\n:\n\n\n:\n:\n:\n:\n:\n:\n:\n:\n\n\n48\n2.062\n1.862\n1.162\n1.349\n0.013\n0\n0.016\n\n\n49\n1.322\n1.819\n0.422\n0.178\n-0.03\n0.001\n-0.013\n\n\n50\n0.946\n5.589\n0.046\n0.002\n3.74\n13.991\n0.171\n\n\nsum\n45.004\n92.429\n0\n71.472\n0\n551.334\n158.356\n\n\n\n\n\n\n\nThe simulated values are plotted below, with vertical lines drawn at \\(x = \\bar x\\) and \\(y = \\bar y\\). The first simulated point \\((i=1)\\) is circled.\n\n\n\n\n\n\n\n\n\nIf the quantity \\((x-\\bar x)(y-\\bar y)\\) is greater than zero, the points are colored blue. Otherwise, they are colored orange.\n\nWhat color are the points if \\((x-\\bar x)\\) and \\((y-\\bar y)\\) have the same sign?\nWhat color are the points if \\((x-\\bar x)\\) and \\((y-\\bar y)\\) have different signs?\n\nTo compute the sample covariance, we divide the sum of the \\((x - \\bar x)(y - \\bar y)\\) values by \\(n-1\\):\n\\[\nCov(x,y)\n=\n\\frac{\\sum\\limits_{i=1}^n (x - \\bar x)(y - \\bar y)}{n-1}\n=\n\\frac{158.356}{50 - 1}\n=\n3.232\n\\]\nYou can think of this as an “average” of the \\((x - \\bar x)(y - \\bar y)\\) values. The only difference is that we divide by \\(n-1\\) instead of \\(n\\).\n\nIf there are more blue points than orange points, what should the sign of the sample covariance be? Why?\nWhat does the sample covariance tell us?\n\nThe sample covariance is related to the sample standard deviation of \\(x\\) and \\(y\\) and the sample correlation coefficient between \\(x\\) and \\(y\\).\nThe sample standard deviations are:\n\\[\n\\begin{align*}\ns_x &= \\sqrt{ \\frac{\\sum\\limits_{i=1}^n (x - \\bar x)^2}{n-1} }\n=\n\\sqrt{\n\\frac{\n71.472\n}{\n50-1\n}\n}\n=\n1.208\n\\\\\ns_y &= \\sqrt{ \\frac{\\sum\\limits_{i=1}^n (y - \\bar y)^2}{n-1} }\n=\n\\sqrt{\n\\frac{\n551.334\n}{\n50-1\n}\n}\n=\n3.354\n\\end{align*}\n\\]\nThe sample correlation coefficient is: \\[\nr = \\frac{\\sum\\limits_{i=1}^n (x - \\bar x)(y - \\bar y)}{\\sqrt{\\sum\\limits_{i=1}^n (x - \\bar x)^2} \\sqrt{\\sum\\limits_{i=1}^n (y - \\bar y)^2}}\n=\n\\frac{\n158.356\n}{\n\\sqrt{ 71.472}\n\\sqrt{ 551.334}\n}\n=\n0.798\n\\]\n\nWhat do you get if you multiply the equations for \\(r\\), \\(s_x\\), and \\(s_y\\) together? \\[\n\\begin{align*}\n  r \\cdot s_x \\cdot s_y\n  &=\n  \\frac{\\sum\\limits_{i=1}^n (x - \\bar x)(y - \\bar y)}{\\sqrt{\\sum\\limits_{i=1}^n (x - \\bar x)^2} \\sqrt{\\sum\\limits_{i=1}^n (y - \\bar y)^2}}\n  \\cdot\n  \\sqrt{ \\frac{\\sum\\limits_{i=1}^n (x - \\bar x)^2}{n-1} }\n  \\cdot\n  \\sqrt{ \\frac{\\sum\\limits_{i=1}^n (y - \\bar y)^2}{n-1} }\n  \\\\\n  ~&~\\\\\n  &=\\\\\n  ~&~\\\\\n\\end{align*}\n\\]\nUse the numerical values above to confirm your result. Any discrepancy is due to roundoff error."
  },
  {
    "objectID": "class_activities/chapter_2_lesson_1_class--OLD-MAYBE.html#team-activity-computational-practice-15-mins",
    "href": "class_activities/chapter_2_lesson_1_class--OLD-MAYBE.html#team-activity-computational-practice-15-mins",
    "title": "Chapter 2: Correlation",
    "section": "Team Activity: Computational Practice (15 mins)",
    "text": "Team Activity: Computational Practice (15 mins)\nWith your assigned partner, compute the following values for the \\(n=6\\) values given in the table below:\n\n\\(\\bar x =\\)\n\\(\\bar y =\\)\n\\(s_x =\\)\n\\(s_y =\\)\n\\(r =\\)\n\\(Cov(x,y) =\\)\n\n\n\n\n\n\ni\nx\ny\nx-mean(x)\n(x-mean(x))^2\ny-mean(y)\n(y-mean(y))^2\n(x-mean(x))(y-mean(y))\n\n\n\n\n1\n-2.1\n2.8\n-1.9\n3.61\n1\n1\n-1.9\n\n\n2\n-0.2\n2.2\n\n\n\n\n\n\n\n3\n0.8\n0.9\n\n\n\n\n\n\n\n4\n0.4\n2\n\n\n\n\n\n\n\n5\n2.3\n-1\n\n\n\n\n\n\n\n6\n-2.4\n3.9\n\n\n\n\n\n\n\nsum\n-1.2\n10.8"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_1_class--OLD-MAYBE.html#recap-5-min",
    "href": "class_activities/chapter_2_lesson_1_class--OLD-MAYBE.html#recap-5-min",
    "title": "Chapter 2: Correlation",
    "section": "Recap (5 min)",
    "text": "Recap (5 min)\nWorking with your partner, prepare to explain the following concepts to the class:\n\nVariance\nStandard deviation\nCorrelation\nCovariance"
  },
  {
    "objectID": "chapter_7_lesson_3.html",
    "href": "chapter_7_lesson_3.html",
    "title": "ARCH and GARCH Models",
    "section": "",
    "text": "Explain the concept of volatility in time series\n\n\nDefine volatility and its importance in financial and climate time series\nIdentify patterns of volatility in time series plots\n\n\n\n\nInterpret ARCH and GARCH models\n\n\nDefine ARCH models and their extensions, including GARCH models\nUnderstand the role of ARCH/GARCH models in capturing time-varying volatility\n\n\n\n\nSimulate and fit GARCH models using R\n\n\nSimulate GARCH processes using R\nFit GARCH models to time series data using R\nInterpret the R output for GARCH models, including coefficients and model diagnostics\n\n\n\n\nApply GARCH modeling to real-world time series\n\n\nUse GARCH models to analyze volatility in financial time series\nApply GARCH models to climate time series to understand changing variability\nIncorporate GARCH models into forecasts and simulations for improved accuracy",
    "crumbs": [
      "Lesson 2",
      "ARCH and GARCH Models"
    ]
  },
  {
    "objectID": "chapter_7_lesson_3.html#learning-outcomes",
    "href": "chapter_7_lesson_3.html#learning-outcomes",
    "title": "ARCH and GARCH Models",
    "section": "",
    "text": "Explain the concept of volatility in time series\n\n\nDefine volatility and its importance in financial and climate time series\nIdentify patterns of volatility in time series plots\n\n\n\n\nInterpret ARCH and GARCH models\n\n\nDefine ARCH models and their extensions, including GARCH models\nUnderstand the role of ARCH/GARCH models in capturing time-varying volatility\n\n\n\n\nSimulate and fit GARCH models using R\n\n\nSimulate GARCH processes using R\nFit GARCH models to time series data using R\nInterpret the R output for GARCH models, including coefficients and model diagnostics\n\n\n\n\nApply GARCH modeling to real-world time series\n\n\nUse GARCH models to analyze volatility in financial time series\nApply GARCH models to climate time series to understand changing variability\nIncorporate GARCH models into forecasts and simulations for improved accuracy",
    "crumbs": [
      "Lesson 2",
      "ARCH and GARCH Models"
    ]
  },
  {
    "objectID": "chapter_7_lesson_3.html#preparation",
    "href": "chapter_7_lesson_3.html#preparation",
    "title": "ARCH and GARCH Models",
    "section": "Preparation",
    "text": "Preparation\n\nRead Sections 7.4-7.5",
    "crumbs": [
      "Lesson 2",
      "ARCH and GARCH Models"
    ]
  },
  {
    "objectID": "chapter_7_lesson_3.html#learning-journal-exchange-10-min",
    "href": "chapter_7_lesson_3.html#learning-journal-exchange-10-min",
    "title": "ARCH and GARCH Models",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading another student’s?\nWhat would you recommend the other student add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 2",
      "ARCH and GARCH Models"
    ]
  },
  {
    "objectID": "chapter_7_lesson_3.html#class-activity-arch-models-xxx-min",
    "href": "chapter_7_lesson_3.html#class-activity-arch-models-xxx-min",
    "title": "ARCH and GARCH Models",
    "section": "Class Activity: ARCH Models (xxx min)",
    "text": "Class Activity: ARCH Models (xxx min)\n\nHeteroskedasticity\n\n\n\n\n\n\nDefinition of Heteroskedasticity and Conditional Heteroskedasticity\n\n\n\n\n\n\n\n\nDefinition of ARCH Models\n\n\n\n\n\n\nDefinition of ARCH Models\n\n\n\n\n\n\n\n\nDefinition of GARCH Models\n\n\n\n\n\n\nDefinition of GARCH Models\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nTheInitialPromptGoesHere\n\nQuestion1\nQuestion2",
    "crumbs": [
      "Lesson 2",
      "ARCH and GARCH Models"
    ]
  },
  {
    "objectID": "chapter_7_lesson_3.html#class-activity-fitting-a-arch-and-garch-models-xxx-min",
    "href": "chapter_7_lesson_3.html#class-activity-fitting-a-arch-and-garch-models-xxx-min",
    "title": "ARCH and GARCH Models",
    "section": "Class Activity: Fitting a ARCH and GARCH Models (xxx min)",
    "text": "Class Activity: Fitting a ARCH and GARCH Models (xxx min)\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestion1\nQuestion2",
    "crumbs": [
      "Lesson 2",
      "ARCH and GARCH Models"
    ]
  },
  {
    "objectID": "chapter_7_lesson_3.html#small-group-activity-fitting-a-arch-and-garch-models-xxx-min",
    "href": "chapter_7_lesson_3.html#small-group-activity-fitting-a-arch-and-garch-models-xxx-min",
    "title": "ARCH and GARCH Models",
    "section": "Small-Group Activity: Fitting a ARCH and GARCH Models (xxx min)",
    "text": "Small-Group Activity: Fitting a ARCH and GARCH Models (xxx min)\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestion1\nQuestion2",
    "crumbs": [
      "Lesson 2",
      "ARCH and GARCH Models"
    ]
  },
  {
    "objectID": "chapter_7_lesson_3.html#homework-preview-5-min",
    "href": "chapter_7_lesson_3.html#homework-preview-5-min",
    "title": "ARCH and GARCH Models",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_7_3.qmd \n\n\nClass Activity\n\n\n\nClass Activity\n\n\n\nClass Activity",
    "crumbs": [
      "Lesson 2",
      "ARCH and GARCH Models"
    ]
  },
  {
    "objectID": "chapter_7_lesson_1.html",
    "href": "chapter_7_lesson_1.html",
    "title": "Introduction to Non-stationary Models and Differencing",
    "section": "",
    "text": "Explain the concept of non-stationarity in time series\n\n\nDefine non-stationarity and its implications for time series analysis\nIdentify non-stationary behavior in time series plots\n\n\n\n\nApply differencing to remove non-stationarity\n\n\nExplain the concept of differencing and its role in removing non-stationarity\nUse differencing to transform non-stationary time series into stationary ones\nInterpret the results of differencing on time series plots and ACF/PACF\n\n\n\n\nIdentify integrated models and ARIMA notation\n\n\nDefine integrated models and their relationship to differencing\nUnderstand the ARIMA notation and its components (p, d, q)\nRecognize the role of the ‘d’ parameter in ARIMA models",
    "crumbs": [
      "Lesson 1",
      "Introduction to Non-stationary Models and Differencing"
    ]
  },
  {
    "objectID": "chapter_7_lesson_1.html#learning-outcomes",
    "href": "chapter_7_lesson_1.html#learning-outcomes",
    "title": "Introduction to Non-stationary Models and Differencing",
    "section": "",
    "text": "Explain the concept of non-stationarity in time series\n\n\nDefine non-stationarity and its implications for time series analysis\nIdentify non-stationary behavior in time series plots\n\n\n\n\nApply differencing to remove non-stationarity\n\n\nExplain the concept of differencing and its role in removing non-stationarity\nUse differencing to transform non-stationary time series into stationary ones\nInterpret the results of differencing on time series plots and ACF/PACF\n\n\n\n\nIdentify integrated models and ARIMA notation\n\n\nDefine integrated models and their relationship to differencing\nUnderstand the ARIMA notation and its components (p, d, q)\nRecognize the role of the ‘d’ parameter in ARIMA models",
    "crumbs": [
      "Lesson 1",
      "Introduction to Non-stationary Models and Differencing"
    ]
  },
  {
    "objectID": "chapter_7_lesson_1.html#preparation",
    "href": "chapter_7_lesson_1.html#preparation",
    "title": "Introduction to Non-stationary Models and Differencing",
    "section": "Preparation",
    "text": "Preparation\n\nRead Sections 7.1-7.2\nRead Prof. Frenzel’s Blog Post",
    "crumbs": [
      "Lesson 1",
      "Introduction to Non-stationary Models and Differencing"
    ]
  },
  {
    "objectID": "chapter_7_lesson_1.html#learning-journal-exchange-10-min",
    "href": "chapter_7_lesson_1.html#learning-journal-exchange-10-min",
    "title": "Introduction to Non-stationary Models and Differencing",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading another student’s?\nWhat would you recommend the other student add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 1",
      "Introduction to Non-stationary Models and Differencing"
    ]
  },
  {
    "objectID": "chapter_7_lesson_1.html#class-activity-non-seasonal-arima-models-15-min",
    "href": "chapter_7_lesson_1.html#class-activity-non-seasonal-arima-models-15-min",
    "title": "Introduction to Non-stationary Models and Differencing",
    "section": "Class Activity: Non-seasonal ARIMA Models (15 min)",
    "text": "Class Activity: Non-seasonal ARIMA Models (15 min)\n\nEffect of Differencing\nIn Chapter 4 Lesson 2, we found that if we compute the first difference of the price of McDonald’s stock from July 2020 through December 2023, the differences can be modeled as white noise. Sometimes differencing can remove trends.\nConsider the case of a random walk and a linear trend with white noise errors\n\nRandom Walk\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nConsider the random walk\n\\[\n  x_t = x_{t-1} + w_t\n\\]\nwhere \\(\\{w_t\\}\\) is a white noise process.\n\nWhat is the model for the first differences of this time series?\n\n\n\n\n\n\nLinear Trend with White Noise Errors\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nConsider a time series with a linear trend and white noise errors.\n\\[\n  x_t = a + bt + w_t\n\\]\nwhere \\(\\{w_t\\}\\) is a white noise process.\n\nWhat is the model for the first differences of this time series? \nWhat is the model obtained by subtracting \\(a+bt\\) from this series? \nWhat are some potential concerns of using differencing to eliminate a deterministic trend? \n\n\n\n\n\n\nFitting an ARIMA Model when the Difference Model has a Non-Zero Mean\n(See the last sentence in the first paragraph on page 138.)\n\n\nDifferencing a Time Series or the Logarithm of a Time Series\nIf the difference of a time series demonstrates an increasing trend, taking the logarithm before differencing can eliminate the increasing variation in the differences. As an example, consider the Australian electricity production series given in the book.\n\n\nShow the code\npacman::p_load(\"tsibble\", \"fable\", \"feasts\",\n    \"tsibbledata\", \"fable.prophet\",\n    \"tidyverse\", \"patchwork\")\ncbe &lt;- read_table(\"https://byuistats.github.io/timeseries/data/cbe.dat\") |&gt;\n  select(elec) |&gt;\n  mutate(\n    date = seq(\n      ymd(\"1958-01-01\"),\n      by = \"1 months\",\n      length.out = n()),\n      year_month = tsibble::yearmonth(date)) |&gt;\n  as_tsibble(index = year_month)\n\ncbe |&gt;\n  mutate(\n    `Diff series` = elec - lag(elec),\n    `Diff log-series` = log(elec) - lag(log(elec))) |&gt;\n  pivot_longer(\n    cols = all_of(c(\"elec\", \"Diff series\", \"Diff log-series\"))) |&gt;\n  mutate(name = factor(name, levels =c(\"elec\",\"Diff series\", \"Diff log-series\"))) |&gt;\n  ggplot(aes(x = date, y = value)) +\n  geom_line() +\n  facet_wrap(~name, ncol = 1, scales = \"free\", strip.position = \"left\") +\n  labs(x = \"Time\", y = \"\") +\n  scale_x_date(breaks = \"5 years\", date_labels = \"%Y\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\n\nFigure 1: Plot of Australian electricity production, first differences, and first differences of the logarithm of the series\n\n\n\n\n\n\n\nIntegrated Time Series of Order d\n\n\n\n\n\n\nDefinition of an Integrated Series of Order \\(d\\), \\(I(d)\\)\n\n\n\nWe say that a time series is integrated of order d if the \\(d^{th}\\) difference of \\(\\{x_t\\}\\) is a white noise process \\(\\{w_t\\}\\). Expressed differently, we write this as \\({\\nabla^d x_t = w_t}\\). We denote an integrated time series of order \\(d\\) as \\(I(d)\\).\n\n\nRecall that \\(\\nabla^d \\equiv \\left( 1 - \\mathbf{B} \\right)^d\\). So, either of the following can be used to indicate an integrated time series of order \\(d\\):\n\\[\\begin{align*}\n  \\nabla^d x_t &= w_t \\\\\n  ~\\\\\n  \\left( 1 - \\mathbf{B} \\right)^d x_t &= w_t\n\\end{align*}\\]\n\nSpecial Case: \\(I(1)\\)\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat model is given by the special case \\(I(1)\\)? \n\n\n\n\n\n\nSecond-Order Differencing and Lagged Differences\nA linear trend can be removed by first-order differencing. A curved trend can sometimes be eliminated by second order differencing.\nIn some cases, a lagged difference is more appropriate. For example, if you have monthly data and need to remove additive seasonal effects, you may want to take a difference with a lag of 12. This subtracts sequential January observations from each other. This models the year-over-year growth.\nNotice that taking a lag 12 difference\n\\[  \n  \\left( 1 - \\mathbf{B}^{12} \\right) x_t  = x_t - x_{t-12}\n\\]\nis very different from taking the twelfth differences\n\\[\\begin{align*}\n  \\nabla^{12} x_t\n    &= \\left( 1 - \\mathbf{B} \\right)^{12} x_t \\\\\n    &= x_t - 12 x_{t-1} + 66 x_{t-2} - 220 x_{t-3} + 495 x_{t-4} - 792 x_{t-5} + 924 x_{t-6} \\\\\n    & ~~~~~~~~~~~~~~~~~~~ - 792 x_{t-7} + 495 x_{t-8} - 220 x_{t-9} + 66 x_{t-10} - 12 x_{t-11} + x_{t-12}\n\\end{align*}\\]\n\n\nARIMA Process\n\nARIMA\n\n\n\n\n\n\nDefinition of an ARIMA Process\n\n\n\nA time series is said to follow an \\(ARIMA(p,d,q)\\) process if the \\(d^{th}\\) differences of the time series follow an \\(ARMA(p,q)\\) process.\n\n\nSuppose we let \\(y_t = \\left( 1 - \\mathbf{B} \\right)^d x_t\\). The series \\(\\{y_t\\}\\) follows an \\(ARMA(p,q)\\) process if \\(\\theta_p \\left(\\mathbf{B} \\right) y_t = \\phi_q \\left(\\mathbf{B} \\right)w_t\\).\nSubstituting, we find that \\(\\{x_t\\}\\) follows an \\(ARIMA(p,d,q)\\) process if\n\\[\n    \\theta_p \\left(\\mathbf{B} \\right) \\left( 1 - \\mathbf{B} \\right)^d x_t = \\phi_q \\left(\\mathbf{B} \\right) w_t\n\\]\nwhere \\(\\theta_p \\left(\\mathbf{B} \\right)\\) and \\(\\phi_q \\left(\\mathbf{B} \\right)\\) are polynomials of orders \\(p\\) and \\(q\\), respectively.\n\n\nSpecial Case: \\(IMA(d,q)\\) Process\n\n\n\n\n\n\nDefinition of an IMA Process\n\n\n\nA time series \\(\\{x_t\\}\\) follows an \\(IMA(d,q)\\) process if it can be expressed as:\n\\[\n    \\left( 1 - \\mathbf{B} \\right)^d x_t = \\phi_q \\left(\\mathbf{B} \\right) w_t\n\\]\nNote that \\(IMA(d,q) \\equiv ARIMA(0,d,q)\\).\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nSolve for \\(x_t\\) in an \\(IMA(1,1)\\) process. \n\n\n\n\n\nSpecial Case: \\(ARI(p,d)\\) Process\n\n\n\n\n\n\nDefinition of an ARI Process\n\n\n\nA time series \\(\\{x_t\\}\\) follows an \\(ARI(p,d)\\) process if it can be expressed as:\n\\[\n    \\theta_p \\left(\\mathbf{B} \\right) \\left( 1 - \\mathbf{B} \\right)^d x_t = w_t\n\\]\nNote that \\(ARI(p,d) \\equiv ARIMA(p,d,0)\\).\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nSolve for \\(x_t\\) in an \\(ARI(1,1)\\) process. \n\n\n\n\n\n\nSimulating an ARIMA Process\nWe can simulate data from the ARIMA process\n\\[\n  x_t = 0.5 x_{t-1} + x_{t-1} - 0.5 x_{t-2} + w_t + 0.3 w_{t-1}\n\\]\nusing the following R code.\n\nset.seed(1)\nn &lt;- 10000\nx &lt;- rnorm(n)\nw &lt;- rnorm(n)\nfor (i in 3:n) {\n  x[i] &lt;- 0.5 * x[i - 1] + x[i - 1] - 0.5 * x[i - 2] + w[i] + 0.3 * w[i - 1]\n}\narima(x, order = c(1, 1, 1))\n\n\nCall:\narima(x = x, order = c(1, 1, 1))\n\nCoefficients:\n         ar1     ma1\n      0.4811  0.3170\ns.e.  0.0125  0.0134\n\nsigma^2 estimated as 0.9814:  log likelihood = -14094.25,  aic = 28194.49\n\n\nThis is an ARIMA(1,1,1) process with parameters \\(\\alpha = 0.5\\) and \\(\\beta = 0.3\\).\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nModify the code above to simulate from an \\(ARIMA(2,1,2)\\) process with parameters \\(\\alpha_1 = 0.5\\), \\(\\alpha_2 = 0.2\\), \\(\\beta_1 = 0.4\\), and \\(\\beta_2 = 0.1\\).",
    "crumbs": [
      "Lesson 1",
      "Introduction to Non-stationary Models and Differencing"
    ]
  },
  {
    "objectID": "chapter_7_lesson_1.html#class-activity-fitting-an-arima-process---exchange-rates-10-min",
    "href": "chapter_7_lesson_1.html#class-activity-fitting-an-arima-process---exchange-rates-10-min",
    "title": "Introduction to Non-stationary Models and Differencing",
    "section": "Class Activity: Fitting an ARIMA Process - Exchange Rates (10 min)",
    "text": "Class Activity: Fitting an ARIMA Process - Exchange Rates (10 min)\nThe data file exchange_rates.parquet gives the exchange rates for foreign currencies. The daily-observed values in the time series are the amount in the foreign currency equivalent to one U. S. dollar. We will consider the exchange rates to convert one dollar into Euros.\n\n\nShow the code\nexchange_ts &lt;- rio::import(\"data/exchange_rates.parquet\") |&gt;\n  filter(currency == \"USD.EUR\") |&gt;\n  as_tsibble(index = date) |&gt;\n  na.omit()\n\nexchange_ts |&gt;\n  display_partial_table(6,3)\n\n\n\n\nTable 1: Select values of the time series representing the exchange rate to convert US$1 into Euros\n\n\n\n\n\n\ndate\ncurrency\nrate\n\n\n\n\n2023-08-24\nUSD.EUR\n0.922\n\n\n2023-08-25\nUSD.EUR\n0.926\n\n\n2023-08-26\nUSD.EUR\n0.926\n\n\n2023-08-27\nUSD.EUR\n0.926\n\n\n2023-08-28\nUSD.EUR\n0.925\n\n\n2023-08-29\nUSD.EUR\n0.923\n\n\n⋮\n⋮\n⋮\n\n\n2024-06-30\nUSD.EUR\n0.933\n\n\n2024-07-01\nUSD.EUR\n0.931\n\n\n2024-07-02\nUSD.EUR\n0.932\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nexchange_ts |&gt;\n  autoplot(.vars = rate) + labs(title = exchange_ts$currency[1])\n\n\n\n\n\n\n\n\nFigure 2: Time plot of the exchange rate to convert US$1 into Euros\n\n\n\n\n\n\n\nShow the code\nacf_plot &lt;- exchange_ts |&gt; select(rate) |&gt; ACF() |&gt; autoplot(var = .resid)\n\npacf_plot &lt;- exchange_ts |&gt; select(rate) |&gt; PACF() |&gt; autoplot(var = .resid)\n\nacf_plot | pacf_plot\n\n\n\n\n\n\n\n\nFigure 3: Correlogram and partial correlogram for the time series representing the exchange rate to convert US$1 into Euros\n\n\n\n\n\n\n# Fit the ARIMA Model\n\nexchange_model &lt;- exchange_ts |&gt;\n  model(\n    auto = ARIMA(rate ~ 1 + pdq(0:2,0:1,0:2) + PDQ(0, 0, 0)),\n    \n    a000 = ARIMA(rate ~ 1 + pdq(0,0,0) + PDQ(0, 0, 0)),\n    a001 = ARIMA(rate ~ 1 + pdq(0,0,1) + PDQ(0, 0, 0)),\n    a002 = ARIMA(rate ~ 1 + pdq(0,0,2) + PDQ(0, 0, 0)),\n    a100 = ARIMA(rate ~ 1 + pdq(1,0,0) + PDQ(0, 0, 0)),\n    a101 = ARIMA(rate ~ 1 + pdq(1,0,1) + PDQ(0, 0, 0)),\n    a102 = ARIMA(rate ~ 1 + pdq(1,0,2) + PDQ(0, 0, 0)),\n    a200 = ARIMA(rate ~ 1 + pdq(2,0,0) + PDQ(0, 0, 0)),\n    a201 = ARIMA(rate ~ 1 + pdq(2,0,1) + PDQ(0, 0, 0)),\n    a202 = ARIMA(rate ~ 1 + pdq(2,0,2) + PDQ(0, 0, 0)),\n    \n    a011 = ARIMA(rate ~ 1 + pdq(0,1,1) + PDQ(0, 0, 0)),\n    a012 = ARIMA(rate ~ 1 + pdq(0,1,2) + PDQ(0, 0, 0)),\n    a110 = ARIMA(rate ~ 1 + pdq(1,1,0) + PDQ(0, 0, 0)),\n    a111 = ARIMA(rate ~ 1 + pdq(1,1,1) + PDQ(0, 0, 0)),\n    a112 = ARIMA(rate ~ 1 + pdq(1,1,2) + PDQ(0, 0, 0)),\n    a210 = ARIMA(rate ~ 1 + pdq(2,1,0) + PDQ(0, 0, 0)),\n    a211 = ARIMA(rate ~ 1 + pdq(2,1,1) + PDQ(0, 0, 0)),\n    a212 = ARIMA(rate ~ 1 + pdq(2,1,2) + PDQ(0, 0, 0))\n    )\n\nHere is one way to determine which model is selected by the “auto” process.\n\n\nShow the code\nexchange_model |&gt;\n  select(auto)\n\n\n# A mable: 1 x 1\n                     auto\n                  &lt;model&gt;\n1 &lt;ARIMA(1,1,1) w/ drift&gt;\n\n\nWe now examine all the fitted models to determine the value of the residual mean squared error (sigma2), log-likelihood, AIC, AICc, and BIC. For the log-likelihood, larger values are preferable. For all other measures, smaller values are preferred.\n\n\nShow the code\nexchange_model |&gt;\n  glance()\n\n\n\n\n\n\nTable 2: Values used in the model selection process for the time series representing the exchange rate to convert US$1 into Euros\n\n\n\n\n\n\nModel\nsigma2\nlog_lik\nAIC\nAICc\nBIC\n\n\n\n\nauto\n0\n1485.2\n-2962.5\n-2962.3\n-2947.5\n\n\na000\n0\n979.2\n-1954.4\n-1954.3\n-1946.9\n\n\na001\n0\n1176.6\n-2347.1\n-2347.1\n-2335.9\n\n\na002\n0\n1296.1\n-2584.2\n-2584.1\n-2569.2\n\n\na100\n0\n1469.5\n-2932.9\n-2932.9\n-2921.7\n\n\na101\n0\n1490.6\n**-2973.2**\n**-2973.1**\n**-2958.2**\n\n\na102\n**0**\n1491.5\n-2973\n-2972.8\n-2954.2\n\n\na200\n0\n1484.2\n-2960.4\n-2960.3\n-2945.4\n\n\na201\n0\n1491.3\n-2972.7\n-2972.5\n-2953.9\n\n\na202\n0\n**1491.6**\n-2971.2\n-2970.9\n-2948.7\n\n\na011\n0\n1484.2\n-2962.3\n-2962.2\n-2951.1\n\n\na012\n0\n1485.6\n-2963.1\n-2963\n-2948.1\n\n\na110\n0\n1477.6\n-2949.1\n-2949\n-2937.9\n\n\na111\n0\n1485.2\n-2962.5\n-2962.3\n-2947.5\n\n\na112\n0\n1485.9\n-2961.7\n-2961.5\n-2943\n\n\na210\n0\n1485.1\n-2962.2\n-2962\n-2947.2\n\n\na211\n0\n1485.9\n-2961.8\n-2961.6\n-2943.1\n\n\na212\n0\n1486\n-2959.9\n-2959.6\n-2937.4\n\n\n\n\n\n\n\n\n\n\nSuppose we choose to apply the “auto” model, which is \\(ARIMA(1,1,1)\\). The model parameters are summarized here:\n\n\nShow the code\nexchange_model |&gt;\n  select(auto) |&gt;\n  coefficients()\n\n\n\n\n\n\n\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nauto\nar1\n-0.1898907\n0.1302066\n-1.4583798\n0.1457386\n\n\nauto\nma1\n0.5648437\n0.1095075\n5.1580342\n0.0000004\n\n\nauto\nconstant\n0.0000450\n0.0001977\n0.2275258\n0.8201635\n\n\n\n\n\n\n\nThe following plots give the acf and pacf of the residuals from this model.\n\n\nShow the code\nmodel_resid &lt;- exchange_model |&gt;\n  select(auto) |&gt;\n  residuals()\n\nacf_plot &lt;- model_resid |&gt; ACF() |&gt; autoplot(var = .resid)\n\npacf_plot &lt;- model_resid |&gt; PACF() |&gt; autoplot(var = .resid)\n\nacf_plot | pacf_plot\n\n\n\n\n\n\n\n\nFigure 4: Correlogram and Partial Correlogram for the residuals from the ARIMA(1,1,1) model for the daily exchange rates to convert US$1 into Euros\n\n\n\n\n\nHere is a histogram of the residuals from our model.\n\n\nShow the code\nmodel_resid |&gt;\n  mutate(density = dnorm(.resid, mean(model_resid$.resid), sd(model_resid$.resid))) |&gt;\n  ggplot(aes(x = .resid)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"#56B4E9\", binwidth = 0.001) +\n    geom_line(aes(x = .resid, y = density)) +\n    theme_bw() +\n    labs(\n      x = \"Values\",\n      y = \"Frequency\",\n      title = \"Histogram of Residuals\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\nFigure 5: Histogram of the residuals from the ARIMA(1,1,1) model for the daily exchange rates to convert US$1 into Euros\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWrite the fitted model: \\[\n  x_t = ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n\\]\nDoes the model provide an appropriate fit for the data?\n\n\n\nHere is a forecast for the next 7 days based on our model.\n\nfinal_model &lt;- exchange_model |&gt;\n  select(auto)\n\ntemps_forecast &lt;- final_model |&gt;\n  select(auto) |&gt;\n  forecast(h = \"7 days\")\n\ntemps_forecast |&gt;\n  autoplot(exchange_ts, level = 95) +\n  geom_line(aes(y = .fitted, color = \"Fitted\"),\n    data = augment(final_model)) +\n  scale_color_discrete(name = \"\") +\n  labs(\n    x = paste0(\n                \"Date (\",\n                format(ymd(min(exchange_ts$date)), \"%d %b %Y\"),\n                  \" - \",\n                format(ymd(max(exchange_ts$date)), \"%d %b %Y\"),\n                \")\"\n              ),\n    y = \"Exchange Rate\",\n    title = \"Exchange Rate for Converting US$1 to Euros\",\n    subtitle = \"7-Day Forecast Based on our AR(1,1,1) Model\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )",
    "crumbs": [
      "Lesson 1",
      "Introduction to Non-stationary Models and Differencing"
    ]
  },
  {
    "objectID": "chapter_7_lesson_1.html#small-group-activity-fitting-an-arima-process---microsoft-stock-prices-20-min",
    "href": "chapter_7_lesson_1.html#small-group-activity-fitting-an-arima-process---microsoft-stock-prices-20-min",
    "title": "Introduction to Non-stationary Models and Differencing",
    "section": "Small-Group Activity: Fitting an ARIMA Process - Microsoft Stock Prices (20 min)",
    "text": "Small-Group Activity: Fitting an ARIMA Process - Microsoft Stock Prices (20 min)\nA time series given the daily closing price for Microsoft (MSFT) stock is given below. To handle the gaps in the data, we define a new variable, t, which gives the observation number.\n\n\nShow the code\n# Set symbol and date range\nsymbol &lt;- \"MSFT\"                # Abercrombie & Fitch stock trading symbol\ndate_start &lt;- \"2020-01-01\"\ndate_end &lt;- \"2024-03-28\"\n\n# Fetch stock prices\ndf_stock &lt;- tq_get(symbol, from = date_start, to = date_end, get = \"stock.prices\")\n\n# Transform data into tsibble\nstock_ts &lt;- df_stock |&gt;\n  mutate(\n    dates = date, \n    value = close\n  ) |&gt;\n  dplyr::select(dates, value) |&gt;\n  as_tibble() |&gt; \n  arrange(dates) |&gt;\n  mutate(t = 1:n()) |&gt;\n  as_tsibble(index = t, key = NULL)\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUsing the daily closing prices of Microsoft stock, do the following.\n\nMake a time plot of the data.\nCreate a correlogram and partial correlogram of the stock prices.\nFit candidate \\(ARIMA(p,d,q)\\) models to the data.\nChoose the “best” model, and justify your selection.\nGenerate a correlagram and partial correlogram of the residuals from your chosen model.\nMake a histogram of the residuals from your model.\nDid your your model account for the the time series?\nPredict the value 60 trading days in the future.\n\nNote: The “time” index is just an integer sequence in the stock_ts tsibble. So, apply the forecast() function as forecast(h = 60), rather than forecast(h = \"60 days\").",
    "crumbs": [
      "Lesson 1",
      "Introduction to Non-stationary Models and Differencing"
    ]
  },
  {
    "objectID": "chapter_7_lesson_1.html#homework-preview-5-min",
    "href": "chapter_7_lesson_1.html#homework-preview-5-min",
    "title": "Introduction to Non-stationary Models and Differencing",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_7_1.qmd \n\n\nClass Activity\n\n\n\nClass Activity\n\n\n\nClass Activity",
    "crumbs": [
      "Lesson 1",
      "Introduction to Non-stationary Models and Differencing"
    ]
  },
  {
    "objectID": "chapter_6_lesson_2.html",
    "href": "chapter_6_lesson_2.html",
    "title": "Autoregressive Moving Average (ARMA) Models",
    "section": "",
    "text": "Define autoregressive moving average (ARMA) models\n\n\nWrite the equation for an ARMA(p,q) model\nExpress an ARMA model in terms of the backward shift operators for the AR and MA components\nState facts about ARMA processes related to stationarity, invertibility, special cases, parsimony, and parameter redundancy\nUse ACF and PACF plots to determine if an AR, MA or ARMA model is appropriate for a time series\n\n\n\n\nApply an iterative time series modeling process\n\n\nFit a regression model to capture trend and seasonality\nExamine residual diagnostic plots to assess autocorrelation\nFit an ARMA model to the residuals if needed\nCheck the residuals of the ARMA model for white noise\nForecast the original series by combining the regression and ARMA model forecasts",
    "crumbs": [
      "Lesson 2",
      "Autoregressive Moving Average (ARMA) Models"
    ]
  },
  {
    "objectID": "chapter_6_lesson_2.html#learning-outcomes",
    "href": "chapter_6_lesson_2.html#learning-outcomes",
    "title": "Autoregressive Moving Average (ARMA) Models",
    "section": "",
    "text": "Define autoregressive moving average (ARMA) models\n\n\nWrite the equation for an ARMA(p,q) model\nExpress an ARMA model in terms of the backward shift operators for the AR and MA components\nState facts about ARMA processes related to stationarity, invertibility, special cases, parsimony, and parameter redundancy\nUse ACF and PACF plots to determine if an AR, MA or ARMA model is appropriate for a time series\n\n\n\n\nApply an iterative time series modeling process\n\n\nFit a regression model to capture trend and seasonality\nExamine residual diagnostic plots to assess autocorrelation\nFit an ARMA model to the residuals if needed\nCheck the residuals of the ARMA model for white noise\nForecast the original series by combining the regression and ARMA model forecasts",
    "crumbs": [
      "Lesson 2",
      "Autoregressive Moving Average (ARMA) Models"
    ]
  },
  {
    "objectID": "chapter_6_lesson_2.html#preparation",
    "href": "chapter_6_lesson_2.html#preparation",
    "title": "Autoregressive Moving Average (ARMA) Models",
    "section": "Preparation",
    "text": "Preparation\n\nRead Sections 6.5.1 and 6.6-6.7",
    "crumbs": [
      "Lesson 2",
      "Autoregressive Moving Average (ARMA) Models"
    ]
  },
  {
    "objectID": "chapter_6_lesson_2.html#learning-journal-exchange-10-min",
    "href": "chapter_6_lesson_2.html#learning-journal-exchange-10-min",
    "title": "Autoregressive Moving Average (ARMA) Models",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading another student’s?\nWhat would you recommend the other student add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 2",
      "Autoregressive Moving Average (ARMA) Models"
    ]
  },
  {
    "objectID": "chapter_6_lesson_2.html#class-activity-introduction-to-autoregressive-moving-average-arma-models-10-min",
    "href": "chapter_6_lesson_2.html#class-activity-introduction-to-autoregressive-moving-average-arma-models-10-min",
    "title": "Autoregressive Moving Average (ARMA) Models",
    "section": "Class Activity: Introduction to Autoregressive Moving Average (ARMA) Models (10 min)",
    "text": "Class Activity: Introduction to Autoregressive Moving Average (ARMA) Models (10 min)\n\nAutoregressive (AR) Models\nIn Chapter 4, Lesson 3, we learned the definition of an AR model:\n\n\n\n\n\n\nDefinition of an Autoregressive (AR) Model\n\n\n\nThe time series \\(\\{x_t\\}\\) is an autoregressive process of order \\(p\\), denoted as \\(AR(p)\\), if \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t ~~~~~~~~~~~~~~~~~~~~~~~ (4.15)\n\\]\nwhere \\(\\{w_t\\}\\) is white noise and the \\(\\alpha_i\\) are the model parameters with \\(\\alpha_p \\ne 0\\).\n\n\nThe \\(AR(p)\\) model can be expressed as: \\[\n  \\underbrace{\\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right)}_{\\theta_p \\left( \\mathbf{B} \\right)} x_t = w_t\n\\]\n\n\nMoving Average (MA) Models\nThe definition of an \\(MA(q)\\) model is:\n\n\n\n\n\n\nDefinition of a Moving Average (MA) Model\n\n\n\nWe say that a time series \\(\\{x_t\\}\\) is a moving average process of order \\(q\\), denoted as \\(MA(q)\\), if each term in the time series is a linear combination of the current white noise term and the \\(q\\) most recent past white noise terms.\nIt is given as: \\[\n  x_t = w_t + \\beta_1 w_{t-1} + \\beta_2 w_{t-2} + \\beta_3 w_{t-3} + \\cdots + \\beta_{q-1} w_{t-(q-1)} + \\beta_q w_{t-q}\n\\]\nwhere \\(\\{w_t\\}\\) is white noise with zero mean and variance \\(\\sigma_w^2\\), and the \\(\\beta_i\\) are the model parameters with \\(\\beta_q \\ne 0\\).\n\n\nWritten in terms of the backward shift operator, we have \\[\n  x_t = \\underbrace{\\left( 1 + \\beta_1 \\mathbf{B} + \\beta_2 \\mathbf{B}^2  + \\beta_3 \\mathbf{B}^3  + \\cdots + \\beta_{q-1} \\mathbf{B}^{q-1}  + \\beta_q \\mathbf{B}^{q} \\right)}_{\\phi_q(\\mathbf{B})} w_t\n\\]\nPutting the \\(AR\\) and \\(MA\\) models together, we get the \\(ARMA\\) model.\n\n\n\n\n\n\nDefinition of an Autogregressive Moving Average (ARMA) Model\n\n\n\nA time series \\(\\{ x_t \\}\\) follows an autoregressive moving average (ARMA) model of order \\((p, q)\\), which we write as \\(ARMA(p,q)\\), if it can be written as:\n\\[\n  x_t =\n    \\underbrace{\n      \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2}\n      + \\alpha_3 x_{t-3}\n      + \\cdots\n      % + \\alpha_{p-1} x_{t-(p-1)} +\n      \\alpha_p x_{t-p}\n    }_{\n      AR(p) ~ \\text{model}\n    }\n    + \\underbrace{\n      w_t + \\beta_1 w_{t-1} + \\beta_2 w_{t-2}\n      + \\beta_3 w_{t-3}\n      + \\cdots\n      % + \\beta_{q-1} w_{t-(q-1)}\n      + \\beta_q w_{t-q}\n    }_{\n      MA(q) ~ \\text{model}\n    }\n\\] where \\(\\{w_t\\}\\) is a white noise process.\n\n\nWe can write this as: \\[\n  \\theta_p \\left( \\mathbf{B} \\right) x_t\n  =\n  \\phi_q \\left( \\mathbf{B} \\right) w_t\n\\]\n\n\n\n\n\n\nFacts about ARMA Processes\n\n\n\nThe following facts are true for \\(ARMA(p,q)\\) processes:\n\nThe ARMA process is stationary if all the roots of \\(\\theta_p \\left( \\mathbf{B} \\right)\\) are greater than 1 in absolute value.\nThe ARMA process is invertible if all the roots of \\(\\phi_q \\left( \\mathbf{B} \\right)\\) are greater than 1 in absolute value.\nThe special case \\(ARMA(p,0)\\) is the \\(AR(p)\\) model.\nThe special case \\(ARMA(0,q)\\) is the \\(MA(q)\\) model.\nAn \\(ARMA\\) model will usually require fewer parameters than a single \\(MA\\) or \\(AR\\) model. This is called parameter parsimony.\nIf \\(\\theta\\) and \\(\\phi\\) have a common factor, a stationary model can be simplified. This is called parameter redundancy. As an example, the model \\[\n  \\left( 1 - \\frac{1}{2} \\mathbf{B} \\right)\\left( 1 - \\frac{1}{3} \\mathbf{B} \\right) x_t\n  =\n  \\left( 1-\\frac{1}{2} \\mathbf{B} \\right)\\left( 1 - \\frac{1}{4} \\mathbf{B} \\right) w_t\n\\] is the same as the model \\[\n  \\left( 1 - \\frac{1}{3} \\mathbf{B} \\right) x_t\n  =\n  \\left( 1 - \\frac{1}{4} \\mathbf{B} \\right) w_t\n\\]\n\n\n\n\n\nComparison of AR and MA Models\n\n\n\n\n\n\nACF and PACF of an \\(AR(p)\\) Process\n\n\n\nWe can use the pacf and acf plots to assess if an \\(AR(p)\\) or \\(MA(q)\\) model is appropriate. For an \\(AR(p)\\) or \\(MA(q)\\) process, we observe the following:\n\n\n\n\n\n\n\n\n\n\n\nAR(p)\nMA(q)\nARMA(p,q)\n\n\n\n\nACF\nTails off\nCuts off after lag \\(q\\)\nTails off\n\n\nPACF\nCuts off after lag \\(p\\)\nTails off\nTails off\n\n\n\n\n\n\n\n\n\nAnalyzing Time Series with a Regular Seasonal Pattern\nHere are some steps you can use to guide your work as you analyze time series with a regular seasonal pattern. Even though these are presented linearly, you may find it helpful to iterate between some of the steps as needed.\n\n\n\n\n\n\nAnalyzing a Time Series with a Regular Seasonal Pattern\n\n\n\n\nCreate a time plot of the series\nDetermine an appropriate model for the trend and seasonality; call this Model 1\n\nRegression\nHolt-Winters\nAdditive / Multiplicative Decomposition\nOther Techniques\n\nUse AIC/AICc/BIC (or other metrics) and your reasoning skills to choose the best model\nObtain the residuals\nCheck the residuals of Model 1 for evidence of autocorrelation and non-stationarity\n\nTime plot of the residuals\nACF plot\nPACF plot\n\nFit an AR/MA/ARMA model to the (stationary) residuals; call this Model 2\nCheck the residuals of Model 2 for evidence of autocorrelation and normality\n\nACF plot\nPACF plot\nHistogram\n\nForecast the trend and seasonality of the time series using Model 1\nForecast the residuals of Model 1 using Model 2\nAdd the two forecasts together\nPlot and summarize the resulting forecast\nRefine the models above as needed",
    "crumbs": [
      "Lesson 2",
      "Autoregressive Moving Average (ARMA) Models"
    ]
  },
  {
    "objectID": "chapter_6_lesson_2.html#class-activity-model-for-the-residuals-from-the-rexburg-weather-model-15-min",
    "href": "chapter_6_lesson_2.html#class-activity-model-for-the-residuals-from-the-rexburg-weather-model-15-min",
    "title": "Autoregressive Moving Average (ARMA) Models",
    "section": "Class Activity: Model for the Residuals from the Rexburg Weather Model (15 min)",
    "text": "Class Activity: Model for the Residuals from the Rexburg Weather Model (15 min)\n\nReview\nWe now review the model we built in Chapter 5 Lesson 3 for the monthly average of the daily high temperature in Rexburg, Idaho.\n\n\nShow the code\nweather_df &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/rexburg_weather_monthly.csv\") |&gt;\n  mutate(dates = my(date_text)) |&gt;\n  filter(dates &gt;= my(\"1/2008\") & dates &lt;= my(\"12/2023\")) |&gt;\n  rename(x = avg_daily_high_temp) |&gt;\n  mutate(TIME = 1:n()) |&gt;\n  mutate(\n    cos1 = cos(2 * pi * 1 * TIME/12),\n    cos2 = cos(2 * pi * 2 * TIME/12),\n    cos3 = cos(2 * pi * 3 * TIME/12),\n    cos4 = cos(2 * pi * 4 * TIME/12),\n    cos5 = cos(2 * pi * 5 * TIME/12),\n    cos6 = cos(2 * pi * 6 * TIME/12),\n    sin1 = sin(2 * pi * 1 * TIME/12),\n    sin2 = sin(2 * pi * 2 * TIME/12),\n    sin3 = sin(2 * pi * 3 * TIME/12),\n    sin4 = sin(2 * pi * 4 * TIME/12),\n    sin5 = sin(2 * pi * 5 * TIME/12),\n    sin6 = sin(2 * pi * 6 * TIME/12)) |&gt;\n  mutate(zTIME = (TIME - mean(TIME)) / sd(TIME)) |&gt;\n  as_tsibble(index = TIME)\n\nweather_df |&gt;\n  as_tsibble(index = dates) |&gt;\n  autoplot(.vars = x) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#F0E442\") +\n    labs(\n      x = \"Month\",\n      y = \"Mean Daily High Temperature (Fahrenheit)\",\n      title = \"Time Plot of Mean Daily Rexburg High Temperature by Month\",\n      subtitle = paste0(\"(\", format(weather_df$dates %&gt;% head(1), \"%b %Y\"), endash, format(weather_df$dates %&gt;% tail(1), \"%b %Y\"), \")\")\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5),\n      plot.subtitle = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\nFigure 1: Time plot of the monthly mean daily high temperatures in Rexburg Idaho, in degrees Fahrenheit\n\n\n\n\n\nWe chose the “Reduced Linear 5” model. For convenience, we reprint the coefficients here.\n\n\nShow the code\nreduced5_linear_lm &lt;- weather_df |&gt;\n  model(reduced_linear_5  = TSLM(x ~ zTIME + sin1 + cos1 + sin2 + cos2))\n\nreduced5_linear_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 6 × 7\n  .model           term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;            &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_linear_5 (Intercept)   56.5       0.279    202.   5.06e-220 TRUE \n2 reduced_linear_5 zTIME          0.692     0.280      2.47 1.44e-  2 TRUE \n3 reduced_linear_5 sin1         -16.1       0.395    -40.7  1.66e- 94 TRUE \n4 reduced_linear_5 cos1         -23.7       0.395    -60.1  9.40e-124 TRUE \n5 reduced_linear_5 sin2           1.26      0.395      3.20 1.60e-  3 TRUE \n6 reduced_linear_5 cos2          -2.86      0.395     -7.24 1.13e- 11 TRUE \n\n\nShow the code\nr5lin_coef_unrounded &lt;- reduced5_linear_lm |&gt;\n  tidy() |&gt;\n  select(term, estimate, std.error)\n\nr5lin_coef &lt;- r5lin_coef_unrounded |&gt;\n  round_df(3)\n\nstats_unrounded &lt;- weather_df |&gt;\n  as_tibble() |&gt;\n  dplyr::select(TIME) |&gt;\n  summarize(mean = mean(TIME), sd = sd(TIME))\n\nstats &lt;- stats_unrounded |&gt;\n  round_df(3)\n\n\nThe fitted model is:\n\\[\\begin{align*}\n  x_t\n      &= \\hat \\beta_0 + \\hat \\beta_1 \\left( \\frac{t - \\bar t}{s_t} \\right)  \\\\\n      & ~~~~~~~~~~ + \\hat \\beta_2 \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + \\hat \\beta_3 \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~ + \\hat \\beta_4 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + \\hat \\beta_5 \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n      \\\\\n      &= 56.467\n            + 0.692 \\left( \\frac{t - 96.5}{55.57} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~ + (-16.068) \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + (-23.708) \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~ + 1.264 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + (-2.858) \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n      \\\\\n\\end{align*}\\]\n\n\nShow the code\nnum_months &lt;- weather_df |&gt; \n  as_tibble() |&gt; \n  dplyr::select(TIME) |&gt; \n  tail(1) |&gt; \n  pull()\n\ndf &lt;- tibble( TIME = seq(1, num_months, 0.01) ) |&gt;\n  mutate(\n    cos1 = cos(2 * pi * 1 * TIME/12),\n    cos2 = cos(2 * pi * 2 * TIME/12),\n    cos3 = cos(2 * pi * 3 * TIME/12),\n    cos4 = cos(2 * pi * 4 * TIME/12),\n    cos5 = cos(2 * pi * 5 * TIME/12),\n    cos6 = cos(2 * pi * 6 * TIME/12),\n    sin1 = sin(2 * pi * 1 * TIME/12),\n    sin2 = sin(2 * pi * 2 * TIME/12),\n    sin3 = sin(2 * pi * 3 * TIME/12),\n    sin4 = sin(2 * pi * 4 * TIME/12),\n    sin5 = sin(2 * pi * 5 * TIME/12),\n    sin6 = sin(2 * pi * 6 * TIME/12)) |&gt;\n  mutate(zTIME = (TIME - mean(TIME)) / sd(TIME)) |&gt;\n  as_tsibble(index = TIME)\n\nlinear5_ts &lt;- reduced5_linear_lm |&gt;\n  forecast(df) |&gt;\n  as_tibble() |&gt;\n  dplyr::select(TIME, .mean) |&gt;\n  rename(value = .mean) |&gt;\n  mutate(Model = \"Linear 5\")\n\ndata_ts &lt;- weather_df |&gt; \n  as_tibble() |&gt;\n  rename(value = x) |&gt;\n  mutate(Model = \"Data\") |&gt;\n  dplyr::select(TIME, value, Model)\n\ncombined_ts &lt;- bind_rows(data_ts, linear5_ts) \npoint_ts &lt;- combined_ts |&gt; filter(TIME == floor(TIME))\n\nokabe_ito_colors &lt;- c(\"#000000\", \"#E69F00\")\n\ncombined_ts |&gt;\n  ggplot(aes(x = TIME, y = value, color = Model)) +\n  geom_line() +\n  geom_point(data = point_ts, alpha = 0.5) +\n  labs(\n      x = \"Month Number\",\n      y = \"Temperature (Fahrenheit)\",\n      title = \"Monthly Average of Daily High Temperatures in Rexburg\",\n      subtitle = paste0(\"(\", format(weather_df$dates %&gt;% head(1), \"%b %Y\"), endash, format(weather_df$dates %&gt;% tail(1), \"%b %Y\"), \")\")\n  ) +    \n  scale_color_manual(\n    values = okabe_ito_colors[1:nrow(combined_ts |&gt; as_tibble() |&gt; select(Model) |&gt; unique())], \n    name = \"\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"top\", # Position the legend at the top\n    legend.direction = \"horizontal\" # Set the legend direction to horizontal\n  )\n\n\n\n\n\n\n\n\nFigure 2: Time plot of the monthly mean daily high temperatures in Rexburg Idaho, in degrees Fahrenheit; the fitted values from the regression model are given in orange\n\n\n\n\n\n\n\nFitting an ARMA(p,q) Model\nFirst, we create an acf and pacf plot of the residuals from the model above.\n\n\nShow the code\nreduced5_linear_lm |&gt;\n  residuals() |&gt;\n  ACF() |&gt; \n  autoplot(var = .resid)\n\n\n\n\n\n\n\n\nFigure 3: ACF Plot of the Residuals from the Rexburg Weather Model\n\n\n\n\n\n\n\nShow the code\nreduced5_linear_lm |&gt; \n  residuals() |&gt;\n  PACF() |&gt;\n  autoplot(var = .resid)\n\n\n\n\n\n\n\n\nFigure 4: PACF Plot of the Residuals from the Rexburg Weather Model\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat ARMA model does the combined information in the acf and pacf plots suggest would be appropriate for the residuals of the Rexburg weather model?\n\n\n\nHere are summaries of some ARMA models that could be constructed.\n\nmodel_resid &lt;- reduced5_linear_lm |&gt;\n  residuals() |&gt;\n  select(-.model) |&gt;\n  model(\n    auto = ARIMA(.resid ~ 1 + pdq(0:2,0,0:2) + PDQ(0, 0, 0)),\n    a000 = ARIMA(.resid ~ 1 + pdq(0,0,0) + PDQ(0, 0, 0)),\n    a001 = ARIMA(.resid ~ 1 + pdq(0,0,1) + PDQ(0, 0, 0)),\n    a002 = ARIMA(.resid ~ 1 + pdq(0,0,2) + PDQ(0, 0, 0)),\n    a100 = ARIMA(.resid ~ 1 + pdq(1,0,0) + PDQ(0, 0, 0)),\n    a101 = ARIMA(.resid ~ 1 + pdq(1,0,1) + PDQ(0, 0, 0)),\n    a102 = ARIMA(.resid ~ 1 + pdq(1,0,2) + PDQ(0, 0, 0)),\n    a200 = ARIMA(.resid ~ 1 + pdq(2,0,0) + PDQ(0, 0, 0)),\n    a201 = ARIMA(.resid ~ 1 + pdq(2,0,1) + PDQ(0, 0, 0)),\n    a202 = ARIMA(.resid ~ 1 + pdq(2,0,2) + PDQ(0, 0, 0))) \n\nmodel_resid |&gt;\n  glance() \n\n\n\n\n\nTable 1: Comparison of the AIC, AICc, and BIC values for the models fitted to the Rexburg weather data\n\n\n\n\n\n\nModel\nAIC\nAICc\nBIC\n\n\n\n\nauto\n**1056**\n**1056.1**\n**1065.7**\n\n\na000\n1062.1\n1062.1\n1068.6\n\n\na001\n1056.9\n1057\n1066.7\n\n\na002\n1057.9\n1058.1\n1071\n\n\na100\n**1056**\n**1056.1**\n**1065.7**\n\n\na101\n1057.2\n1057.4\n1070.2\n\n\na102\n1059.1\n1059.4\n1075.4\n\n\na200\n1057.5\n1057.7\n1070.5\n\n\na201\n1059.1\n1059.5\n1075.4\n\n\na202\n1059.6\n1060\n1079.1\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nmodel_resid |&gt;\n  tidy() |&gt;\n  filter(.model == \"a101\") \n\n\n# A tibble: 3 × 6\n  .model term     estimate std.error statistic p.value\n  &lt;chr&gt;  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;\n1 a101   ar1       0.528       0.262    2.02    0.0451\n2 a101   ma1      -0.341       0.288   -1.18    0.238 \n3 a101   constant  0.00236     0.176    0.0134  0.989 \n\n\n\n\nShow the code\nmodel_resid |&gt;\n  select(a101) |&gt;\n  residuals() |&gt;\n  ACF() |&gt;\n  autoplot(var = .resid)\n\n\n\n\n\n\n\n\nFigure 5: ACF plot of the residuals from the \\(ARMA(1,1)\\) model\n\n\n\n\n\n\n\nShow the code\nmodel_resid |&gt;\n  select(a101) |&gt;\n  residuals() |&gt;\n  PACF() |&gt;\n  autoplot(var = .resid)\n\n\n\n\n\n\n\n\nFigure 6: PACF plot of the residuals from the \\(ARMA(1,1)\\) model",
    "crumbs": [
      "Lesson 2",
      "Autoregressive Moving Average (ARMA) Models"
    ]
  },
  {
    "objectID": "chapter_6_lesson_2.html#small-group-activity-industrial-electricity-consumption-in-texas-20-min",
    "href": "chapter_6_lesson_2.html#small-group-activity-industrial-electricity-consumption-in-texas-20-min",
    "title": "Autoregressive Moving Average (ARMA) Models",
    "section": "Small-Group Activity: Industrial Electricity Consumption in Texas (20 min)",
    "text": "Small-Group Activity: Industrial Electricity Consumption in Texas (20 min)\nThese data represent the amount of electricity used each month for industrial applications in Texas.\n\n\nShow the code\nelec_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/electricity_tx.csv\") |&gt;\n  dplyr::select(-comments) |&gt;\n  mutate(month = my(month)) |&gt;\n  mutate(\n    t = 1:n(),\n    std_t = (t - mean(t)) / sd(t)\n  ) |&gt;\n  mutate(\n    cos1 = cos(2 * pi * 1 * t / 12),\n    cos2 = cos(2 * pi * 2 * t / 12),\n    cos3 = cos(2 * pi * 3 * t / 12),\n    cos4 = cos(2 * pi * 4 * t / 12),\n    cos5 = cos(2 * pi * 5 * t / 12),\n    cos6 = cos(2 * pi * 6 * t / 12),\n    sin1 = sin(2 * pi * 1 * t / 12),\n    sin2 = sin(2 * pi * 2 * t / 12),\n    sin3 = sin(2 * pi * 3 * t / 12),\n    sin4 = sin(2 * pi * 4 * t / 12),\n    sin5 = sin(2 * pi * 5 * t / 12)\n  ) |&gt;\n  as_tsibble(index = month)\n\nelec_plot_raw &lt;- elec_ts |&gt;\n    autoplot(.vars = megawatthours) +\n    labs(\n      x = \"Month\",\n      y = \"Megawatt-hours\",\n      title = \"Texas' Industrial Electricity Use\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\nelec_plot_log &lt;- elec_ts |&gt;\n    autoplot(.vars = log(megawatthours)) +\n    labs(\n      x = \"Month\",\n      y = \"log(Megwatt-hours)\",\n      title = \"Log of Texas' Industrial Electricity Use\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\nelec_plot_raw | elec_plot_log\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUse the Texas industrial electricity consumption data to do the following.\n\nSelect an appropriate model to fit the time series using the AIC, AICc, or BIC critera.\nDetermine the best \\(ARMA(p,q)\\) model for the residuals.",
    "crumbs": [
      "Lesson 2",
      "Autoregressive Moving Average (ARMA) Models"
    ]
  },
  {
    "objectID": "chapter_6_lesson_2.html#small-group-activity-fitting-an-armapq-model-to-the-retail-data-30-min",
    "href": "chapter_6_lesson_2.html#small-group-activity-fitting-an-armapq-model-to-the-retail-data-30-min",
    "title": "Autoregressive Moving Average (ARMA) Models",
    "section": "Small Group Activity: Fitting an ARMA(p,q) Model to the Retail Data (30 min)",
    "text": "Small Group Activity: Fitting an ARMA(p,q) Model to the Retail Data (30 min)\nApply what you have learned in this lesson to the retail sales data. In particular, consider full-service restaurants (NAICS code 722511).\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUse the total retail sales for full-service restaurants to do the following.\n\nSelect an appropriate model to fit the time series using the AIC, AICc, or BIC critera.\nDetermine the best \\(ARMA(p,q)\\) model for the residuals.\nForecast the data for the next 5 years using the regression model.\nForecast the residuals for the next 5 years using ARMA model.\nSum the two forecasted time series and plot the combined series against the original retail sales data.\n\n\n\nHere is some code you can use to help you get started.\n\n\nShow the code\n# Read in retail sales data for \"Full-Service Restaurants\"\nretail_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/retail_by_business_type.parquet\") |&gt;\n  filter(naics == 722511) |&gt;\n  mutate(\n    month = yearmonth(as_date(month)),\n    month_number = month(month)\n  ) |&gt;\n  mutate(t = 1:n()) |&gt;\n  mutate(std_t = (t - mean(t)) / sd(t)) |&gt;\n  mutate(\n    cos1 = cos(2 * pi * 1 * t / 12),\n    cos2 = cos(2 * pi * 2 * t / 12),\n    cos3 = cos(2 * pi * 3 * t / 12),\n    cos4 = cos(2 * pi * 4 * t / 12),\n    cos5 = cos(2 * pi * 5 * t / 12),\n    cos6 = cos(2 * pi * 6 * t / 12),\n    sin1 = sin(2 * pi * 1 * t / 12),\n    sin2 = sin(2 * pi * 2 * t / 12),\n    sin3 = sin(2 * pi * 3 * t / 12),\n    sin4 = sin(2 * pi * 4 * t / 12),\n    sin5 = sin(2 * pi * 5 * t / 12)\n  ) |&gt;\n  mutate(\n    in_great_recession = ifelse(ym(month) &gt;= my(\"December 2007\") & month &lt;= my(\"June 2009\"), 1, 0),\n    after_great_recession = ifelse(ym(month) &gt; my(\"June 2009\"), 1, 0)\n  ) |&gt;\n  as_tsibble(index = month)\n\nretail_plot_raw &lt;- retail_ts |&gt;\n    autoplot(.vars = sales_millions) +\n    labs(\n      x = \"Month\",\n      y = \"sales_millions\",\n      title = \"Other General Merchandise Sales\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\nretail_plot_log &lt;- retail_ts |&gt;\n    autoplot(.vars = log(sales_millions)) +\n    labs(\n      x = \"Month\",\n      y = \"log(sales_millions)\",\n      title = \"Logarithm of Other Gen. Merch. Sales\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\nretail_plot_raw | retail_plot_log\n\n\n\n\n\n\n\n\nFigure 7: Time plot of the time series (left) and the natural logarithm of the time series (right)\n\n\n\n\n\n\n\nShow the code\nretail_final_lm &lt;- retail_ts |&gt;\n  model(retail_full = TSLM(log(sales_millions) ~  \n      (in_great_recession * std_t) + (after_great_recession * std_t) \n      + (in_great_recession * I(std_t^2) ) + (after_great_recession * I(std_t^2) )\n      + (in_great_recession * I(std_t^3) ) + (after_great_recession * I(std_t^3) )\n      + sin1 + cos1 + sin2 + cos2 + sin3 + cos3\n      + sin4 + cos4 + sin5 + cos5 + cos6 # Note sin6 is omitted\n    )\n    ) \n\nretail_final_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) |&gt;\n  knitr::kable(format = \"html\", align='ccccccc', escape = FALSE, width = NA, row.names = FALSE) |&gt;\n  kable_styling(full_width = FALSE, \"striped\")\n\n\n\n\nTable 2: Coefficients for the fitted model for the total sales in full-service restaurants in the United States\n\n\n\n\n\n\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\nsig\n\n\n\n\nretail_full\n(Intercept)\n9.6959936\n0.0062100\n1561.3586384\n0.0000000\nTRUE\n\n\nretail_full\nin_great_recession\n0.1057700\n0.1187801\n0.8904693\n0.3738957\nFALSE\n\n\nretail_full\nstd_t\n0.4905760\n0.0326870\n15.0083105\n0.0000000\nTRUE\n\n\nretail_full\nafter_great_recession\n-0.1556298\n0.0312565\n-4.9791109\n0.0000011\nTRUE\n\n\nretail_full\nI(std_t^2)\n-0.0163208\n0.0458329\n-0.3560935\n0.7220097\nFALSE\n\n\nretail_full\nI(std_t^3)\n-0.0102056\n0.0178717\n-0.5710498\n0.5683744\nFALSE\n\n\nretail_full\nsin1\n-0.0000252\n0.0020362\n-0.0123921\n0.9901207\nFALSE\n\n\nretail_full\ncos1\n-0.0360436\n0.0020595\n-17.5008207\n0.0000000\nTRUE\n\n\nretail_full\nsin2\n-0.0037502\n0.0020154\n-1.8607818\n0.0637096\nFALSE\n\n\nretail_full\ncos2\n0.0078111\n0.0020169\n3.8728011\n0.0001309\nTRUE\n\n\nretail_full\nsin3\n-0.0116397\n0.0020098\n-5.7913822\n0.0000000\nTRUE\n\n\nretail_full\ncos3\n0.0213598\n0.0020189\n10.5801523\n0.0000000\nTRUE\n\n\nretail_full\nsin4\n0.0001229\n0.0020100\n0.0611468\n0.9512812\nFALSE\n\n\nretail_full\ncos4\n0.0101346\n0.0020145\n5.0308564\n0.0000008\nTRUE\n\n\nretail_full\nsin5\n0.0277666\n0.0020122\n13.7990897\n0.0000000\nTRUE\n\n\nretail_full\ncos5\n0.0203802\n0.0020132\n10.1232890\n0.0000000\nTRUE\n\n\nretail_full\ncos6\n0.0038084\n0.0014225\n2.6772474\n0.0078131\nTRUE\n\n\nretail_full\nin_great_recession:std_t\n-2.5718611\n3.0873172\n-0.8330408\n0.4054549\nFALSE\n\n\nretail_full\nstd_t:after_great_recession\n0.0198043\n0.1413695\n0.1400891\n0.8886794\nFALSE\n\n\nretail_full\nin_great_recession:I(std_t^2)\n14.8230840\n24.4731229\n0.6056883\n0.5451593\nFALSE\n\n\nretail_full\nafter_great_recession:I(std_t^2)\n0.1215930\n0.1888445\n0.6438792\n0.5201238\nFALSE\n\n\nretail_full\nin_great_recession:I(std_t^3)\n-35.1756536\n60.1509944\n-0.5847892\n0.5591094\nFALSE\n\n\nretail_full\nafter_great_recession:I(std_t^3)\n-0.0653848\n0.0766366\n-0.8531788\n0.3942105\nFALSE\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nretail_resid_df &lt;- retail_final_lm |&gt; \n  residuals() |&gt; \n  as_tibble() |&gt; \n  dplyr::select(.resid) |&gt;\n  rename(x = .resid) \n  \nretail_resid_df |&gt;\n  mutate(density = dnorm(x, mean(retail_resid_df$x), sd(retail_resid_df$x))) |&gt;\n  ggplot(aes(x = x)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"#56B4E9\", binwidth = 0.02) +\n    geom_line(aes(x = x, y = density)) +\n    theme_bw() +\n    labs(\n      x = \"Values\",\n      y = \"Frequency\",\n      title = \"Histogram of Residuals\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\nFigure 8: Histogram of the residuals from the fitted regression model\n\n\n\n\n\nThe skewness is given by the command skewness:\n\n\nShow the code\nskewness(retail_resid_df$x)\n\n\n[1] 0.01014588\n\n\n\n\nShow the code\nretail_final_lm |&gt;\n  residuals() |&gt;\n  ACF() |&gt; \n  autoplot(var = .resid)\n\n\n\n\n\n\n\n\nFigure 9: Correlogram of the residuals from the fitted regression model\n\n\n\n\n\n\n\nShow the code\nretail_final_lm |&gt;\n  residuals() |&gt;\n  PACF() |&gt; \n  autoplot(var = .resid)\n\n\n\n\n\n\n\n\nFigure 10: Partial correlogram of the residuals from the fitted regression model\n\n\n\n\n\n\n\nShow the code\n################################################################\n#   Be sure to adjust this model to match what you did above   #\n################################################################\n#\n# Model without any seasonal trends\nretail_no_seasonal_lm &lt;- retail_ts |&gt;\n  model(retail_full_quad = TSLM(log(sales_millions) ~  \n      (in_great_recession * std_t) + (after_great_recession * std_t) \n      + (in_great_recession * I(std_t^2) ) + (after_great_recession * I(std_t^2) )\n      + (in_great_recession * I(std_t^3) ) + (after_great_recession * I(std_t^3) )\n    )\n    )\n\nretail_ts |&gt; \n  autoplot(.vars = sales_millions) +\n  # ggplot(mapping = aes(x = month, y = sales_millions)) +\n  # geom_line() +\n  geom_line(data = augment(retail_no_seasonal_lm),\n            aes(x = month, y = .fitted),\n            color = \"#E69F00\",\n            linewidth = 1\n            ) +\n  geom_line(data = augment(retail_final_lm),\n            aes(x = month, y = .fitted),\n            color = \"blue\",\n            alpha = 0.7,\n            # linewidth = 1\n            ) +\n  labs(\n    x = \"Month\",\n    y = \"Sales (Millions of U.S. Dollars)\",\n    title = paste0(retail_ts$business[1], \" (\", retail_ts$naics[1], \")\")\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFigure 11: Time plot of the retail sales data for full-service restaurants; the regression curve without the seasonal terms is given in orange; the predicted values from the model are given in blue\n\n\n\n\n\n\n\nShow the code\nmodel_resid &lt;- retail_final_lm |&gt;\n  residuals() |&gt;\n  select(-.model) |&gt;\n  model(\n    auto = ARIMA(.resid ~ 1 + pdq(0:5,0,0:5) + PDQ(0, 0, 0)),\n    a000 = ARIMA(.resid ~ 1 + pdq(0,0,0) + PDQ(0, 0, 0)),\n    a001 = ARIMA(.resid ~ 1 + pdq(0,0,1) + PDQ(0, 0, 0)),\n    a002 = ARIMA(.resid ~ 1 + pdq(0,0,2) + PDQ(0, 0, 0)),\n    a003 = ARIMA(.resid ~ 1 + pdq(0,0,3) + PDQ(0, 0, 0)),\n    a004 = ARIMA(.resid ~ 1 + pdq(0,0,4) + PDQ(0, 0, 0)),\n    a005 = ARIMA(.resid ~ 1 + pdq(0,0,5) + PDQ(0, 0, 0)),\n    a100 = ARIMA(.resid ~ 1 + pdq(1,0,0) + PDQ(0, 0, 0)),\n    a101 = ARIMA(.resid ~ 1 + pdq(1,0,1) + PDQ(0, 0, 0)),\n    a102 = ARIMA(.resid ~ 1 + pdq(1,0,2) + PDQ(0, 0, 0)),\n    a103 = ARIMA(.resid ~ 1 + pdq(1,0,3) + PDQ(0, 0, 0)),\n    a104 = ARIMA(.resid ~ 1 + pdq(1,0,4) + PDQ(0, 0, 0)),\n    a105 = ARIMA(.resid ~ 1 + pdq(1,0,5) + PDQ(0, 0, 0)), \n    a200 = ARIMA(.resid ~ 1 + pdq(2,0,0) + PDQ(0, 0, 0)),\n    a201 = ARIMA(.resid ~ 1 + pdq(2,0,1) + PDQ(0, 0, 0)),\n    a202 = ARIMA(.resid ~ 1 + pdq(2,0,2) + PDQ(0, 0, 0)),\n    a203 = ARIMA(.resid ~ 1 + pdq(2,0,3) + PDQ(0, 0, 0)),\n    a204 = ARIMA(.resid ~ 1 + pdq(2,0,4) + PDQ(0, 0, 0)),\n    a205 = ARIMA(.resid ~ 1 + pdq(2,0,5) + PDQ(0, 0, 0)),\n    a300 = ARIMA(.resid ~ 1 + pdq(3,0,0) + PDQ(0, 0, 0)),\n    a301 = ARIMA(.resid ~ 1 + pdq(3,0,1) + PDQ(0, 0, 0)),\n    a302 = ARIMA(.resid ~ 1 + pdq(3,0,2) + PDQ(0, 0, 0)),\n    a303 = ARIMA(.resid ~ 1 + pdq(3,0,3) + PDQ(0, 0, 0)),\n    a304 = ARIMA(.resid ~ 1 + pdq(3,0,4) + PDQ(0, 0, 0)),\n    a305 = ARIMA(.resid ~ 1 + pdq(3,0,5) + PDQ(0, 0, 0)), \n    a400 = ARIMA(.resid ~ 1 + pdq(4,0,0) + PDQ(0, 0, 0)),\n    a401 = ARIMA(.resid ~ 1 + pdq(4,0,1) + PDQ(0, 0, 0)),\n    a402 = ARIMA(.resid ~ 1 + pdq(4,0,2) + PDQ(0, 0, 0)),\n    a403 = ARIMA(.resid ~ 1 + pdq(4,0,3) + PDQ(0, 0, 0)),\n    a404 = ARIMA(.resid ~ 1 + pdq(4,0,4) + PDQ(0, 0, 0)),\n    a405 = ARIMA(.resid ~ 1 + pdq(4,0,5) + PDQ(0, 0, 0)),\n    a500 = ARIMA(.resid ~ 1 + pdq(5,0,0) + PDQ(0, 0, 0)),\n    a501 = ARIMA(.resid ~ 1 + pdq(5,0,1) + PDQ(0, 0, 0)),\n    a502 = ARIMA(.resid ~ 1 + pdq(5,0,2) + PDQ(0, 0, 0)),\n    a503 = ARIMA(.resid ~ 1 + pdq(5,0,3) + PDQ(0, 0, 0)),\n    a504 = ARIMA(.resid ~ 1 + pdq(5,0,4) + PDQ(0, 0, 0)),\n    a505 = ARIMA(.resid ~ 1 + pdq(5,0,5) + PDQ(0, 0, 0))\n  )\n\n\n# This command indicates which model was selected by \"auto\"\n# model_resid |&gt; select(auto)\n\nmodel_resid |&gt; \n  glance() |&gt;\n  # uncomment the next line to only show the \"best\" models according to these criteria\n  # filter(AIC == min(AIC) | AICc == min(AICc) | BIC == min(BIC) | .model == \"auto\") |&gt;\n  knitr::kable(format = \"html\", align='ccccccc', escape = FALSE, width = NA, row.names = FALSE) |&gt;\n  kable_styling(full_width = FALSE, \"striped\")\n\n\n\n\nTable 3: Results from the process of fitting an ARMA model to the residuals of the model for the total sales in full-service restaurants in the United States\n\n\n\n\n\n\n.model\nsigma2\nlog_lik\nAIC\nAICc\nBIC\nar_roots\nma_roots\n\n\n\n\nauto\n0.0003644\n841.5138\n-1667.028\n-1666.631\n-1635.677\n-0.5762646+0.8613026i, -0.5762646-0.8613026i, 2.2627735-0.8408307i, 2.2627735+0.8408307i\n-0.5154592+0.9948076i, -0.5154592-0.9948076i\n\n\na000\n0.0005764\n762.4127\n-1520.825\n-1520.793\n-1512.988\n\n\n\n\na001\n0.0004803\n793.5159\n-1581.032\n-1580.966\n-1569.275\n\n-2.382155+0i\n\n\na002\n0.0004782\n794.7146\n-1581.429\n-1581.320\n-1565.754\n\n-1.822802+2.494324i, -1.822802-2.494324i\n\n\na003\n0.0004109\n820.4694\n-1630.939\n-1630.775\n-1611.344\n\n0.232624+1.346072i, -1.379822+0.000000i, 0.232624-1.346072i\n\n\na004\n0.0004086\n821.8842\n-1631.768\n-1631.538\n-1608.255\n\n0.090446+1.334121i, -1.288012-0.000000i, 0.090446-1.334121i, 4.263380+0.000000i\n\n\na005\n0.0004097\n821.8871\n-1629.774\n-1629.466\n-1602.342\n\n0.096742+1.327683i, -1.295555-0.000000i, 0.096742-1.327683i, 3.658704-0.000000i, -19.273408-0.000000i\n\n\na100\n0.0004615\n800.2336\n-1594.467\n-1594.402\n-1582.710\n2.220227+0i\n\n\n\na101\n0.0004590\n801.5727\n-1595.145\n-1595.036\n-1579.470\n1.709493+0i\n6.008737+0i\n\n\na102\n0.0004537\n803.9495\n-1597.899\n-1597.735\n-1578.305\n2.092518+0i\n0.291385+2.033057i, 0.291385-2.033057i\n\n\na103\n0.0004091\n821.6652\n-1631.330\n-1631.100\n-1607.817\n-4.593936+0i\n0.114692+1.365411i, -1.273789-0.000000i, 0.114692-1.365411i\n\n\na104\n0.0003923\n827.7659\n-1641.532\n-1641.224\n-1614.100\n1.114879+0i\n0.202209+1.31577i, 0.202209-1.31577i, 1.000001-0.00000i, -1.352297+0.00000i\n\n\na105\n0.0003929\n828.0239\n-1640.048\n-1639.651\n-1608.697\n1.103023+0i\n1.000002-0.000000i, -1.312010+0.000000i, 0.146005-1.323116i, 0.146005+1.323116i, 9.340950+0.000000i\n\n\na200\n0.0004586\n801.7290\n-1595.458\n-1595.349\n-1579.782\n1.741604+0i, -6.053939+0i\n\n\n\na201\n0.0004597\n801.7718\n-1593.544\n-1593.380\n-1573.949\n1.707303+0i, -8.322029+0i\n17.35635+0i\n\n\na202\n0.0004424\n808.6177\n-1605.235\n-1605.005\n-1581.722\n1.11252+1.235597i, 1.11252-1.235597i\n0.478362+1.381186i, 0.478362-1.381186i\n\n\na203\n0.0004020\n824.8516\n-1635.703\n-1635.395\n-1608.271\n-0.603528+1.04516i, -0.603528-1.04516i\n-0.341680+1.04375i, -1.490094-0.00000i, -0.341680-1.04375i\n\n\na204\n0.0003928\n828.0543\n-1640.109\n-1639.712\n-1608.757\n1.103352+0i, -8.074288+0i\n1.000000+0.00000i, -1.299607-0.00000i, 0.140388-1.33086i, 0.140388+1.33086i\n\n\na205\n0.0003944\n827.7675\n-1637.535\n-1637.038\n-1602.265\n1.114557+0i, -1.380078+0i\n1.000000+0.000000i, -1.304935+0.000000i, 0.200885-1.317038i, 0.200885+1.317038i, -1.439098+0.000000i\n\n\na300\n0.0004588\n802.1164\n-1594.233\n-1594.069\n-1574.638\n1.561490-0.000000i, -1.559974+3.283859i, -1.559974-3.283859i\n\n\n\na301\n0.0004395\n809.7261\n-1607.452\n-1607.222\n-1583.939\n1.489169-0.000000i, -1.438632+0.982914i, -1.438632-0.982914i\n-1.421429+0i\n\n\na302\n0.0003535\n844.0552\n-1674.110\n-1673.803\n-1646.678\n-0.5774684+0.8165105i, -0.5774684-0.8165105i, 1.7298070-0.0000000i\n-0.5791569+0.8214568i, -0.5791569-0.8214568i\n\n\na303\n0.0003668\n840.4061\n-1664.812\n-1664.416\n-1633.461\n-0.5727277+0.8592709i, -0.5727277-0.8592709i, 1.7714851-0.0000000i\n-0.5205722+0.9760787i, -0.5205722-0.9760787i, -6.1372611-0.0000000i\n\n\na304\n0.0003475\n849.1941\n-1680.388\n-1679.891\n-1645.118\n-0.5791893+0.8188125i, -0.5791893-0.8188125i, 2.5938880+0.0000000i\n-0.6126397+0.8363729i, -0.6126397-0.8363729i, -0.2481793-1.7853598i, -0.2481793+1.7853598i\n\n\na305\n0.0003467\n849.2092\n-1678.418\n-1677.809\n-1639.230\n1.0685664-0.0000000i, -0.5815414+0.8262367i, -0.5815414-0.8262367i\n1.0000003-0.000000i, -0.6708443+0.861622i, -0.6708443-0.861622i, -0.4789438-1.295557i, -0.4789438+1.295557i\n\n\na400\n0.0003926\n828.4866\n-1644.973\n-1644.743\n-1621.460\n1.074908+0.622598i, -0.815559+1.008846i, -0.815559-1.008846i, 1.074908-0.622598i\n\n\n\na401\n0.0003936\n828.5076\n-1643.015\n-1642.707\n-1615.583\n1.079994+0.6285086i, -0.818089+0.9991499i, -0.818089-0.9991499i, 1.079994-0.6285086i\n-39.24509+0i\n\n\na402\n0.0003644\n841.5138\n-1667.028\n-1666.631\n-1635.677\n-0.5762646+0.8613026i, -0.5762646-0.8613026i, 2.2627735-0.8408307i, 2.2627735+0.8408307i\n-0.5154592+0.9948076i, -0.5154592-0.9948076i\n\n\na403\n0.0003546\n846.4790\n-1674.958\n-1674.461\n-1639.688\n1.192965+0.5326171i, -0.576099+0.8530305i, -0.576099-0.8530305i, 1.192965-0.5326171i\n-0.5438287+0.9807507i, -0.5438287-0.9807507i, 1.5196159+0.0000000i\n\n\na404\n0.0003349\n854.9148\n-1689.830\n-1689.220\n-1650.641\n1.0067552+0.5651317i, -0.5776288+0.8165889i, -0.5776288-0.8165889i, 1.0067552-0.5651317i\n1.2962822+0.9424546i, -0.5840364+0.8220893i, -0.5840364-0.8220893i, 1.2962822-0.9424546i\n\n\na405\n0.0003282\n859.1482\n-1696.296\n-1695.563\n-1653.189\n0.8764700+0.5283505i, -0.5780574+0.8170427i, -0.5780574-0.8170427i, 0.8764700-0.5283505i\n0.8854442+0.6218356i, -0.5844324+0.8275889i, -0.5844324-0.8275889i, 0.8854442-0.6218356i, -3.5686492+0.0000000i\n\n\na500\n0.0003936\n828.5134\n-1643.027\n-1642.719\n-1615.595\n1.0816019+0.630746i, -0.8181991+0.996680i, -0.8181991-0.996680i, 1.0816019-0.630746i, 30.0866777+0.000000i\n\n\n\na501\n0.0003943\n828.6939\n-1641.388\n-1640.991\n-1610.037\n1.063879+0.614274i, -0.782909+1.024026i, -0.782909-1.024026i, 1.063879-0.614274i, -1.633662-0.000000i\n-1.791808+0i\n\n\na502\n0.0003465\n849.6644\n-1681.329\n-1680.832\n-1646.059\n1.3231535+0.8089297i, -0.5790055+0.8187047i, -0.5790055-0.8187047i, 1.3231535-0.8089297i, -1.8902606+0.0000000i\n-0.6099102+0.8427657i, -0.6099102-0.8427657i\n\n\na503\n0.0003371\n853.7740\n-1687.548\n-1686.939\n-1648.359\n1.1212944+0.5571492i, -0.5777218+0.8165442i, -0.5777218-0.8165442i, 1.1212944-0.5571492i, -2.8445945-0.0000000i\n-0.5853707+0.822848i, -0.5853707-0.822848i, 1.8347464-0.000000i\n\n\na504\n0.0003174\n863.4734\n-1704.947\n-1704.214\n-1661.839\n0.8683242+0.5235239i, -0.5775260+0.8164740i, -0.5775260-0.8164740i, 0.8683242-0.5235239i, 2.5819016+0.0000000i\n0.8814016+0.5774634i, -0.5799805+0.8211971i, -0.5799805-0.8211971i, 0.8814016-0.5774634i\n\n\na505\n0.0003172\n864.3575\n-1704.715\n-1703.846\n-1657.688\n0.8686868+0.5163216i, -0.5776237+0.8164885i, -0.5776237-0.8164885i, 0.8686868-0.5163216i, 1.6856268-0.0000000i\n0.8977736+0.5532624i, -0.5820940+0.8217810i, -0.5820940-0.8217810i, 0.8977736-0.5532624i, 4.7797944+0.0000000i\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nmodel_resid |&gt;\n  select(a504) |&gt;\n  residuals() |&gt;\n  ACF() |&gt; \n  autoplot(var = .resid)\n\n\n\n\n\n\n\n\nFigure 12: Correlogram of the residuals from the ARMA model fitted to the residuals from the regression model\n\n\n\n\n\n\n\nShow the code\nmodel_resid |&gt;\n  select(a504) |&gt;\n  residuals() |&gt;\n  PACF() |&gt; \n  autoplot(var = .resid)\n\n\n\n\n\n\n\n\nFigure 13: Partial correlogram of the residuals from the ARMA model fitted to the residuals from the regression model",
    "crumbs": [
      "Lesson 2",
      "Autoregressive Moving Average (ARMA) Models"
    ]
  },
  {
    "objectID": "chapter_6_lesson_2.html#homework-preview-5-min",
    "href": "chapter_6_lesson_2.html#homework-preview-5-min",
    "title": "Autoregressive Moving Average (ARMA) Models",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_6_2.qmd",
    "crumbs": [
      "Lesson 2",
      "Autoregressive Moving Average (ARMA) Models"
    ]
  },
  {
    "objectID": "chapter_6.html",
    "href": "chapter_6.html",
    "title": "Lessons & Homework",
    "section": "",
    "text": "Chapter 6\n\n\n\n\n\n\nLessons\n\n\n\n\nLesson 1 - Moving Average (MA) Models\nLesson 2 - Autoregressive Moving Average (ARMA) Models\n\n\n\n\n\n\n\n\n\nDownload Homework Assignments\n\n\n\n\nUSE THIS DOWNLOAD (Winter 24)\n homework_6.qmd \nIGNORE THESE\n homework_6_1.qmd \n homework_6_2.qmd",
    "crumbs": [
      "Overview",
      "Lessons & Homework"
    ]
  },
  {
    "objectID": "chapter_5_lesson_4.html",
    "href": "chapter_5_lesson_4.html",
    "title": "Transformations, Forecasting adn Bias Correction",
    "section": "",
    "text": "Apply logarithmic transformations to time series\n\n\nExplain when to use a log-transformation\nEstimate a harmonic seasonal model using GLS with a log-transformed series\nExplain how to use logarithms to linearize certain non-linear trends\n\n\n\n\nApply non-linear models to time series\n\n\nExplain when to use non-linear models\nSimulate a time series with an exponential trend\nFit a time series model with an exponential trend",
    "crumbs": [
      "Lesson 4",
      "Transformations, Forecasting adn Bias Correction"
    ]
  },
  {
    "objectID": "chapter_5_lesson_4.html#learning-outcomes",
    "href": "chapter_5_lesson_4.html#learning-outcomes",
    "title": "Transformations, Forecasting adn Bias Correction",
    "section": "",
    "text": "Apply logarithmic transformations to time series\n\n\nExplain when to use a log-transformation\nEstimate a harmonic seasonal model using GLS with a log-transformed series\nExplain how to use logarithms to linearize certain non-linear trends\n\n\n\n\nApply non-linear models to time series\n\n\nExplain when to use non-linear models\nSimulate a time series with an exponential trend\nFit a time series model with an exponential trend",
    "crumbs": [
      "Lesson 4",
      "Transformations, Forecasting adn Bias Correction"
    ]
  },
  {
    "objectID": "chapter_5_lesson_4.html#preparation",
    "href": "chapter_5_lesson_4.html#preparation",
    "title": "Transformations, Forecasting adn Bias Correction",
    "section": "Preparation",
    "text": "Preparation\n\nRead Sections 5.7, 5.9, 5.11",
    "crumbs": [
      "Lesson 4",
      "Transformations, Forecasting adn Bias Correction"
    ]
  },
  {
    "objectID": "chapter_5_lesson_4.html#learning-journal-exchange-10-min",
    "href": "chapter_5_lesson_4.html#learning-journal-exchange-10-min",
    "title": "Transformations, Forecasting adn Bias Correction",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading another student’s?\nWhat would you recommend the other student add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 4",
      "Transformations, Forecasting adn Bias Correction"
    ]
  },
  {
    "objectID": "chapter_5_lesson_4.html#class-activity-simulate-an-exponential-trend-with-a-seasonal-component-15-min",
    "href": "chapter_5_lesson_4.html#class-activity-simulate-an-exponential-trend-with-a-seasonal-component-15-min",
    "title": "Transformations, Forecasting adn Bias Correction",
    "section": "Class Activity: Simulate an Exponential Trend with a Seasonal Component (15 min)",
    "text": "Class Activity: Simulate an Exponential Trend with a Seasonal Component (15 min)\nWe will simulate code that has a seasonal component and impose an exponential trend.\n\n\n\n\n\n\nFigures\n\n\n\n\n\n\nFigure 1 shows the simulated time series and the time series after the natural logarithm is applied.\n\nShow the code\nset.seed(12345)\n\nn_years &lt;- 9 # Number of years to simulate\nn_months &lt;- n_years * 12 # Number of months\nsigma &lt;- .05 # Standard deviation of random term\n\nz_t &lt;- rnorm(n = n_months, mean = 0, sd = sigma)\n\ndates_seq &lt;- seq(floor_date(now(), unit = \"year\"), length.out=n_months + 1, by=\"-1 month\") |&gt;\n  floor_date(unit = \"month\") |&gt; sort() |&gt; head(n_months)\n\nsim_ts &lt;- tibble(\n  t = 1:n_months,\n  dates = dates_seq,\n  random = arima.sim(model=list(ar=c(.5,0.2)), n = n_months, sd = 0.02),\n  x_t = exp(2 + 0.015 * t +\n      0.03 * sin(2 * pi * 1 * t / 12) + 0.04 * cos(2 * pi * 1 * t / 12) + \n      0.05 * sin(2 * pi * 2 * t / 12) + 0.03 * cos(2 * pi * 2 * t / 12) +\n      0.01 * sin(2 * pi * 3 * t / 12) + 0.005 * cos(2 * pi * 3 * t / 12) +\n      random\n    )\n  ) |&gt;\n  mutate(\n    cos1 = cos(2 * pi * 1 * t / 12),\n    cos2 = cos(2 * pi * 2 * t / 12),\n    cos3 = cos(2 * pi * 3 * t / 12),\n    cos4 = cos(2 * pi * 4 * t / 12),\n    cos5 = cos(2 * pi * 5 * t / 12),\n    cos6 = cos(2 * pi * 6 * t / 12),\n    sin1 = sin(2 * pi * 1 * t / 12),\n    sin2 = sin(2 * pi * 2 * t / 12),\n    sin3 = sin(2 * pi * 3 * t / 12),\n    sin4 = sin(2 * pi * 4 * t / 12),\n    sin5 = sin(2 * pi * 5 * t / 12),\n    sin6 = sin(2 * pi * 6 * t / 12)) |&gt;\n  mutate(std_t = (t - mean(t)) / sd(t)) |&gt;\n  as_tsibble(index = dates)\n\nplot_raw &lt;- sim_ts |&gt;\n    autoplot(.vars = x_t) +\n    labs(\n      x = \"Month\",\n      y = \"x_t\",\n      title = \"Simulated Time Series\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\nplot_log &lt;- sim_ts |&gt;\n    autoplot(.vars = log(x_t)) +\n    labs(\n      x = \"Month\",\n      y = \"log(x_t)\",\n      title = \"Logarithm of Simulated Time Series\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\nplot_raw | plot_log\n\n\n\n\n\n\n\nFigure 1: Time plot of the time series (left) and the natural logarithm of the time series (right)\n\n\n\nWe will compute the (natural) logarithm of the time series values before fitting any linear models. So, our response variable will be \\(\\log(x_t)\\), rather than \\(x_t\\).\nEven though there is no visual evidence of curvature in the trend for the logarithm of the time series, we will start by fitting a model that allows for a cubic trend. (In practice, we would probably not fit this model. However, there are some things that will occur that are helpful to understand the underlying process.)\n\n\n\n\n\nCubicQuadraticLinear\n\n\n\n\n\n\n\n\nCubic Model\n\n\n\n\n\n\n\nCubic Model\nAfter taking the (natural) logarithm of \\(x_t\\), we fit a cubic model to the log-transformed time series.\n\nFull Cubic Model\n\\[\\begin{align*}\n  \\log(x_t) &= \\beta_0\n            + \\beta_1 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right)\n            + \\beta_2 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right)^2\n            + \\beta_3 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right)^3 \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_4 \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + \\beta_5 \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_6 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + \\beta_7 \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_8 \\sin \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right)\n            + \\beta_9 \\cos \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_{10} \\sin \\left( \\frac{2\\pi \\cdot 4 t}{12} \\right)\n            + \\beta_{11} \\cos \\left( \\frac{2\\pi \\cdot 4 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_{12} \\sin \\left( \\frac{2\\pi \\cdot 5 t}{12} \\right)\n            + \\beta_{13} \\cos \\left( \\frac{2\\pi \\cdot 5 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            \\phantom{+ \\beta_{15} \\sin \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right)}\n            + \\beta_{14} \\cos \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right)\n      + z_t\n\\end{align*}\\]\n\n\nShow the code\n# Cubic model with standardized time variable\n\nfull_cubic_lm &lt;- sim_ts |&gt;\n  model(full_cubic = TSLM(log(x_t) ~ std_t + I(std_t^2) + I(std_t^3) +\n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n    + sin4 + cos4 + sin5 + cos5 + cos6 )) # Note sin6 is omitted\nfull_cubic_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 15 × 7\n   .model     term         estimate std.error statistic   p.value sig  \n   &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 full_cubic (Intercept)  2.82       0.00327   865.    2.08e-183 TRUE \n 2 full_cubic std_t        0.473      0.00550    86.0   1.90e- 90 TRUE \n 3 full_cubic I(std_t^2)  -0.00488    0.00246    -1.98  5.02e-  2 FALSE\n 4 full_cubic I(std_t^3)   0.000860   0.00285     0.302 7.64e-  1 FALSE\n 5 full_cubic sin1         0.0328     0.00312    10.5   1.68e- 17 TRUE \n 6 full_cubic cos1         0.0389     0.00308    12.6   7.52e- 22 TRUE \n 7 full_cubic sin2         0.0496     0.00309    16.1   1.33e- 28 TRUE \n 8 full_cubic cos2         0.0297     0.00308     9.65  1.10e- 15 TRUE \n 9 full_cubic sin3         0.0111     0.00308     3.60  5.15e-  4 TRUE \n10 full_cubic cos3         0.00765    0.00308     2.48  1.49e-  2 TRUE \n11 full_cubic sin4         0.00155    0.00308     0.504 6.15e-  1 FALSE\n12 full_cubic cos4         0.00185    0.00308     0.600 5.50e-  1 FALSE\n13 full_cubic sin5         0.00169    0.00308     0.550 5.84e-  1 FALSE\n14 full_cubic cos5         0.00186    0.00308     0.602 5.48e-  1 FALSE\n15 full_cubic cos6         0.00184    0.00218     0.845 4.00e-  1 FALSE\n\n\nNote that neither the quadratic nor the cubic terms are statistically significant in this model. We will eliminate the cubic term and fit a model with a quadratic trend.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuadratic Model\n\n\n\n\n\n\n\nQuadratic Model\nWe now fit a quadratic model to the log-transformed time series.\n\nFull Quadratic Model\nThe full model with a quadratic trend is written as:\n\\[\\begin{align*}\n  \\log(x_t) &= \\beta_0\n            + \\beta_1 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right)\n            + \\beta_2 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right)^2 \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_3 \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + \\beta_4 \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_5 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + \\beta_6 \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_7 \\sin \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right)\n            + \\beta_8 \\cos \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_9 \\sin \\left( \\frac{2\\pi \\cdot 4 t}{12} \\right)\n            + \\beta_{10} \\cos \\left( \\frac{2\\pi \\cdot 4 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_{11} \\sin \\left( \\frac{2\\pi \\cdot 5 t}{12} \\right)\n            + \\beta_{12} \\cos \\left( \\frac{2\\pi \\cdot 5 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n           \\phantom{+ \\beta_{14} \\sin \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right)}\n            + \\beta_{13} \\cos \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right)\n      + z_t\n\\end{align*}\\]\n\n\nShow the code\nfull_quad_lm &lt;- sim_ts |&gt;\n  model(full_quad = TSLM(log(x_t) ~ std_t + I(std_t^2) + \n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n    + sin4 + cos4 + sin5 + cos5 + cos6 )) # Note sin6 is omitted\nfull_quad_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 14 × 7\n   .model    term        estimate std.error statistic   p.value sig  \n   &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 full_quad (Intercept)  2.82      0.00325   869.    2.41e-185 TRUE \n 2 full_quad std_t        0.475     0.00219   217.    1.02e-128 TRUE \n 3 full_quad I(std_t^2)  -0.00488   0.00244    -1.99  4.90e-  2 TRUE \n 4 full_quad sin1         0.0326    0.00307    10.6   9.09e- 18 TRUE \n 5 full_quad cos1         0.0389    0.00306    12.7   4.29e- 22 TRUE \n 6 full_quad sin2         0.0496    0.00307    16.2   6.71e- 29 TRUE \n 7 full_quad cos2         0.0298    0.00306     9.72  7.30e- 16 TRUE \n 8 full_quad sin3         0.0111    0.00306     3.61  4.97e-  4 TRUE \n 9 full_quad cos3         0.00768   0.00306     2.51  1.39e-  2 TRUE \n10 full_quad sin4         0.00153   0.00306     0.500 6.18e-  1 FALSE\n11 full_quad cos4         0.00188   0.00306     0.614 5.41e-  1 FALSE\n12 full_quad sin5         0.00168   0.00306     0.550 5.84e-  1 FALSE\n13 full_quad cos5         0.00189   0.00306     0.616 5.39e-  1 FALSE\n14 full_quad cos6         0.00186   0.00217     0.857 3.93e-  1 FALSE\n\n\nNow, note that the quadratic term is statistically significant. It was not significant when the cubic term was included in the model, but it is now.\n\n\nReduced Quadratic Trend: Model 1\nThis model omits all the Fourier (sine and cosine) terms that are not significant in the previous model.\n\n\nShow the code\nreduced_quadratic_lm1 &lt;- sim_ts |&gt;\n  model(reduced_quadratic1 = TSLM(log(x_t) ~ std_t + I(std_t^2) + \n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3))\nreduced_quadratic_lm1 |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 9 × 7\n  .model             term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;              &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_quadratic1 (Intercept)  2.82      0.00320    882.   1.18e-194 TRUE \n2 reduced_quadratic1 std_t        0.475     0.00216    220.   4.66e-135 TRUE \n3 reduced_quadratic1 I(std_t^2)  -0.00487   0.00241     -2.02 4.57e-  2 TRUE \n4 reduced_quadratic1 sin1         0.0326    0.00303     10.8  2.18e- 18 TRUE \n5 reduced_quadratic1 cos1         0.0389    0.00302     12.9  6.79e- 23 TRUE \n6 reduced_quadratic1 sin2         0.0496    0.00302     16.4  5.12e- 30 TRUE \n7 reduced_quadratic1 cos2         0.0298    0.00302      9.87 2.15e- 16 TRUE \n8 reduced_quadratic1 sin3         0.0111    0.00302      3.66 4.02e-  4 TRUE \n9 reduced_quadratic1 cos3         0.00768   0.00302      2.54 1.25e-  2 TRUE \n\n\nAll the terms are statistically significant in this model.\n\n\nReduced Quadratic Trend: Model 2\nThis model only includes the Fourier series terms where \\(i=1\\) or \\(i=2\\).\n\n\nShow the code\nreduced_quadratic_lm2 &lt;- sim_ts |&gt;\n  model(reduced_quadratic2 = TSLM(log(x_t) ~ std_t + I(std_t^2) + \n    sin1 + cos1 + sin2 + cos2))\nreduced_quadratic_lm2 |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 7 × 7\n  .model             term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;              &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_quadratic2 (Intercept)  2.82      0.00347    813.   1.54e-194 TRUE \n2 reduced_quadratic2 std_t        0.475     0.00234    203.   9.18e-134 TRUE \n3 reduced_quadratic2 I(std_t^2)  -0.00486   0.00261     -1.86 6.57e-  2 FALSE\n4 reduced_quadratic2 sin1         0.0326    0.00329      9.93 1.25e- 16 TRUE \n5 reduced_quadratic2 cos1         0.0389    0.00327     11.9  6.98e- 21 TRUE \n6 reduced_quadratic2 sin2         0.0496    0.00328     15.1  1.03e- 27 TRUE \n7 reduced_quadratic2 cos2         0.0298    0.00327      9.09 8.89e- 15 TRUE \n\n\nAs we would expect, all the terms are statistically significant. (They were all significant in the previous model, so it is not surprising that they are still significant.)\n\n\nReduced Quadratic Trend: Model 3\nThis model is reduced to include only the Fourier series terms for \\(i=1\\).\n\n\nShow the code\nreduced_quadratic_lm3 &lt;- sim_ts |&gt;\n  model(reduced_quadratic3 = TSLM(log(x_t) ~ std_t + I(std_t^2) + \n    sin1 + cos1)) \nreduced_quadratic_lm3 |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 5 × 7\n  .model             term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;              &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_quadratic3 (Intercept)  2.82      0.00695   406.    7.13e-167 TRUE \n2 reduced_quadratic3 std_t        0.474     0.00467   101.    5.16e-105 TRUE \n3 reduced_quadratic3 I(std_t^2)  -0.00475   0.00523    -0.908 3.66e-  1 FALSE\n4 reduced_quadratic3 sin1         0.0325    0.00658     4.95  2.97e-  6 TRUE \n5 reduced_quadratic3 cos1         0.0389    0.00656     5.94  3.97e-  8 TRUE \n\n\nAll the terms in this parsimonious model are statistically significant.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Model\n\n\n\n\n\n\n\nLinear Model\nEven though the quadratic terms were statistically significant, there is no visual indication that there is a quadratic trend in the time series after taking the logarithm. Hence, we will now fit a linear model to the log-transformed time series. We want to be able to compare the fit of models with a linear trend to the models with quadratic trends.\n\nFull Linear Model\nFirst, we fit a full model with a linear trend. We can express this model as:\n\\[\\begin{align*}\n  \\log(x_t) &= \\beta_0\n            + \\beta_1 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_2 \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + \\beta_3 \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_4 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + \\beta_5 \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_6 \\sin \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right)\n            + \\beta_7 \\cos \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_8 \\sin \\left( \\frac{2\\pi \\cdot 4 t}{12} \\right)\n            + \\beta_9 \\cos \\left( \\frac{2\\pi \\cdot 4 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_{10} \\sin \\left( \\frac{2\\pi \\cdot 5 t}{12} \\right)\n            + \\beta_{11} \\cos \\left( \\frac{2\\pi \\cdot 5 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            \\phantom{+ \\beta_{13} \\sin \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right)}\n            + \\beta_{12} \\cos \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right)\n      + z_t\n\\end{align*}\\]\n\n\nShow the code\nfull_linear_lm &lt;- sim_ts |&gt;\n  model(full_linear = TSLM(log(x_t) ~ std_t + \n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n    + sin4 + cos4 + sin5 + cos5 + cos6 )) # Note sin6 is omitted\nfull_linear_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 13 × 7\n   .model      term        estimate std.error statistic   p.value sig  \n   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 full_linear (Intercept)  2.82      0.00220  1281.    4.18e-203 TRUE \n 2 full_linear std_t        0.475     0.00222   214.    3.22e-129 TRUE \n 3 full_linear sin1         0.0326    0.00312    10.4   1.83e- 17 TRUE \n 4 full_linear cos1         0.0388    0.00311    12.5   9.93e- 22 TRUE \n 5 full_linear sin2         0.0496    0.00311    15.9   1.47e- 28 TRUE \n 6 full_linear cos2         0.0298    0.00311     9.56  1.41e- 15 TRUE \n 7 full_linear sin3         0.0110    0.00311     3.55  5.99e-  4 TRUE \n 8 full_linear cos3         0.00767   0.00311     2.47  1.54e-  2 TRUE \n 9 full_linear sin4         0.00153   0.00311     0.492 6.24e-  1 FALSE\n10 full_linear cos4         0.00188   0.00311     0.604 5.47e-  1 FALSE\n11 full_linear sin5         0.00168   0.00311     0.541 5.90e-  1 FALSE\n12 full_linear cos5         0.00189   0.00311     0.607 5.45e-  1 FALSE\n13 full_linear cos6         0.00186   0.00220     0.844 4.01e-  1 FALSE\n\n\n\n\nReduced Linear Trend: Model 1\nThis model excludes the terms that were not significant in the full model with a linear trend.\n\n\nShow the code\nreduced_linear_lm1 &lt;- sim_ts |&gt;\n  model(reduced_linear1 = TSLM(log(x_t) ~ std_t + \n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3))\nreduced_linear_lm1 |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 8 × 7\n  .model          term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;           &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_linear1 (Intercept)  2.82      0.00217   1301.   2.91e-213 TRUE \n2 reduced_linear1 std_t        0.475     0.00219    217.   1.58e-135 TRUE \n3 reduced_linear1 sin1         0.0326    0.00307     10.6  4.53e- 18 TRUE \n4 reduced_linear1 cos1         0.0388    0.00306     12.7  1.63e- 22 TRUE \n5 reduced_linear1 sin2         0.0496    0.00307     16.2  1.18e- 29 TRUE \n6 reduced_linear1 cos2         0.0298    0.00306      9.71 4.25e- 16 TRUE \n7 reduced_linear1 sin3         0.0111    0.00306      3.61 4.86e-  4 TRUE \n8 reduced_linear1 cos3         0.00767   0.00306      2.50 1.39e-  2 TRUE \n\n\nAll of the terms are significant in this model.\n\n\nReduced Linear Trend: Model 2\nWe reduce the model to see if a more parsimonious model will suffice. This model contains a linear trend and the Fourier series terms associated with \\(i=1\\) and \\(i=2\\).\n\n\nShow the code\nreduced_linear_lm2 &lt;- sim_ts |&gt;\n  model(reduced_linear2 = TSLM(log(x_t) ~ std_t + \n    sin1 + cos1 + sin2 + cos2))\nreduced_linear_lm2 |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 6 × 7\n  .model          term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;           &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_linear2 (Intercept)   2.82     0.00234   1203.   1.37e-213 TRUE \n2 reduced_linear2 std_t         0.475    0.00237    201.   2.51e-134 TRUE \n3 reduced_linear2 sin1          0.0326   0.00332      9.81 2.14e- 16 TRUE \n4 reduced_linear2 cos1          0.0388   0.00331     11.7  1.35e- 20 TRUE \n5 reduced_linear2 sin2          0.0496   0.00332     14.9  1.88e- 27 TRUE \n6 reduced_linear2 cos2          0.0298   0.00331      8.98 1.46e- 14 TRUE \n\n\nAll the terms in this model are statistically significant.\n\n\nReduced Linear Trend: Model 3\nFinally, we fit a more reduced model that includes a linear trend and only the Fourier terms associated with \\(i=1\\).\n\n\nShow the code\nreduced_linear_lm3 &lt;- sim_ts |&gt;\n  model(reduced_linear3 = TSLM(log(x_t) ~ std_t + \n    sin1 + cos1)) \nreduced_linear_lm3 |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 4 × 7\n  .model          term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;           &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_linear3 (Intercept)   2.82     0.00463    609.   1.56e-186 TRUE \n2 reduced_linear3 std_t         0.474    0.00467    101.   7.71e-106 TRUE \n3 reduced_linear3 sin1          0.0325   0.00657      4.95 2.92e-  6 TRUE \n4 reduced_linear3 cos1          0.0389   0.00655      5.93 3.98e-  8 TRUE \n\n\nEach of these terms is significant.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Comparison\n\n\n\n\n\n\n\nComparison of Fitted Models\n\nAIC, AICc, and BIC\nWe will now compare the models we fitted above. #tbl-ModelComparison gives the AIC, AICc, and BIC of the models fitted above.\n\n\nShow the code\nmodel_combined &lt;- sim_ts |&gt;\n  model(\n    full_cubic = TSLM(log(x_t) ~ std_t + I(std_t^2) + I(std_t^3) +\n                    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n                  + sin4 + cos4 + sin5 + cos5 + cos6 ),\n    full_quad = TSLM(log(x_t) ~ std_t + I(std_t^2) + \n                       sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n                     + sin4 + cos4 + sin5 + cos5 + cos6 ),\n    reduced_quadratic1 = TSLM(log(x_t) ~ std_t + I(std_t^2) + sin1 + cos1 + sin2 + cos2 + sin3 + cos3),\n    reduced_quadratic2 = TSLM(log(x_t) ~ std_t + I(std_t^2) + sin1 + cos1 + sin2 + cos2),\n    reduced_quadratic3 = TSLM(log(x_t) ~ std_t + I(std_t^2) + sin1 + cos1),\n    full_linear = TSLM(log(x_t) ~ std_t + \n                         sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n                       + sin4 + cos4 + sin5 + cos5 + cos6 ),\n    reduced_linear1 = TSLM(log(x_t) ~ std_t + sin1 + cos1 + sin2 + cos2 + sin3 + cos3),\n    reduced_linear2 = TSLM(log(x_t) ~ std_t + sin1 + cos1 + sin2 + cos2),\n    reduced_linear3 = TSLM(log(x_t) ~ std_t + sin1 + cos1) \n  )\n\nglance(model_combined) |&gt;\n  select(.model, AIC, AICc, BIC)\n\n\n\n\n\n\nTable 1: Comparison of the AIC, AICc, and BIC values for the models fitted to the logarithm of the simulated time series.\n\n\n\n\n\n\nModel\nAIC\nAICc\nBIC\n\n\n\n\nfull_cubic\n-802.6\n-796.6\n-759.6\n\n\nfull_quad\n-804.5\n-799.2\n-764.2\n\n\nreduced_quadratic1\n**-812.1**\n**-809.9**\n-785.3\n\n\nreduced_quadratic2\n-796.3\n-794.9\n-774.9\n\n\nreduced_quadratic3\n-648.3\n-647.4\n-632.2\n\n\nfull_linear\n-802\n-797.5\n-764.4\n\n\nreduced_linear1\n-809.8\n-807.9\n**-785.6**\n\n\nreduced_linear2\n-794.7\n-793.6\n-775.9\n\n\nreduced_linear3\n-649.4\n-648.8\n-636\n\n\n\n\n\n\n\n\n\n\nThe model with the lowest AIC and AICc values is the reduced quadratic trend model 1. This can be written as:\n\\[\\begin{align*}\n  \\log(x_t) &= \\beta_0\n            + \\beta_1 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right)\n            + \\beta_2 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right)^2 \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_3 \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + \\beta_4 \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_5 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + \\beta_6 \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_7 \\sin \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right)\n            + \\beta_8 \\cos \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right)\n      + z_t\n\\end{align*}\\]\nThe model with the lowest BIC is the reduced linear trend model 1. We express this model as:\n\\[\\begin{align*}\n  \\log(x_t) &= \\beta_0\n            + \\beta_1 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_2 \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + \\beta_3 \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_4 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + \\beta_5 \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_6 \\sin \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right)\n            + \\beta_7 \\cos \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right)\n      + z_t\n\\end{align*}\\]\nBoth of these models have very low AIC, AICc, and BIC values.\nWe will take a deeper look at the residuals of these two models to assess if there is evidence of autocorrelation in the random terms. We compute the autocorrelation function and the partial autocorrelation function of the residuals for both. If there is evidence of autocorrelation, we will use the GLS algorithm to fit the models, since it will take into account the autocorrelation in the terms.\n\n\n\n\n\n\n\n\n\n\n\n\nAutocorrelation of the Random Component\n\n\n\n\n\n\n\nInvestigating Autocorrelation of the Random Component\nRecall that if there is autocorrelation in the random component, the standard error of the parameter estimates tends to be underestimated. We can account for this autocorrelation using Generalized Least Squares, GLS, if needed.\n\nReduced Quadratic Trend: Model 1\nFigure 2 illustrates the ACF of the reduced model 1 with quadratic trend.\n\n\nShow the code\nresid_q1_ts &lt;- reduced_quadratic_lm1 |&gt;\n  residuals() \n\nacf(resid_q1_ts$.resid, plot=TRUE, lag.max = 25)\n\n\n\n\n\n\n\n\nFigure 2: ACF of reduced model 1 with a quadratic trend\n\n\n\n\n\nNotice that the residual correlogram indicates a positive autocorrelation in the values. This suggests that the standard errors of the regression coefficients will be underestimated, which means that some predictors can appear to be statistically significant when they are not.\nFigure 3 illustrates the PACF of the reduced model 1 with quadratic trend.\n\n\nShow the code\npacf(resid_q1_ts$.resid, plot=TRUE, lag.max = 25)\n\n\n\n\n\n\n\n\nFigure 3: PACF of reduced model 1 with a quadratic trend\n\n\n\n\n\nOnly the first partial autocorrelation is statistically significant. The partial autocorrelation plot indicates that an \\(AR(1)\\) model could adequately model the random component of the logarithm of the time series. Recall that in Chapter 5, Lesson 1, we fitted a linear regression model using the value of the partial autocorrelation function for \\(k=1\\). This helps account for the autocorrelation in the residuals.\nThe first few partial autocorrelation values are:\n\n\nShow the code\npacf(resid_q1_ts$.resid, plot=FALSE, lag.max = 10)\n\n\n\nPartial autocorrelations of series 'resid_q1_ts$.resid', by lag\n\n     1      2      3      4      5      6      7      8      9     10 \n 0.479  0.091 -0.004 -0.101 -0.018 -0.129 -0.051  0.016  0.037  0.069 \n\n\nThe partial autocorrelation when \\(k=1\\) is approximately 0.479. We will use this value as we recompute the regression coefficients.\n\n\nShow the code\n# Load additional packages\npacman::p_load(tidymodels, multilevelmod,\n  nlme, broom.mixed)\n\ntemp_spec &lt;- linear_reg() |&gt;\n  set_engine(\"gls\", correlation = nlme::corAR1(0.479))\n\ntemp_gls &lt;- temp_spec |&gt;\n  fit(log(x_t) ~ std_t + I(std_t^2) + sin1 + cos1 + sin2 + cos2 + sin3 + cos3, data = sim_ts)\n\ntidy(temp_gls) |&gt;\n  mutate(\n    lower = estimate + qnorm(0.025) * std.error,\n    upper = estimate + qnorm(0.975) * std.error\n  ) \n\n\n# A tibble: 9 × 7\n  term        estimate std.error statistic   p.value    lower   upper\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)  2.82      0.00590    479.   2.33e-168  2.81    2.84   \n2 std_t        0.475     0.00389    122.   9.61e-110  0.467   0.482  \n3 I(std_t^2)  -0.00497   0.00427     -1.16 2.48e-  1 -0.0133  0.00341\n4 sin1         0.0325    0.00442      7.36 5.38e- 11  0.0239  0.0412 \n5 cos1         0.0389    0.00436      8.92 2.45e- 14  0.0303  0.0474 \n6 sin2         0.0495    0.00307     16.1  1.99e- 29  0.0435  0.0555 \n7 cos2         0.0298    0.00306      9.75 3.77e- 16  0.0238  0.0358 \n8 sin3         0.0110    0.00235      4.68 9.18e-  6  0.00639 0.0156 \n9 cos3         0.00773   0.00235      3.29 1.40e-  3  0.00312 0.0123 \n\n\nThe quadratic term is not statistically significant in this model! After accounting for the autocorrelation in the random component, the quadratic component of the trend is not statistically significant. This is a great example of an instance where ordinary linear regression leads to errant results.\nWe now consider the reduced model 1 where the trend is linear.\n\n\nReduced Linear Trend: Model 1\nFigure 4 illustrates the ACF of the reduced model 1 with linear trend.\n\nresid_lin1_ts &lt;- reduced_linear_lm1 |&gt;\n  residuals() \n\nacf(resid_lin1_ts$.resid, plot=TRUE, lag.max = 25)\n\n\n\n\n\n\n\nFigure 4: ACF of reduced model 1 with a linear trend\n\n\n\n\n\nWe observe evidence of autocorrelation in the random terms.\nFigure 5 illustrates the PACF of the reduced model 1 with linear trend.\n\n\nShow the code\nalphas_lin &lt;- pacf(resid_lin1_ts$.resid, plot=FALSE, lag.max = 25) \npacf(resid_lin1_ts$.resid, plot=TRUE, lag.max = 25)\n\n\n\n\n\n\n\n\nFigure 5: ACF of reduced model 1 with a linear trend\n\n\n\n\n\nWe will use the PACF when \\(k=1\\) to apply the GLS algorithm. The first few partial autocorrelation values are:\n\n\nShow the code\npacf(resid_lin1_ts$.resid, plot=FALSE, lag.max = 10)\n\n\n\nPartial autocorrelations of series 'resid_lin1_ts$.resid', by lag\n\n     1      2      3      4      5      6      7      8      9     10 \n 0.497  0.122  0.017 -0.088  0.006 -0.100 -0.018  0.042  0.060  0.087 \n\n\nThe partial autocorrelation when \\(k=1\\) is approximately 0.497. We will use this value as we recompute the regression coefficients.\n\n\nShow the code\n# Load additional packages\npacman::p_load(tidymodels, multilevelmod,\n  nlme, broom.mixed)\n\ntemp_spec &lt;- linear_reg() |&gt;\n  set_engine(\"gls\", correlation = nlme::corAR1(0.497))\n\ntemp_gls &lt;- temp_spec |&gt;\n  fit(log(x_t) ~ std_t + sin1 + cos1 + sin2 + cos2 + sin3 + cos3, data = sim_ts)\n\ntidy(temp_gls) |&gt;\n  mutate(\n    lower = estimate + qnorm(0.025) * std.error,\n    upper = estimate + qnorm(0.975) * std.error\n  ) \n\n\n# A tibble: 8 × 7\n  term        estimate std.error statistic   p.value   lower  upper\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;\n1 (Intercept)  2.82      0.00398    708.   8.15e-187 2.81    2.83  \n2 std_t        0.475     0.00393    121.   3.80e-110 0.467   0.482 \n3 sin1         0.0324    0.00444      7.30 6.96e- 11 0.0237  0.0412\n4 cos1         0.0386    0.00438      8.82 3.85e- 14 0.0300  0.0472\n5 sin2         0.0494    0.00308     16.1  1.83e- 29 0.0434  0.0555\n6 cos2         0.0297    0.00306      9.71 4.31e- 16 0.0237  0.0357\n7 sin3         0.0110    0.00235      4.66 9.69e-  6 0.00635 0.0156\n8 cos3         0.00769   0.00235      3.27 1.47e-  3 0.00308 0.0123\n\n\nFigure 6 illustrates the original time series (in black) and the fitted model (in blue). For reference, a dotted line illustrating the simple least squares line is plotted on this figure for reference. It helps highlight the exponential shape of the trend.\n\n\nShow the code\nforecast_df &lt;- reduced_linear_lm1 |&gt; \n  forecast(sim_ts) |&gt; # computes the anti-log of the predicted values and returns them as .mean\n  as_tibble() |&gt; \n  dplyr::select(std_t, .mean) |&gt; \n  rename(pred = .mean)\n\nsim_ts |&gt;\n  left_join(forecast_df, by = \"std_t\") |&gt;\n  as_tsibble(index = dates) |&gt;\n  autoplot(.vars = x_t) +\n  geom_smooth(method = \"lm\", formula = 'y ~ x', se = FALSE, color = \"#E69F00\", linewidth = 0.5, linetype = \"dotted\") +\n  geom_line(aes(y = pred), color = \"#56B4E9\", alpha = 0.75) +\n    labs(\n      x = \"Month\",\n      y = \"Simulated Time Series\",\n      title = \"Time Plot of Simulated Time Series with an Exponential Trend\",\n      subtitle = \"Predicted values based on the full cubic model are given in blue\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5),\n      plot.subtitle = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\nFigure 6: Time plot of the time series and the fitted linear regression model\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of the Fitted Coefficients to the Simulation Parameters\n\n\n\n\n\n\n\nComparison of Model Coefficients\nNote that the mean of the \\(x_t\\) values is \\(\\bar x_t = 18.699\\), and the standard deviation is \\(s_t = 8.691\\).\nThe fitted model is: \\[\\begin{align*}\n  \\hat x_t\n      &= e^{\n          2.819\n          + 0.475\n                ~ \\left( \\frac{t - 18.699}{8.691} \\right)\n          + 0.032\n                ~ \\sin \\left( \\frac{2 \\pi \\cdot 1 t }{ 12 } \\right)\n          + 0.039\n                ~ \\cos \\left( \\frac{2 \\pi \\cdot 1 t }{ 12 } \\right)\n          + 0.049\n                ~ \\sin \\left( \\frac{2 \\pi \\cdot 2 t }{ 12 } \\right)\n          + 0.03\n                ~ \\cos \\left( \\frac{2 \\pi \\cdot 2 t }{ 12 } \\right)\n          + 0.011\n                ~ \\sin \\left( \\frac{2 \\pi \\cdot 3 t }{ 12 } \\right)\n          + 0.008\n                ~ \\cos \\left( \\frac{2 \\pi \\cdot 3 t }{ 12 } \\right)\n     }\n     \\\\\n     &= e^{\n          2.819\n          + 0.475\n                ~ \\left( \\frac{t}{8.691} \\right)\n          - 0.475\n                ~ \\left( \\frac{18.699}{8.691} \\right)\n          + 0.032\n                ~ \\sin \\left( \\frac{2 \\pi \\cdot 1 t }{ 12 } \\right)\n          + 0.039\n                ~ \\cos \\left( \\frac{2 \\pi \\cdot 1 t }{ 12 } \\right)\n          + 0.049\n                ~ \\sin \\left( \\frac{2 \\pi \\cdot 2 t }{ 12 } \\right)\n          + 0.03\n                ~ \\cos \\left( \\frac{2 \\pi \\cdot 2 t }{ 12 } \\right)\n          + 0.011\n                ~ \\sin \\left( \\frac{2 \\pi \\cdot 3 t }{ 12 } \\right)\n          + 0.008\n                ~ \\cos \\left( \\frac{2 \\pi \\cdot 3 t }{ 12 } \\right)\n     }\n     \\\\\n     &= e^{\n          2.819\n          + 0.055 ~ t\n          - 1.021\n          + 0.032\n                ~ \\sin \\left( \\frac{2 \\pi \\cdot 1 t }{ 12 } \\right)\n          + 0.039\n                ~ \\cos \\left( \\frac{2 \\pi \\cdot 1 t }{ 12 } \\right)\n          + 0.049\n                ~ \\sin \\left( \\frac{2 \\pi \\cdot 2 t }{ 12 } \\right)\n          + 0.03\n                ~ \\cos \\left( \\frac{2 \\pi \\cdot 2 t }{ 12 } \\right)\n          + 0.011\n                ~ \\sin \\left( \\frac{2 \\pi \\cdot 3 t }{ 12 } \\right)\n          + 0.008\n                ~ \\cos \\left( \\frac{2 \\pi \\cdot 3 t }{ 12 } \\right)\n     }\n     \\\\\n     &= e^{\n          1.797\n          + 0.055 ~ t\n          + 0.032\n                ~ \\sin \\left( \\frac{2 \\pi \\cdot 1 t }{ 12 } \\right)\n          + 0.039\n                ~ \\cos \\left( \\frac{2 \\pi \\cdot 1 t }{ 12 } \\right)\n          + 0.049\n                ~ \\sin \\left( \\frac{2 \\pi \\cdot 2 t }{ 12 } \\right)\n          + 0.03\n                ~ \\cos \\left( \\frac{2 \\pi \\cdot 2 t }{ 12 } \\right)\n          + 0.011\n                ~ \\sin \\left( \\frac{2 \\pi \\cdot 3 t }{ 12 } \\right)\n          + 0.008\n                ~ \\cos \\left( \\frac{2 \\pi \\cdot 3 t }{ 12 } \\right)\n     }\n\\end{align*}\\]\nNotice how well this matches the original model used to simulate the data.\n\\[\\begin{align*}\n  x_t &= e^{\n          2 + 0.015 t +\n       0.03 ~ \\sin \\left( \\frac{2 \\pi \\cdot 1 t }{ 12 } \\right) + 0.04 ~ \\cos \\left( \\frac{2 \\pi \\cdot 1 t }{ 12 } \\right) +\n       0.05 ~ \\sin \\left( \\frac{2 \\pi \\cdot 2 t }{ 12 } \\right) + 0.03 ~ \\cos \\left( \\frac{2 \\pi \\cdot 2 t }{ 12 } \\right) +\n       0.01 ~ \\sin \\left( \\frac{2 \\pi \\cdot 3 t }{ 12 } \\right) + 0.005 ~ \\cos \\left( \\frac{2 \\pi \\cdot 3 t }{ 12 } \\right) +\n        z_t\n     }\n\\end{align*}\\] where \\(z_t = 0.5 z_{t-1} + 0.2 z_{t-2} + w_t\\) and \\(w_t\\) is a white noise process with standard deviation of 0.02.",
    "crumbs": [
      "Lesson 4",
      "Transformations, Forecasting adn Bias Correction"
    ]
  },
  {
    "objectID": "chapter_5_lesson_4.html#small-group-activity-retail-sales-20-min",
    "href": "chapter_5_lesson_4.html#small-group-activity-retail-sales-20-min",
    "title": "Transformations, Forecasting adn Bias Correction",
    "section": "Small-Group Activity: Retail Sales (20 min)",
    "text": "Small-Group Activity: Retail Sales (20 min)\nFigure 7 gives the total sales (in millions of U.S. dollars) for the category “all other general merchandise stores (45299).”\n\n\nShow the code\n# Read in retail sales data for \"all other general merchandise stores\"\nretail_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/retail_by_business_type.parquet\") |&gt;\n  filter(naics == 45299) |&gt;\n  filter(as_date(month) &gt;= my(\"Jan 1998\")) |&gt;\n  as_tsibble(index = month)\n\nretail_ts |&gt;\n  autoplot(.vars = sales_millions) +\n    labs(\n      x = \"Month\",\n      y = \"Sales (Millions of U.S. Dollars)\",\n      title = paste0(retail_ts$business[1], \" (\", retail_ts$naics[1], \")\")\n    ) +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFigure 7: Time plot of the total monthly retail sales for all other general merchandise stores (45299)\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUse Figure 7 to explain the following questions to a partner.\n\nWhat is the shape of the trend of this time series?\nWhich decomposition would be more appropriate: additive or multiplicative? Justify your answer.\nApply the appropriate transformation to the time series.\nFit appropriate models utilizing the Fourier terms for seasonal components.\nDetermine the “best” model for these data. Justify your decision.   \nPlot the fitted values and the time series on the same figure.\n\nDo the following to explore a cubic trend model with all the seasonal Fourier terms.\n\nFit a full model with a cubic trend and the logarithm of the time series for the response, including all the Fourier seasonal terms. (Be sure to include the linear and quadratic components of the trend.)\nFit a full model with a cubic trend and the logarithm of the time series for the response, but use indicator seasonal variables. (Be sure to include the linear and quadratic components of the trend.)\nCompare the coefficients on the linear, quadratic, and cubic terms between the two models above. Why would we observe this result?\nWhy does the coefficient on the intercept term differ in the two models?",
    "crumbs": [
      "Lesson 4",
      "Transformations, Forecasting adn Bias Correction"
    ]
  },
  {
    "objectID": "chapter_5_lesson_4.html#class-activity-simulated-non-linear-series-10-min",
    "href": "chapter_5_lesson_4.html#class-activity-simulated-non-linear-series-10-min",
    "title": "Transformations and Non-Linear Models",
    "section": "Class Activity: Simulated Non-Linear Series (10 min)",
    "text": "Class Activity: Simulated Non-Linear Series (10 min)\nIn Section 5.8 of the textbook, we are introduced to the possibility that a time series could have an exponential trend but also exhibit negative values. In this case, the logarithm of the negative values is not defined, so we need a different approach to fit the model.\nWe will fit the non-linear time series\n\\[\nx_t = -c_0 + e^{\\alpha_0 + \\alpha_1 t} + z_t\n\\]\nwhere the time series can assume negative values.\nWe present the simulation from the textbook here:\n\n\nShow the code\nset.seed(1)\ndat &lt;- tibble(w = rnorm(100, sd = 10)) |&gt;\n    mutate(\n        Time = 1:n(),\n         z = purrr::accumulate2(\n            lag(w), w, \n            \\(acc, nxt, w) 0.7 * acc + w,\n            .init = 0)[-1],\n          x = exp(1 + 0.05 * Time) + z) |&gt;\n    tsibble::as_tsibble(index = Time)\n\nautoplot(dat, .var = x) +\n  geom_hline(yintercept = 0) +\n  labs(\n    x = \"Time\",\n    y = \"Value\",\n    title = \"Time Plot of Simulated Non-Linear Series\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFigure 8: Time plot of a simulated non-linear series\n\n\n\n\n\nWe will fit the non-linear model given above.\n\nequation &lt;- x ~ exp(alp0 + alp1 * Time)\n\ndat_nls &lt;- nls(equation, data = dat,\n  start = list(alp0 = 0.1, alp1 = 0.5))\n\nsummary(dat_nls)$parameters\n\n       Estimate   Std. Error  t value     Pr(&gt;|t|)\nalp0 1.17299000 0.0743864844 15.76886 1.232132e-28\nalp1 0.04832158 0.0008197862 58.94413 2.432828e-78\n\n\nThe model we fitted was\n\\[\n  x_t = e^{1 + 0.05 t} + z_t\n\\]\nwhere\n\\[\n  z_t = 0.7 z_{t-1} + w_t\n\\] and \\(w_t\\) is a white noise process.\nOur fitted model is: \\[\n  \\hat x_t = e^{1.173 + 0.048 t}\n\\]\nNotice that our estimates are quite close to the parameters used in the simulation.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nModify the time series and repeat the simulation.\nFit your model using the nls function in R.\nCompare the parameter estimates with the actual values from your simulation.",
    "crumbs": [
      "Lesson 4",
      "Transformations and Non-Linear Models"
    ]
  },
  {
    "objectID": "chapter_5_lesson_4.html#homework-preview-5-min",
    "href": "chapter_5_lesson_4.html#homework-preview-5-min",
    "title": "Transformations, Forecasting adn Bias Correction",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_5_5.qmd \n\n\nSmall-Group Activity: Retail Sales\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUse the retail sales data to do the following.\n\nSelect an appropriate fitted model using the AIC, AICc, or BIC critera.\nUse the residuals to determine the appropriate correction for the data.\nForecast the data for the next 5 years.\nApply the appropriate correction to the forecasted values.\nPlot the fitted (forecasted) values along with the time series.\n\n\n\n\n\nShow the code\nretail_full_quad_lm &lt;- retail_ts |&gt;\n  model(retail_full_quad = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + \n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n    + sin4 + cos4 + sin5 + cos5 + cos6 )) # Note sin6 is omitted\nretail_full_quad_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 14 × 7\n   .model           term        estimate std.error statistic   p.value sig  \n   &lt;chr&gt;            &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 retail_full_quad (Intercept)   8.37     0.00352   2378.   0         TRUE \n 2 retail_full_quad std_t         0.369    0.00235    157.   2.37e-279 TRUE \n 3 retail_full_quad I(std_t^2)    0.0130   0.00263      4.94 1.30e-  6 TRUE \n 4 retail_full_quad sin1         -0.0231   0.00332     -6.96 2.31e- 11 TRUE \n 5 retail_full_quad cos1          0.0353   0.00332     10.7  1.54e- 22 TRUE \n 6 retail_full_quad sin2         -0.0692   0.00332    -20.9  2.28e- 59 TRUE \n 7 retail_full_quad cos2          0.0785   0.00332     23.7  2.33e- 69 TRUE \n 8 retail_full_quad sin3         -0.0434   0.00332    -13.1  5.77e- 31 TRUE \n 9 retail_full_quad cos3          0.0699   0.00332     21.1  3.96e- 60 TRUE \n10 retail_full_quad sin4         -0.0288   0.00332     -8.69 2.83e- 16 TRUE \n11 retail_full_quad cos4          0.0627   0.00332     18.9  2.84e- 52 TRUE \n12 retail_full_quad sin5          0.0118   0.00332      3.57 4.25e-  4 TRUE \n13 retail_full_quad cos5          0.0636   0.00332     19.2  2.81e- 53 TRUE \n14 retail_full_quad cos6          0.0251   0.00235     10.7  1.04e- 22 TRUE \n\n\nShow the code\nretail_resid_df &lt;- retail_full_quad_lm |&gt; \n  residuals() |&gt; \n  as_tibble() |&gt; \n  dplyr::select(.resid) |&gt;\n  rename(x = .resid) \n  \nretail_resid_df |&gt;\n  mutate(density = dnorm(x, mean(retail_resid_df$x), sd(retail_resid_df$x))) |&gt;\n  ggplot(aes(x = x)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"#56B4E9\", binwidth = 0.02) +\n    geom_line(aes(x = x, y = density)) +\n    theme_bw() +\n    labs(\n      x = \"Values\",\n      y = \"Frequency\",\n      title = \"Histogram of Residuals from the Full Quadratic Model\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nShow the code\nskewness(retail_resid_df$x)\n\n\n[1] 0.2205956\n\n\nThere is little skewness. We will use the log-normal correction factor.\n\nSmall-Group Activity: Texas Industrial Electricity Usage\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUse the Texas industrial electricity consumption data to do the following.\n\nSelect an appropriate fitted model using the AIC, AICc, or BIC critera.\nUse the residuals to determine the appropriate correction for the data.\nForecast the data for the next 5 years.\nApply the appropriate correction to the forecasted values.\nPlot the fitted (forecasted) values along with the time series.\n\n\n\n\nHomework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_5_4.qmd \n\n\n\nSmall-Group Activity\n\n\nSmall-Group Activity: Retail Sales Code\n\n\n\n\n\n\nCheck Your Understanding Code\n\n\n\n\n\n\n\n\n\nFigures\n\n\n\n\n\nFigure 15 gives the total sales (in millions of U.S. dollars) for the category “all other general merchandise stores (45299),” beginning with January 1998.\n\n\nShow the code\n# Read in retail sales data for \"all other general merchandise stores\"\nretail_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/retail_by_business_type.parquet\") |&gt;\n  filter(naics == 45299) |&gt;\n  mutate(t = 1:n()) |&gt;\n  mutate(std_t = (t - mean(t)) / sd(t)) |&gt;\n  mutate(\n    cos1 = cos(2 * pi * 1 * t / 12),\n    cos2 = cos(2 * pi * 2 * t / 12),\n    cos3 = cos(2 * pi * 3 * t / 12),\n    cos4 = cos(2 * pi * 4 * t / 12),\n    cos5 = cos(2 * pi * 5 * t / 12),\n    cos6 = cos(2 * pi * 6 * t / 12),\n    sin1 = sin(2 * pi * 1 * t / 12),\n    sin2 = sin(2 * pi * 2 * t / 12),\n    sin3 = sin(2 * pi * 3 * t / 12),\n    sin4 = sin(2 * pi * 4 * t / 12),\n    sin5 = sin(2 * pi * 5 * t / 12)\n  ) |&gt;\n  filter(as_date(month) &gt;= my(\"Jan 1998\")) |&gt;\n  as_tsibble(index = month)\n\nretail_ts |&gt;\n  autoplot(.vars = sales_millions) +\n    labs(\n      x = \"Month\",\n      y = \"Sales (Millions of U.S. Dollars)\",\n      title = paste0(retail_ts$business[1], \" (\", retail_ts$naics[1], \")\")\n    ) +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFigure 15: Time plot of the total monthly retail sales for all other general merchandise stores (45299)\n\n\n\n\n\nFigure 16 shows the “All other general merchandise” retail sales data.\n\nShow the code\nplot_raw &lt;- retail_ts |&gt;\n    autoplot(.vars = sales_millions) +\n    labs(\n      x = \"Month\",\n      y = \"sales_millions\",\n      title = \"Other General Merchandise Sales\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\nplot_log &lt;- retail_ts |&gt;\n    autoplot(.vars = log(sales_millions)) +\n    labs(\n      x = \"Month\",\n      y = \"log(sales_millions)\",\n      title = \"Logarithm of Other Gen. Merch. Sales\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\nplot_raw | plot_log\n\n\n\n\n\n\n\nFigure 16: Time plot of the time series (left) and the natural logarithm of the time series (right)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCubic Trend\n\n\n\n\n\n\n\nShow the code\n# Cubic model with standardized time variable\n\nfull_cubic_lm &lt;- retail_ts |&gt;\n  model(full_cubic = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + I(std_t^3) +\n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n    + sin4 + cos4 + sin5 + cos5 + cos6)) # Note sin6 is omitted\nfull_cubic_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 15 × 7\n   .model     term        estimate std.error statistic   p.value sig  \n   &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 full_cubic (Intercept)  8.21      0.00391  2101.    0         TRUE \n 2 full_cubic std_t        0.445     0.00621    71.8   1.58e-184 TRUE \n 3 full_cubic I(std_t^2)   0.0224    0.00704     3.19  1.59e-  3 TRUE \n 4 full_cubic I(std_t^3)  -0.00244   0.00573    -0.426 6.70e-  1 FALSE\n 5 full_cubic sin1        -0.0232    0.00333    -6.96  2.30e- 11 TRUE \n 6 full_cubic cos1         0.0354    0.00332    10.6   1.71e- 22 TRUE \n 7 full_cubic sin2        -0.0692    0.00332   -20.8   3.31e- 59 TRUE \n 8 full_cubic cos2         0.0785    0.00332    23.6   3.68e- 69 TRUE \n 9 full_cubic sin3        -0.0434    0.00332   -13.1   6.81e- 31 TRUE \n10 full_cubic cos3         0.0699    0.00332    21.0   5.82e- 60 TRUE \n11 full_cubic sin4        -0.0288    0.00332    -8.68  3.06e- 16 TRUE \n12 full_cubic cos4         0.0627    0.00332    18.9   3.93e- 52 TRUE \n13 full_cubic sin5         0.0118    0.00332     3.56  4.35e-  4 TRUE \n14 full_cubic cos5         0.0636    0.00332    19.2   3.92e- 53 TRUE \n15 full_cubic cos6         0.0251    0.00235    10.7   1.17e- 22 TRUE \n\n\nNote that each of the Fourier terms is statistically significant.\n\n\n\n\n\n\n\n\n\n\nQuadratic Trend\n\n\n\n\n\n\n\nShow the code\nfull_quad_lm &lt;- retail_ts |&gt;\n  model(full_quad = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + \n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n    + sin4 + cos4 + sin5 + cos5 + cos6 )) # Note sin6 is omitted\nfull_quad_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 14 × 7\n   .model    term        estimate std.error statistic   p.value sig  \n   &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 full_quad (Intercept)   8.22     0.00334   2460.   0         TRUE \n 2 full_quad std_t         0.443    0.00398    111.   1.01e-237 TRUE \n 3 full_quad I(std_t^2)    0.0200   0.00404      4.94 1.30e-  6 TRUE \n 4 full_quad sin1         -0.0231   0.00332     -6.96 2.31e- 11 TRUE \n 5 full_quad cos1          0.0353   0.00332     10.7  1.54e- 22 TRUE \n 6 full_quad sin2         -0.0692   0.00332    -20.9  2.28e- 59 TRUE \n 7 full_quad cos2          0.0785   0.00332     23.7  2.33e- 69 TRUE \n 8 full_quad sin3         -0.0434   0.00332    -13.1  5.77e- 31 TRUE \n 9 full_quad cos3          0.0699   0.00332     21.1  3.96e- 60 TRUE \n10 full_quad sin4         -0.0288   0.00332     -8.69 2.83e- 16 TRUE \n11 full_quad cos4          0.0627   0.00332     18.9  2.84e- 52 TRUE \n12 full_quad sin5          0.0118   0.00332      3.57 4.25e-  4 TRUE \n13 full_quad cos5          0.0636   0.00332     19.2  2.81e- 53 TRUE \n14 full_quad cos6          0.0251   0.00235     10.7  1.04e- 22 TRUE \n\n\n\n\n\n\n\n\n\n\n\n\nLinear Trend\n\n\n\n\n\n\n\nShow the code\nfull_linear_lm &lt;- retail_ts |&gt;\n  model(full_quadratic = TSLM(log(sales_millions) ~ std_t +\n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n    + sin4 + cos4 + sin5 + cos5 + cos6 )) # Note sin6 is omitted\nfull_linear_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 13 × 7\n   .model         term        estimate std.error statistic   p.value sig  \n   &lt;chr&gt;          &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 full_quadratic (Intercept)   8.23     0.00264   3114.   0         TRUE \n 2 full_quadratic std_t         0.457    0.00303    151.   2.86e-275 TRUE \n 3 full_quadratic sin1         -0.0231   0.00345     -6.69 1.16e- 10 TRUE \n 4 full_quadratic cos1          0.0354   0.00345     10.3  3.26e- 21 TRUE \n 5 full_quadratic sin2         -0.0692   0.00345    -20.1  1.59e- 56 TRUE \n 6 full_quadratic cos2          0.0785   0.00345     22.8  2.95e- 66 TRUE \n 7 full_quadratic sin3         -0.0434   0.00345    -12.6  3.44e- 29 TRUE \n 8 full_quadratic cos3          0.0699   0.00345     20.3  2.87e- 57 TRUE \n 9 full_quadratic sin4         -0.0288   0.00345     -8.36 2.81e- 15 TRUE \n10 full_quadratic cos4          0.0627   0.00345     18.2  1.18e- 49 TRUE \n11 full_quadratic sin5          0.0118   0.00345      3.43 6.95e-  4 TRUE \n12 full_quadratic cos5          0.0636   0.00345     18.4  1.26e- 50 TRUE \n13 full_quadratic cos6          0.0251   0.00244     10.3  2.38e- 21 TRUE \n\n\n\n\n\n\n\n\n\n\n\n\nModel Comparison\n\n\n\n\n\nWe will now compare the models we fitted above. #tbl-ModelComparison2 gives the AIC, AICc, and BIC of the models fitted above. In addition, other models with a reduced number of Fourier terms are included. For example, the model labeled reduced_quadratic_fourier_i5 includes linear and quadratic trend terms but also the Fourier terms where \\(i \\le 5\\).\n\n\nShow the code\nmodel_combined &lt;- retail_ts |&gt;\n  model(\n    full_cubic = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + I(std_t^3) +\n                    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n                  + sin4 + cos4 + sin5 + cos5 + cos6 ),\n    full_quadratic = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + \n                       sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n                     + sin4 + cos4 + sin5 + cos5 + cos6 ),\n    reduced_quadratic_fourier_i5 = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + \n                       sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n                     + sin4 + cos4 + sin5 + cos5),\n    reduced_quadratic_fourier_i4 = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + \n                       sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n                     + sin4 + cos4),\n    # reduced_quadratic_fourier_i3 = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + \n    #                    sin1 + cos1 + sin2 + cos2 + sin3 + cos3),\n    # reduced_quadratic_fourier_i2 = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + \n    #                    sin1 + cos1 + sin2 + cos2),\n    # reduced_quadratic_fourier_i1 = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + \n    #                    sin1 + cos1),\n    full_linear = TSLM(log(sales_millions) ~ std_t + \n                         sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n                       + sin4 + cos4 + sin5 + cos5 + cos6 ),\n    reduced_linear_fourier_i5 = TSLM(log(sales_millions) ~ std_t + \n                       sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n                     + sin4 + cos4 + sin5 + cos5),\n    reduced_linear_fourier_i4 = TSLM(log(sales_millions) ~ std_t + \n                       sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n                     + sin4 + cos4),\n    # reduced_linear_fourier_i3 = TSLM(log(sales_millions) ~ std_t + \n    #                    sin1 + cos1 + sin2 + cos2 + sin3 + cos3),\n    # reduced_linear_fourier_i2 = TSLM(log(sales_millions) ~ std_t + \n    #                    sin1 + cos1 + sin2 + cos2),\n    # reduced_linear_fourier_i1 = TSLM(log(sales_millions) ~ std_t + \n    #                    sin1 + cos1)\n  )\n\nglance(model_combined) |&gt;\n  select(.model, AIC, AICc, BIC)\n\n\n\n\n\n\nTable 5: Comparison of the AIC, AICc, and BIC values for the models fitted to the logarithm of the retail sales time series.\n\n\n\n\n\n\nModel\nAIC\nAICc\nBIC\n\n\n\n\nfull_cubic\n-1904.6\n-1902.7\n-1845.4\n\n\nfull_quadratic\n**-1906.5**\n**-1904.8**\n**-1850.9**\n\n\nreduced_quadratic_fourier_i5\n-1807.4\n-1805.9\n-1755.5\n\n\nreduced_quadratic_fourier_i4\n-1611.1\n-1610\n-1566.6\n\n\nfull_linear\n-1883.8\n-1882.4\n-1832\n\n\nreduced_linear_fourier_i5\n-1791.6\n-1790.3\n-1743.5\n\n\nreduced_linear_fourier_i4\n-1603.8\n-1602.9\n-1563.1\n\n\n\n\n\n\n\n\n\n\nThe model with the lowest AIC, AICc, and BIC values is the full quadratic model. This can be written as:\n\\[\\begin{align*}\n  \\log(x_t) &= \\beta_0\n            + \\beta_1 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right)\n            + \\beta_2 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right)^2 \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_3 \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + \\beta_4 \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_5 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + \\beta_6 \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_7 \\sin \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right)\n            + \\beta_8 \\cos \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_9 \\sin \\left( \\frac{2\\pi \\cdot 4 t}{12} \\right)\n            + \\beta_{10} \\cos \\left( \\frac{2\\pi \\cdot 4 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_{11} \\sin \\left( \\frac{2\\pi \\cdot 5 t}{12} \\right)\n            + \\beta_{12} \\cos \\left( \\frac{2\\pi \\cdot 5 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            \\phantom{+ \\beta_{13} \\sin \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right)}\n            + \\beta_{13} \\cos \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right)\n      + z_t\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\n\nAutocorrelation of the Random Component\n\n\n\n\n\nWe check for autocorrelation in the random component to determine if using GLS is warranted.\nFigure 17 illustrates the ACF of the full model with a quadratic trend.\n\nfull_quad_ts &lt;- full_quad_lm |&gt;\n  residuals() \n\nacf(full_quad_ts$.resid, plot=TRUE, lag.max = 25)\n\n\n\n\n\n\n\nFigure 17: ACF of the full model with a quadratic trend\n\n\n\n\n\nWe observe evidence of autocorrelation in the random terms. In fact, there is something happening on an annual\nFigure 18 illustrates the PACF of the reduced model 1 with linear trend.\n\n\nShow the code\nalphas_quad &lt;- pacf(full_quad_ts$.resid, plot=FALSE, lag.max = 25) \npacf(full_quad_ts$.resid, plot=TRUE, lag.max = 25)\n\n\n\n\n\n\n\n\nFigure 18: PACF of the full model with a quadratic trend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplying Genearlized Least Squares, GLS\n\n\n\n\n\nRecall that in Chapter 5, Lesson 1, we fitted a linear regression model using the value of the partial autocorrelation function for \\(k=1\\). This helps account for the autocorrelation in the residuals.\nWe will use the PACF when \\(k=1\\) to apply the GLS algorithm. The first few partial autocorrelation values are:\n\n\nShow the code\npacf(full_quad_ts$.resid, plot=FALSE, lag.max = 10)\n\n\n\nPartial autocorrelations of series 'full_quad_ts$.resid', by lag\n\n     1      2      3      4      5      6      7      8      9     10 \n 0.330  0.129 -0.023  0.062 -0.031  0.019 -0.035  0.044 -0.100  0.048 \n\n\nThe partial autocorrelation when \\(k=1\\) is approximately 0.33. We will use this value as we recompute the regression coefficients.\n\n\nShow the code\n# Load additional packages\npacman::p_load(tidymodels, multilevelmod,\n  nlme, broom.mixed)\n\ntemp_spec &lt;- linear_reg() |&gt;\n  set_engine(\"gls\", correlation = nlme::corAR1(0.497))\n\ntemp_gls &lt;- temp_spec |&gt;\n  fit(log(sales_millions) ~ std_t + sin1 + cos1 + sin2 + cos2 + sin3 + cos3, data = retail_ts)\n\ntidy(temp_gls) |&gt;\n  mutate(\n    lower = estimate + qnorm(0.025) * std.error,\n    upper = estimate + qnorm(0.975) * std.error\n  ) \n\n\n# A tibble: 8 × 7\n  term        estimate std.error statistic   p.value   lower   upper\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   8.23     0.00332   2479.   0          8.22    8.23  \n2 std_t         0.458    0.00381    120.   1.71e-250  0.450   0.465 \n3 sin1         -0.0231   0.00446     -5.18 4.17e-  7 -0.0318 -0.0143\n4 cos1          0.0355   0.00446      7.95 4.13e- 14  0.0267  0.0442\n5 sin2         -0.0692   0.00487    -14.2  3.22e- 35 -0.0787 -0.0597\n6 cos2          0.0787   0.00487     16.2  2.28e- 42  0.0691  0.0882\n7 sin3         -0.0434   0.00567     -7.65 2.86e- 13 -0.0545 -0.0323\n8 cos3          0.0701   0.00567     12.4  1.68e- 28  0.0590  0.0812\n\n\nFigure 19 illustrates the original time series (in black) and the fitted model (in blue). For reference, a dotted line illustrating the simple least squares line is plotted on this figure for reference. It helps highlight the exponential shape of the trend.\n\n\nShow the code\nforecast_df &lt;- full_quad_lm |&gt; \n  forecast(retail_ts) |&gt;  # computes the anti-log of the predicted values and returns them as .mean\n  as_tibble() |&gt; \n  dplyr::select(std_t, .mean) |&gt; \n  rename(pred = .mean)\n\nretail_ts |&gt;\n  left_join(forecast_df, by = \"std_t\") |&gt;\n  as_tsibble(index = month) |&gt;\n  autoplot(.vars = sales_millions) +\n  geom_smooth(method = \"lm\", formula = 'y ~ x', se = FALSE, color = \"#E69F00\", linewidth = 0.5, linetype = \"dotted\") +\n  geom_line(aes(y = pred), color = \"#56B4E9\", alpha = 0.75) +\n    labs(\n      x = \"Month\",\n      y = \"Simulated Time Series\",\n      title = \"Time Plot of Simulated Time Series with an Exponential Trend\",\n      subtitle = \"Predicted values based on the full cubic model are given in blue\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5),\n      plot.subtitle = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\nFigure 19: Time plot of the time series (left) and the natural logarithm of the time series (right)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUse the retail sales data to do the following.\n\nSelect an appropriate fitted model using the AIC, AICc, or BIC critera.\nUse the residuals to determine the appropriate correction for the data.\nForecast the data for the next 5 years.\nApply the appropriate correction to the forecasted values.\nPlot the fitted (forecasted) values along with the time series.\n\n\n\n\n\nShow the code\nretail_full_quad_lm &lt;- retail_ts |&gt;\n  model(retail_full_quad = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + \n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n    + sin4 + cos4 + sin5 + cos5 + cos6 )) # Note sin6 is omitted\nretail_full_quad_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 14 × 7\n   .model           term        estimate std.error statistic   p.value sig  \n   &lt;chr&gt;            &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 retail_full_quad (Intercept)   8.22     0.00334   2460.   0         TRUE \n 2 retail_full_quad std_t         0.443    0.00398    111.   1.01e-237 TRUE \n 3 retail_full_quad I(std_t^2)    0.0200   0.00404      4.94 1.30e-  6 TRUE \n 4 retail_full_quad sin1         -0.0231   0.00332     -6.96 2.31e- 11 TRUE \n 5 retail_full_quad cos1          0.0353   0.00332     10.7  1.54e- 22 TRUE \n 6 retail_full_quad sin2         -0.0692   0.00332    -20.9  2.28e- 59 TRUE \n 7 retail_full_quad cos2          0.0785   0.00332     23.7  2.33e- 69 TRUE \n 8 retail_full_quad sin3         -0.0434   0.00332    -13.1  5.77e- 31 TRUE \n 9 retail_full_quad cos3          0.0699   0.00332     21.1  3.96e- 60 TRUE \n10 retail_full_quad sin4         -0.0288   0.00332     -8.69 2.83e- 16 TRUE \n11 retail_full_quad cos4          0.0627   0.00332     18.9  2.84e- 52 TRUE \n12 retail_full_quad sin5          0.0118   0.00332      3.57 4.25e-  4 TRUE \n13 retail_full_quad cos5          0.0636   0.00332     19.2  2.81e- 53 TRUE \n14 retail_full_quad cos6          0.0251   0.00235     10.7  1.04e- 22 TRUE \n\n\nShow the code\nretail_resid_df &lt;- retail_full_quad_lm |&gt; \n  residuals() |&gt; \n  as_tibble() |&gt; \n  dplyr::select(.resid) |&gt;\n  rename(x = .resid) \n  \nretail_resid_df |&gt;\n  mutate(density = dnorm(x, mean(retail_resid_df$x), sd(retail_resid_df$x))) |&gt;\n  ggplot(aes(x = x)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"#56B4E9\", binwidth = 0.02) +\n    geom_line(aes(x = x, y = density)) +\n    theme_bw() +\n    labs(\n      x = \"Values\",\n      y = \"Frequency\",\n      title = \"Histogram of Residuals from the Full Quadratic Model\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nShow the code\nskewness(retail_resid_df$x)\n\n\n[1] 0.2205956\n\n\nThere is little skewness. We will use the log-normal correction factor.\n\nSmall-Group Activity: Texas Industrial Electricity Usage\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUse the Texas industrial electricity consumption data to do the following.\n\nSelect an appropriate fitted model using the AIC, AICc, or BIC critera.\nUse the residuals to determine the appropriate correction for the data.\nForecast the data for the next 5 years.\nApply the appropriate correction to the forecasted values.\nPlot the fitted (forecasted) values along with the time series.\n\n\n\n\n\nShow the code\n# Cubic model with standardized time variable\n\nelec_full_cubic_lm &lt;- elec_ts |&gt;\n  model(elec_full_cubic = TSLM(log(megawatthours) ~ std_t + I(std_t^2) + I(std_t^3) +\n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 + sin4 + cos4 + sin5 + cos5 + cos6)) # Note sin6 is omitted\nelec_full_cubic_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 15 × 7\n   .model          term          estimate std.error statistic  p.value sig  \n   &lt;chr&gt;           &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;\n 1 elec_full_cubic (Intercept) 16.1         0.00684 2349.     0        TRUE \n 2 elec_full_cubic std_t        0.124       0.0115    10.8    1.19e-20 TRUE \n 3 elec_full_cubic I(std_t^2)   0.0313      0.00513    6.10   8.50e- 9 TRUE \n 4 elec_full_cubic I(std_t^3)   0.0105      0.00589    1.79   7.55e- 2 FALSE\n 5 elec_full_cubic sin1        -0.0541      0.00648   -8.36   3.68e-14 TRUE \n 6 elec_full_cubic cos1        -0.0468      0.00645   -7.25   1.89e-11 TRUE \n 7 elec_full_cubic sin2        -0.0000864   0.00646   -0.0134 9.89e- 1 FALSE\n 8 elec_full_cubic cos2        -0.000831    0.00645   -0.129  8.98e- 1 FALSE\n 9 elec_full_cubic sin3         0.00103     0.00645    0.160  8.73e- 1 FALSE\n10 elec_full_cubic cos3         0.0131      0.00645    2.03   4.46e- 2 TRUE \n11 elec_full_cubic sin4         0.0121      0.00645    1.87   6.29e- 2 FALSE\n12 elec_full_cubic cos4         0.00272     0.00645    0.421  6.74e- 1 FALSE\n13 elec_full_cubic sin5         0.0220      0.00645    3.41   8.36e- 4 TRUE \n14 elec_full_cubic cos5        -0.000505    0.00645   -0.0783 9.38e- 1 FALSE\n15 elec_full_cubic cos6        -0.00172     0.00456   -0.378  7.06e- 1 FALSE\n\n\nShow the code\n# Quadratic model with standardized time variable\n\nelec_full_quadratic_lm &lt;- elec_ts |&gt;\n  model(elec_full_cubic = TSLM(log(megawatthours) ~ std_t + I(std_t^2) + \n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 + sin4 + cos4 + sin5 + cos5 + cos6)) # Note sin6 is omitted\nelec_full_quadratic_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 14 × 7\n   .model          term         estimate std.error statistic  p.value sig  \n   &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;\n 1 elec_full_cubic (Intercept) 16.1        0.00689 2333.     0        TRUE \n 2 elec_full_cubic std_t        0.143      0.00462   31.0    5.06e-68 TRUE \n 3 elec_full_cubic I(std_t^2)   0.0313     0.00517    6.05   1.04e- 8 TRUE \n 4 elec_full_cubic sin1        -0.0551     0.00650   -8.47   1.79e-14 TRUE \n 5 elec_full_cubic cos1        -0.0465     0.00650   -7.16   3.03e-11 TRUE \n 6 elec_full_cubic sin2        -0.000536   0.00650   -0.0825 9.34e- 1 FALSE\n 7 elec_full_cubic cos2        -0.000571   0.00650   -0.0880 9.30e- 1 FALSE\n 8 elec_full_cubic sin3         0.000775   0.00650    0.119  9.05e- 1 FALSE\n 9 elec_full_cubic cos3         0.0133     0.00650    2.05   4.19e- 2 TRUE \n10 elec_full_cubic sin4         0.0119     0.00649    1.84   6.80e- 2 FALSE\n11 elec_full_cubic cos4         0.00298    0.00650    0.458  6.48e- 1 FALSE\n12 elec_full_cubic sin5         0.0219     0.00649    3.37   9.39e- 4 TRUE \n13 elec_full_cubic cos5        -0.000245   0.00650   -0.0378 9.70e- 1 FALSE\n14 elec_full_cubic cos6        -0.00159    0.00459   -0.347  7.29e- 1 FALSE\n\n\nShow the code\nelec_resid_df &lt;- elec_full_quadratic_lm |&gt; \n  residuals() |&gt; \n  as_tibble() |&gt; \n  dplyr::select(.resid) |&gt;\n  rename(x = .resid) \n  \nelec_resid_df |&gt;\n  mutate(density = dnorm(x, mean(elec_resid_df$x), sd(elec_resid_df$x))) |&gt;\n  ggplot(aes(x = x)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"#56B4E9\", binwidth = 0.02) +\n    geom_line(aes(x = x, y = density)) +\n    theme_bw() +\n    labs(\n      x = \"Values\",\n      y = \"Frequency\",\n      title = \"Histogram of Residuals from the Full Quadratic Model\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nShow the code\nskewness(elec_resid_df$x)\n\n\n[1] -0.9556189\n\n\nThere is moderate negative skewness. We will use the emperical correction factor.",
    "crumbs": [
      "Lesson 4",
      "Transformations, Forecasting adn Bias Correction"
    ]
  },
  {
    "objectID": "chapter_5_lesson_2.html",
    "href": "chapter_5_lesson_2.html",
    "title": "Harmonic Seasonal Variables - Part 1",
    "section": "",
    "text": "Fit linear regression models to time series data\n\n\nDescribe a Fourier series\nExplain how a few terms in a Fourier series can be used to fit a seasonal component\nMotivate the use of the harmonic seasonal model\nRepresent seasonal factors using harmonic seasonal terms",
    "crumbs": [
      "Lesson 2",
      "Harmonic Seasonal Variables - Part 1"
    ]
  },
  {
    "objectID": "chapter_5_lesson_2.html#learning-outcomes",
    "href": "chapter_5_lesson_2.html#learning-outcomes",
    "title": "Harmonic Seasonal Variables - Part 1",
    "section": "",
    "text": "Fit linear regression models to time series data\n\n\nDescribe a Fourier series\nExplain how a few terms in a Fourier series can be used to fit a seasonal component\nMotivate the use of the harmonic seasonal model\nRepresent seasonal factors using harmonic seasonal terms",
    "crumbs": [
      "Lesson 2",
      "Harmonic Seasonal Variables - Part 1"
    ]
  },
  {
    "objectID": "chapter_5_lesson_2.html#preparation",
    "href": "chapter_5_lesson_2.html#preparation",
    "title": "Harmonic Seasonal Variables - Part 1",
    "section": "Preparation",
    "text": "Preparation\n\nRead Section 5.6",
    "crumbs": [
      "Lesson 2",
      "Harmonic Seasonal Variables - Part 1"
    ]
  },
  {
    "objectID": "chapter_5_lesson_2.html#learning-journal-exchange-10-min",
    "href": "chapter_5_lesson_2.html#learning-journal-exchange-10-min",
    "title": "Harmonic Seasonal Variables - Part 1",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading another student’s?\nWhat would you recommend the other student add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 2",
      "Harmonic Seasonal Variables - Part 1"
    ]
  },
  {
    "objectID": "chapter_5_lesson_2.html#class-activity-joys-of-trigonometry-15-min",
    "href": "chapter_5_lesson_2.html#class-activity-joys-of-trigonometry-15-min",
    "title": "Harmonic Seasonal Variables - Part 1",
    "section": "Class Activity: Joys of Trigonometry (15 min)",
    "text": "Class Activity: Joys of Trigonometry (15 min)\nIn the previous lesson, we learned how to incorporate an indicator (or dummy) variable for each season in a period. If there are twelve months in a year, this requires having twelve parameters in the model. Given that many seasonal changes are gradual and can be modeled by a continuous function, we can use sines and cosines to approximate the seasonal variation. This can lead to a smaller number of parameters than is required for the indicator variable approach.\nConsider the sine wave with the following parameters:\n\n\\(A\\):\\(~~\\) the amplitude,\n\\(f\\):\\(~~~\\) the frequency or the number of cycles per sampling interval, and\n\\(\\phi\\):\\(~~~\\) the phase shift.\n\n\\[\n  A \\sin ( 2 \\pi f t + \\phi )\n\\]\nHere is an interactive plot of this function. Adjust the values of \\(A\\), \\(f\\), and \\(\\phi\\) to see their effect on the function.\n\n\nNotice that this sine function is not linear in the parameters \\(A\\) and \\(\\phi\\).\n\\[\n  A \\sin ( 2 \\pi f t + \\phi )\n\\]\nOne of the trigonometric sum and difference identities is:\n\\[\n  \\sin(\\theta + \\phi)\n    = \\cos(\\phi) \\sin(\\theta) + \\sin(\\phi) \\cos(\\theta)\n\\]\nWe apply this to our sine function.\n\\[\\begin{align*}\n  A \\sin ( 2 \\pi f t + \\phi )\n    &= \\underbrace{A \\cos( \\phi )}_{\\alpha_s} \\cdot \\sin ( 2 \\pi f t ) + \\underbrace{A \\sin( \\phi )}_{\\alpha_c} \\cdot \\cos ( 2 \\pi f t ) \\\\\n    &= \\alpha_s \\cdot \\sin ( 2 \\pi f t ) + \\alpha_c \\cdot \\cos ( 2 \\pi f t )\n\\end{align*}\\]\nWe have transformed this from something that is not linear in the parameters \\(A\\) and \\(\\phi\\) to an expression that is linear in the parameters \\(\\alpha_s\\) and \\(\\alpha_c\\).\nWe can denote the frequency of a sine function as \\(f = \\frac{i}{s}\\), where \\(s\\) is the number of seasons in a cycle and \\(i\\) is some integer. This leads the following representation:\n\\[\\begin{align*}\n  A \\sin \\left( \\frac{2 \\pi i t}{s} + \\phi \\right)\n    &= \\underbrace{A \\cos( \\phi )}_{\\alpha_s} \\cdot \\sin \\left( \\frac{2 \\pi i t}{s} \\right) + \\underbrace{A \\sin( \\phi )}_{\\alpha_c} \\cdot \\cos \\left( \\frac{2 \\pi i t}{s} \\right) \\\\\n    &= \\alpha_s \\cdot \\sin \\left( \\frac{2 \\pi i t}{s} \\right) + \\alpha_c \\cdot \\cos \\left( \\frac{2 \\pi i t}{s} \\right)\n\\end{align*}\\]\nFigure 1 and Figure 2 illustrate these sine and cosine functions with various values of \\(i\\), where \\(s = 12\\).\n\n\n\n\n\nShow the code\nplot_sine()\n\n\n\n\n\n\n\n\nFigure 1: Sine Functions with Various Frequencies\n\n\n\n\n\n\n\n\n\n\nShow the code\nplot_cosine()\n\n\n\n\n\n\n\n\nFigure 2: Cosine Functions with Various Frequencies\n\n\n\n\n\n\n\n\nOne key objective of this lesson is to use a linear combination of functions like those above to model the seasonal component of a time series.",
    "crumbs": [
      "Lesson 2",
      "Harmonic Seasonal Variables - Part 1"
    ]
  },
  {
    "objectID": "chapter_5_lesson_2.html#class-activity-fourier-series-15-min",
    "href": "chapter_5_lesson_2.html#class-activity-fourier-series-15-min",
    "title": "Harmonic Seasonal Variables - Part 1",
    "section": "Class Activity: Fourier Series (15 min)",
    "text": "Class Activity: Fourier Series (15 min)\n\nWhat is a Fourier Series?\nWe now explore an important mathematical concept that allows us to approximate any periodic function. If we have an infinite number of terms, the Fourier Series described below gives an exact representation of the function.\n\n\n\n\n\n\nFourier Series\n\n\n\nThe Fourier Series is an infinite series representation of a smooth function \\(f(t)\\) with period \\(s\\):\n\\[\n  f(t) = \\frac{A_{s_0}}{2} + \\sum_{i=1}^{\\infty} \\left\\{ \\alpha_{s_i} \\sin \\left( \\frac{2\\pi i t}{s} \\right) + \\alpha_{c_i} \\cos \\left( \\frac{2\\pi i t}{s} \\right) \\right\\}\n\\]\nThe coefficients \\(\\alpha_{s_i}\\) and \\(\\alpha_{c_i}\\) are defined by the integrals:\n\\[\n  \\alpha_{s_i} = \\frac{2}{s} \\int_0^s f(t) \\cos \\left( \\frac{2\\pi i t}{s} \\right) \\; dt\n  ~~~~~~~~~~~~~~~~~~\n  \\alpha_{c_i} = \\frac{2}{s} \\int_0^s f(t) \\sin \\left( \\frac{2\\pi i t}{s} \\right) \\; dt\n\\]\n(You will not need to compute any of these integrals.)\n\n\n\nAs an example, we will approximate the periodic function illustrated here with a Fourier series.\n\nTarget1 term2 terms3 terms4 terms5 terms10 terms15 terms25 terms50 terms100 terms\n\n\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0, xend = 1.75, yend = 0), linewidth = 1, : All aesthetics have length 1, but the data has 3 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0.785, xend = -1, yend = 0.785, : All aesthetics have length 1, but the data has 3 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = 0.785, xend = -1, yend = -0.785, : All aesthetics have length 1, but the data has 3 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = -0.785, xend = 0, yend = -0.785, : All aesthetics have length 1, but the data has 3 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = -0.785, xend = 0, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = 0.785, xend = 1, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = 0.785, xend = 1, yend = -0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = -0.785, xend = 1.75, yend = -0.785, : All aesthetics have length 1, but the data has 3 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning: A numeric `legend.position` argument in `theme()` was deprecated in ggplot2\n3.5.0.\nℹ Please use the `legend.position.inside` argument of `theme()` instead.\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0, xend = 1.75, yend = 0), linewidth = 1, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0.785, xend = -1, yend = 0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = 0.785, xend = -1, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = -0.785, xend = 0, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = -0.785, xend = 0, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = 0.785, xend = 1, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = 0.785, xend = 1, yend = -0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = -0.785, xend = 1.75, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\\[\n  f(x) \\approx sin(\\pi x)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0, xend = 1.75, yend = 0), linewidth = 1, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0.785, xend = -1, yend = 0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = 0.785, xend = -1, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = -0.785, xend = 0, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = -0.785, xend = 0, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = 0.785, xend = 1, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = 0.785, xend = 1, yend = -0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = -0.785, xend = 1.75, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\\[\n  f(x) \\approx \\sin(\\pi x) + \\frac{1}{3} \\sin(3 \\pi x)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0, xend = 1.75, yend = 0), linewidth = 1, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0.785, xend = -1, yend = 0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = 0.785, xend = -1, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = -0.785, xend = 0, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = -0.785, xend = 0, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = 0.785, xend = 1, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = 0.785, xend = 1, yend = -0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = -0.785, xend = 1.75, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\\[\n  f(x) \\approx \\sin(\\pi x) + \\frac{1}{3} \\sin(3 \\pi x) + \\frac{1}{5} \\sin(5 \\pi x)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0, xend = 1.75, yend = 0), linewidth = 1, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0.785, xend = -1, yend = 0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = 0.785, xend = -1, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = -0.785, xend = 0, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = -0.785, xend = 0, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = 0.785, xend = 1, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = 0.785, xend = 1, yend = -0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = -0.785, xend = 1.75, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\\[\n  f(x) \\approx \\sin(\\pi x) + \\frac{1}{3} \\sin(3 \\pi x) + \\frac{1}{5} \\sin(5 \\pi x) + \\frac{1}{7} \\sin(7 \\pi x)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0, xend = 1.75, yend = 0), linewidth = 1, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0.785, xend = -1, yend = 0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = 0.785, xend = -1, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = -0.785, xend = 0, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = -0.785, xend = 0, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = 0.785, xend = 1, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = 0.785, xend = 1, yend = -0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = -0.785, xend = 1.75, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\\[\n  f(x) \\approx sin(\\pi x) + \\frac{1}{3} \\sin(3 \\pi x) + \\frac{1}{5} \\sin(5 \\pi x) + \\frac{1}{7} \\sin(7 \\pi x) + \\frac{1}{9} \\sin(9 \\pi x)\n\\]\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0, xend = 1.75, yend = 0), linewidth = 1, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0.785, xend = -1, yend = 0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = 0.785, xend = -1, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = -0.785, xend = 0, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = -0.785, xend = 0, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = 0.785, xend = 1, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = 0.785, xend = 1, yend = -0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = -0.785, xend = 1.75, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\\[\n  f(x) \\approx sin(\\pi x) + \\frac{1}{3} \\sin(3 \\pi x) + \\frac{1}{5} \\sin(5 \\pi x) + \\cdots + \\frac{1}{17} \\sin(17 \\pi x) + \\frac{1}{19} \\sin(19 \\pi x)\n\\]\n\n\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0, xend = 1.75, yend = 0), linewidth = 1, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0.785, xend = -1, yend = 0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = 0.785, xend = -1, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = -0.785, xend = 0, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = -0.785, xend = 0, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = 0.785, xend = 1, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = 0.785, xend = 1, yend = -0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = -0.785, xend = 1.75, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\\[\n  f(x) \\approx sin(\\pi x) + \\frac{1}{3} \\sin(3 \\pi x) + \\frac{1}{5} \\sin(5 \\pi x) + \\cdots + \\frac{1}{27} \\sin(27 \\pi x) + \\frac{1}{29} \\sin(29 \\pi x)\n\\]\n\n\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0, xend = 1.75, yend = 0), linewidth = 1, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0.785, xend = -1, yend = 0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = 0.785, xend = -1, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = -0.785, xend = 0, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = -0.785, xend = 0, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = 0.785, xend = 1, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = 0.785, xend = 1, yend = -0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = -0.785, xend = 1.75, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\\[\n  f(x) \\approx sin(\\pi x) + \\frac{1}{3} \\sin(3 \\pi x) + \\frac{1}{5} \\sin(5 \\pi x) + \\cdots + \\frac{1}{47} \\sin(47 \\pi x) + \\frac{1}{49} \\sin(49 \\pi x)\n\\]\n\n\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0, xend = 1.75, yend = 0), linewidth = 1, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0.785, xend = -1, yend = 0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = 0.785, xend = -1, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = -0.785, xend = 0, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = -0.785, xend = 0, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = 0.785, xend = 1, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = 0.785, xend = 1, yend = -0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = -0.785, xend = 1.75, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\\[\n  f(x) \\approx sin(\\pi x) + \\frac{1}{3} \\sin(3 \\pi x) + \\frac{1}{5} \\sin(5 \\pi x) + \\cdots + \\frac{1}{97} \\sin(97 \\pi x) + \\frac{1}{99} \\sin(99 \\pi x)\n\\]\n\n\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0, xend = 1.75, yend = 0), linewidth = 1, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1.75, y = 0.785, xend = -1, yend = 0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = 0.785, xend = -1, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = -1, y = -0.785, xend = 0, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = -0.785, xend = 0, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = 0.785, xend = 1, yend = 0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = 0.785, xend = 1, yend = -0.785, colour = \"Target Function\"), : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 1, y = -0.785, xend = 1.75, yend = -0.785, : All aesthetics have length 1, but the data has 3501 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\\[\n  f(x) \\approx sin(\\pi x) + \\frac{1}{3} \\sin(3 \\pi x) + \\frac{1}{5} \\sin(5 \\pi x) + \\cdots + \\frac{1}{197} \\sin(197 \\pi x) + \\frac{1}{199} \\sin(199 \\pi x)\n\\]\n\n\n\n\nWe can continue to get more and more precise estimates of the function by just adding more terms to the sum. So, the function \\(f(t)\\) can be approximated to any desired level of precision by truncating the series after a sufficient number of terms. For the purpose of this discussion, we will define “one term” as an expression of the form \\[\n  \\left\\{ \\alpha_{s_i} \\sin \\left( \\frac{2\\pi i t}{s} \\right) + \\alpha_{c_i} \\cos \\left( \\frac{2\\pi i t}{s} \\right) \\right\\}\n\\]\n\n\nFitting a Seasonal Component\nThe Fourier series exists for any smooth (continuously differentiable) function. Note theoretically, this allows us to obtain the value of the function at any real value by using this series. However, for a discrete time series with \\(s\\) seasons, we only need to evaluate the function at a finite number of points: \\(t = 1, 2, 3, \\ldots, s-1, s\\). For example, in the previous lesson, we used twelve indicator variables to pass through twelve points in a monthly seasonal component to a time series with annual cycles.\nIt turns out, that we only need six terms (which involves twelve coefficients) to fit monthly data with annual cycles. In general, we only need to obtain \\(\\left\\lfloor \\frac{s}{2} \\right\\rfloor\\) terms of this sum to fit the seasonal values perfectly.\n\n\n\nTable 1: A few examples of seasonal patterns and the corresponding values of \\(s\\) and \\(\\left\\lfloor \\frac{s}{2} \\right\\rfloor\\)\n\n\n\n\n\n\n\n\n\n\nPattern\nNumber of Seasons, \\(s\\)\nMaximum terms in the sum, \\(\\left\\lfloor \\frac{s}{2} \\right\\rfloor\\)\n\n\n\n\nDays in a Week\n7\n3\n\n\nQuarters in a Year\n4\n2\n\n\nMonths in a Year\n12\n6\n\n\n\n\n\n\nNote that if \\(s\\) is even and \\(i=\\frac{s}{2}\\), \\[\n  \\sin \\left( \\frac{2\\pi i t}{s} \\right) = \\sin \\left( \\frac{2\\pi \\cdot \\frac{s}{2} \\cdot t}{s} \\right) = \\sin \\left( \\pi t \\right) = 0\n\\] for all integer values of \\(t\\). So, this term must be omitted from the model. If we try to include it in the model, the coefficient will be rediculously large, as R trys to make the product of something very close to 0 (the value from the sine function) and the coefficient multiply to some reasonably small number.\nThe method for fitting seaonal components using indicator variables does not assume any relationship between successive seasons. However, values observed in January are often highly correlated with values observed in February, etc. Fitting a seasonal component using terms in the Fourier Series can often yield a good approximation for the periodic cycles with only a few terms.\nFor a time series with \\(s\\) seasons per cycle, our additive model can be written as:\n\\[\\begin{align*}\n  x_t\n    &= m_t + s_t + z_t \\\\\n    &= m_t +\n        \\sum_{i=1}^{\\left\\lfloor \\frac{s}{2} \\right\\rfloor} \\left\\{ \\alpha_{s_i} \\sin \\left( \\frac{2\\pi i t}{s} \\right) + \\alpha_{c_i} \\cos \\left( \\frac{2\\pi i t}{s} \\right) \\right\\}\n      + z_t\n\\end{align*}\\]\nThe term \\(m_t\\) can take a variety of forms, including:\n\nLinear: \\(~~~~~~~~~~~\\) \\(m_t = \\alpha_0 + \\alpha_1 t\\)\nQuadratic: \\(~~~~~~\\) \\(m_t = \\alpha_0 + \\alpha_1 t + \\alpha_2 t^2\\)\nExponential: \\(~~~\\) \\(m_t = \\alpha_0 e^{\\alpha_1 t}\\)\nAny other functional form\n\nThe term \\(z_t\\) is a (possibly autocorrelated) time series with mean zero.\nWe will now focus on the seasonal term, \\(s_t\\). The full seasonal term when considering 12 months in a year is:\n\\[\\begin{align*}\n  s_t\n    &= \\sum_{i=1}^{\\left\\lfloor \\frac{s}{2} \\right\\rfloor} \\left\\{ \\alpha_{s_i} \\sin \\left( \\frac{2\\pi i t}{s} \\right) + \\alpha_{c_i} \\cos \\left( \\frac{2\\pi i t}{s} \\right) \\right\\} \\\\\n    &= \\sum_{i=1}^{6} \\left\\{ \\alpha_{s_i} \\sin \\left( \\frac{2\\pi i t}{12} \\right) + \\alpha_{c_i} \\cos \\left( \\frac{2\\pi i t}{12} \\right) \\right\\} \\\\\n    &=~~~~ \\left\\{ \\alpha_{s_1} \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) + \\alpha_{c_1} \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\right\\} & \\leftarrow i = 1 \\\\\n    &~~~~~+ \\left\\{ \\alpha_{s_2} \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right) + \\alpha_{c_2} \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right) \\right\\} & \\leftarrow i = 2 \\\\\n    &~~~~~+ \\left\\{ \\alpha_{s_3} \\sin \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right) + \\alpha_{c_3} \\cos \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right) \\right\\} & \\leftarrow i = 3 \\\\\n    &~~~~~+ \\left\\{ \\alpha_{s_4} \\sin \\left( \\frac{2\\pi \\cdot 4 t}{12} \\right) + \\alpha_{c_4} \\cos \\left( \\frac{2\\pi \\cdot 4 t}{12} \\right) \\right\\} & \\leftarrow i = 4 \\\\\n    &~~~~~+ \\left\\{ \\alpha_{s_5} \\sin \\left( \\frac{2\\pi \\cdot 5 t}{12} \\right) + \\alpha_{c_5} \\cos \\left( \\frac{2\\pi \\cdot 5 t}{12} \\right) \\right\\} & \\leftarrow i = 5 \\\\\n    &~~~~~+ \\left\\{ \\alpha_{s_6} \\sin \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right) + \\alpha_{c_6} \\cos \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right) \\right\\} & \\leftarrow i = 6 \\\\\n    &=~~~~ \\left\\{ \\alpha_{s_1} \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) + \\alpha_{c_1} \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\right\\}  \\\\\n    &~~~~~+ \\left\\{ \\alpha_{s_2} \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right) + \\alpha_{c_2} \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right) \\right\\} \\\\\n    &~~~~~+ \\left\\{ \\alpha_{s_3} \\sin \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right) + \\alpha_{c_3} \\cos \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right) \\right\\} \\\\\n    &~~~~~+ \\left\\{ \\alpha_{s_4} \\sin \\left( \\frac{2\\pi \\cdot 4 t}{12} \\right) + \\alpha_{c_4} \\cos \\left( \\frac{2\\pi \\cdot 4 t}{12} \\right) \\right\\} \\\\\n    &~~~~~+ \\left\\{ \\alpha_{s_5} \\sin \\left( \\frac{2\\pi \\cdot 5 t}{12} \\right) + \\alpha_{c_5} \\cos \\left( \\frac{2\\pi \\cdot 5 t}{12} \\right) \\right\\} \\\\\n    &~~~~~+ \\left\\{ \\phantom{\\alpha_{s_6} \\sin \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right) +}~~ \\alpha_{c_6} \\cos \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right) \\right\\} \\\\\n\\end{align*}\\]\nNote that \\(\\sin \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right) = 0\\) for all integer values of \\(t\\), so we can omit the term \\(\\alpha_{s_6} \\sin \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right)\\).\nAs noted above, we can often use a relatively small subset of these terms to get a good approximation of the seasonal component.\n\n\nSimulation\nThe following simulation illustrates harmonic seasonal terms. The values S1, S2, S3, \\(\\ldots\\) represent the coefficients on the sine functions: \\(\\alpha_{s_1}, ~ \\alpha_{s_2}, ~ \\alpha_{s_3}, ~ \\ldots, ~ \\alpha_{s_6}\\). Similarly, the values C1, C2, C3, \\(\\ldots\\) represent the coefficients on the corresponding cosine terms: \\(\\alpha_{c_1}, ~ \\alpha_{c_2}, ~ \\alpha_{c_3}, ~ \\ldots, ~ \\alpha_{c_6}\\).\nAdjust the values of the parameters to create different seasonal patterns. Note that this is just a sum of sine and cosine function with various frequencies and amplitudes.",
    "crumbs": [
      "Lesson 2",
      "Harmonic Seasonal Variables - Part 1"
    ]
  },
  {
    "objectID": "chapter_5_lesson_2.html#homework-preview-5-min",
    "href": "chapter_5_lesson_2.html#homework-preview-5-min",
    "title": "Harmonic Seasonal Variables - Part 1",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_5_2.qmd",
    "crumbs": [
      "Lesson 2",
      "Harmonic Seasonal Variables - Part 1"
    ]
  },
  {
    "objectID": "chapter_5_lesson_1.html",
    "href": "chapter_5_lesson_1.html",
    "title": "Linear Models, GLS, and Seasonal Indicator Variables",
    "section": "",
    "text": "Explain the difference between stochastic and deterministic trends in time series\n\n\nDescribe deterministic trends as smooth, predictable changes over time\nDefine stochastic trends as random, unpredictable fluctuations\nExplain the different treatment of stochastic and deterministic trends when forecasting\n\n\n\n\nFit linear regression models to time series data\n\n\nDefine a linear time series model\nExplain why ordinary linear regression systematically underestimates of the standard error of parameter estimates when the error terms are autocorrelated\nApply generalized least squares GLS in R to estimate linear regression model parameters\nExplain how to estimate the autocorrelation input for the GLS algorithm\nCompare GLS and OLS standard error estimates to evaluate autocorrelation bias\nIdentify an appropriate function to model the trend in a given time series\nRepresent seasonal factors in a regression model using indicator variables\nFit a linear model for a simulated time series with linear time trend and \\(AR(p)\\) error\nUse acf and pacf to test for autocorrelation in the residuals\nEstimate a seasonal indicator model using GLS\nForecast using a fitted GLS model with seasonal indicator variables\n\n\n\n\nApply differencing to nonstationary time series\n\n\nTransform a non-stationary linear to a stationary process using differencing\nState how to remove a polynomial trend of order \\(m\\)\n\n\n\n\nSimulate time series\n\n\nSimulate a time series with a linear time trend and a \\(AR(p)\\) error",
    "crumbs": [
      "Lesson 1",
      "Linear Models, GLS, and Seasonal Indicator Variables"
    ]
  },
  {
    "objectID": "chapter_5_lesson_1.html#learning-outcomes",
    "href": "chapter_5_lesson_1.html#learning-outcomes",
    "title": "Linear Models, GLS, and Seasonal Indicator Variables",
    "section": "",
    "text": "Explain the difference between stochastic and deterministic trends in time series\n\n\nDescribe deterministic trends as smooth, predictable changes over time\nDefine stochastic trends as random, unpredictable fluctuations\nExplain the different treatment of stochastic and deterministic trends when forecasting\n\n\n\n\nFit linear regression models to time series data\n\n\nDefine a linear time series model\nExplain why ordinary linear regression systematically underestimates of the standard error of parameter estimates when the error terms are autocorrelated\nApply generalized least squares GLS in R to estimate linear regression model parameters\nExplain how to estimate the autocorrelation input for the GLS algorithm\nCompare GLS and OLS standard error estimates to evaluate autocorrelation bias\nIdentify an appropriate function to model the trend in a given time series\nRepresent seasonal factors in a regression model using indicator variables\nFit a linear model for a simulated time series with linear time trend and \\(AR(p)\\) error\nUse acf and pacf to test for autocorrelation in the residuals\nEstimate a seasonal indicator model using GLS\nForecast using a fitted GLS model with seasonal indicator variables\n\n\n\n\nApply differencing to nonstationary time series\n\n\nTransform a non-stationary linear to a stationary process using differencing\nState how to remove a polynomial trend of order \\(m\\)\n\n\n\n\nSimulate time series\n\n\nSimulate a time series with a linear time trend and a \\(AR(p)\\) error",
    "crumbs": [
      "Lesson 1",
      "Linear Models, GLS, and Seasonal Indicator Variables"
    ]
  },
  {
    "objectID": "chapter_5_lesson_1.html#preparation",
    "href": "chapter_5_lesson_1.html#preparation",
    "title": "Linear Models, GLS, and Seasonal Indicator Variables",
    "section": "Preparation",
    "text": "Preparation\n\nRead Sections 5.1-5.5",
    "crumbs": [
      "Lesson 1",
      "Linear Models, GLS, and Seasonal Indicator Variables"
    ]
  },
  {
    "objectID": "chapter_5_lesson_1.html#learning-journal-exchange-10-min",
    "href": "chapter_5_lesson_1.html#learning-journal-exchange-10-min",
    "title": "Linear Models, GLS, and Seasonal Indicator Variables",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading another student’s?\nWhat would you recommend the other student add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 1",
      "Linear Models, GLS, and Seasonal Indicator Variables"
    ]
  },
  {
    "objectID": "chapter_5_lesson_1.html#small-group-activity-linear-models-5-min",
    "href": "chapter_5_lesson_1.html#small-group-activity-linear-models-5-min",
    "title": "Linear Models, GLS, and Seasonal Indicator Variables",
    "section": "Small Group Activity: Linear Models (5 min)",
    "text": "Small Group Activity: Linear Models (5 min)\n\nDefinition of Linear Models\n\n\n\n\n\n\nLinear Model for a Time Series\n\n\n\nA model for a time series \\(\\{x_t : t = 1 \\ldots n \\}\\) is linear if it can be written in the form \\[\nx_t = \\alpha_0 + \\alpha_1 u_{1,t} + \\alpha_2 u_{2,t} + \\alpha_3 u_{3,t} + \\ldots + \\alpha_m u_{m,t} + z_t\n\\]\nwhere \\(u_{i,t}\\) represents the \\(i^{th}\\) predictor variable observed at time \\(t\\), \\(z_t\\) is the value of the error time series at time \\(t\\), the values \\(\\{\\alpha_0, \\alpha_1, \\alpha_2, \\ldots, \\alpha_m\\}\\) are model parameters, and \\(i = 1, 2, \\ldots, n\\) and \\(t = 1, 2, \\ldots, m\\).\nThe error terms \\(z_t\\) have mean 0, but they do not need to follow any particular distribution or be independent.\n\n\nWith a partner, practice determining which models are linear and which are non-linear.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhich of the following models are linear?\n\nModel 1: \\[\n  x_t = \\alpha_0 + \\alpha_1 \\sin(t^2-1) + \\alpha_2 e^{t} + z_t\n\\]\nModel 2: \\[\n  x_t = \\sqrt{ \\alpha_0 + \\alpha_1 t + z_t }\n\\]\nModel 3: \\[\n  x_t = \\alpha_0 + \\alpha_1 t^2 + \\alpha_2 ( t \\cdot \\sqrt{t-3} ) + z_t\n\\]\nModel 4: \\[\n  x_t = \\frac{ \\alpha_0 + \\alpha_1 t }{ 1 + \\alpha_2 \\sqrt{t+1} } + z_t\n\\]\nModel 5: \\[\n  x_t = \\alpha_0 + \\alpha_1 t + \\alpha_2 t^2 + \\alpha_3 t^3 + z_t\n\\]\nModel 6: \\[\n  x_t = \\alpha_0 + \\alpha_1 t + \\alpha_2 t^2 + \\alpha_1 \\alpha_2 t^3 + z_t\n\\]\n\nIs there a way to transform the following non-linear models into a linear model? If so, apply the transformation.\n\nModel 7: \\[\n  x_t = \\sqrt{ \\alpha_0 + \\alpha_1 t + z_t }\n\\]\nModel 8: \\[\n  x_t = \\sin( \\alpha_1 + \\alpha_2 t + z_t )\n\\]\nModel 9: \\[\n  x_t = \\alpha_0 \\sin( \\alpha_0 + \\alpha_1 t + z_t )\n\\]\nModel 10: \\[\n  x_t = \\alpha_0 ~ e^{ \\alpha_1 t + \\alpha_2 t^2 + z_t }\n\\]\n\n\n\n\n\n\nStationarity of Linear Models\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nComplete the following quotes from page 93 of the textbook:\n\n“Linear models for time series are non-stationary when they include functions of ______.”\n______________________ can often transform a non-stationary series with a deterministic trend to a stationary series.\n\n\n\nThe book presents a time series that is modeled as a linear function of time plus white noise: \\[\n  x_t = \\alpha_0 + \\alpha_1 t + z_t\n\\] If we difference these values, we get: \\[\\begin{align*}\n  \\nabla x_t\n    &= x_t - x_{t-1} \\\\\n    &= \\left[ \\alpha_0 + \\alpha_1 t + z_t \\right] - \\left[ \\alpha_0 + \\alpha_1 (t-1) + z_t \\right] \\\\\n    &= z_t - z_{t-1} + \\alpha_1\n\\end{align*}\\]\nGiven that the white noise process \\(\\{ z_t \\}\\) is stationary, then \\(\\{ \\nabla x_t \\}\\) is stationary. Differencing is a powerful tool for making a time series stationary.\nIn Chapter 4 Lesson 2, we established that taking the difference of a non-stationary time series with a stochastic trend can convert it to a stationary time series. Later in the same lesson, we learn that a linear deterministic trend can be removed by taking the first difference. A quadratic deterministic trend can be removed by taking the second differences, and so on.\n\n\n\n\n\n\nNote\n\n\n\nIn general, if there is an \\(m^{th}\\) degree polynomial trend in a time series, we can remove it by taking \\(m\\) differences.",
    "crumbs": [
      "Lesson 1",
      "Linear Models, GLS, and Seasonal Indicator Variables"
    ]
  },
  {
    "objectID": "chapter_5_lesson_1.html#class-activity-fitting-models-15-min",
    "href": "chapter_5_lesson_1.html#class-activity-fitting-models-15-min",
    "title": "Linear Models, GLS, and Seasonal Indicator Variables",
    "section": "Class Activity: Fitting Models (15 min)",
    "text": "Class Activity: Fitting Models (15 min)\n\nSimulation\nIn Section 5.2.3, the textbook illustrates the time series \\[\n  x_t = 50 + 3t + z_t\n\\] where \\(\\{ z_t \\}\\) is the \\(AR(1)\\) process \\(z_t = 0.8 z_{t-1} + w_t\\) and \\(\\{ w_t \\}\\) is a Gaussian white noise process with \\(\\sigma = 20\\). The code below simulates this time series and creates the resulting time plot.\n\n\nShow the code\nset.seed(1)\n\ndat &lt;- tibble(w = rnorm(100, sd = 20)) |&gt;\n    mutate(\n        Time = 1:n(),\n         z = purrr::accumulate2(\n            lag(w), w, \n            \\(acc, nxt, w) 0.8 * acc + w,\n            .init = 0)[-1],\n          x = 50 + 3 * Time + z\n            ) |&gt;\n    tsibble::as_tsibble(index = Time)\ndat |&gt; autoplot(.var = x) +\n  theme_minimal()\n\n\n\n\n\n\n\n\nFigure 1: Time plot of a simulated time series with a linear trend.\n\n\n\n\n\nThe advantage of simulation is that when we fit a model to the data, we can asses the fit…we know exactly what was simulated.\n\n\nModel Fitted to the Simulated Data\nWhen applying ordinary least-squares multiple regression, it is common to fit the model by minimizing the sum of squared errors: \\[\n  \\sum r_i^2 = \\sum \\left( x_t - \\left[ \\alpha_0 + \\alpha_1 u_{1,t} + \\alpha_2 u_{2,t} + \\ldots + \\alpha_m u_{m,t} \\right] \\right)^2\n\\] In R, we accomplish this using the lm function.\nWe assume that the simulated time series above follows the model \\[\n  x_t = \\alpha_0 + \\alpha_1 t + z_t\n\\]\n\n\nShow the code\ndat_lm &lt;- dat |&gt;\n  model(lm = TSLM(x ~ Time))\nparams &lt;- tidy(dat_lm) |&gt; pull(estimate)\nstderr &lt;- tidy(dat_lm) |&gt; pull(std.error)\n\n\nThis gives the fitted equation \\[\n  \\hat x_t = 58.551 + 3.063 t  \n\\]\nThe standard errors of these estimated parameters tend to be underestimated by the ordinary least squares method. To illustrate this, a simulation of n = 1000 realizations of the time series above was conducted.\nThe parameter estimates and their standard errors are summarized in the table below. The “Computed SE” is the standard error reported by the lm function in R. The “Simulated SE” is the standard deviation of the parameter estimated obtained in the 1000 simulated realizations of this time series.\n\n\nShow the code\n# This code simulates the data for this example\n#\n# parameter_est &lt;- data.frame(alpha0 = numeric(), alpha1 = numeric())\n# for(count in 1:n) {\n#   dat &lt;- tibble(w = rnorm(100, sd = 20)) |&gt;\n#       mutate(\n#           Time = 1:n(),\n#            z = purrr::accumulate2(\n#               lag(w), w, \n#               \\(acc, nxt, w) 0.8 * acc + w,\n#               .init = 0)[-1],\n#             x = 50 + 3 * Time + z\n#               ) |&gt;\n#       tsibble::as_tsibble(index = Time)\n#   \n#   dat_lm &lt;- dat |&gt;\n#   model(lm = TSLM(x ~ Time))\n# \n#   parameters &lt;- tidy(dat_lm) |&gt; pull(estimate)\n#   parameter_est &lt;- parameter_est |&gt;\n#     bind_rows(data.frame(alpha0 = parameters[1], alpha1 = parameters[2])) \n# }\n# \n# parameter_est |&gt; \n#   rio::export(\"data/chapter_5_lesson_1_simulation.parquet\")\n\nparameter_est &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/chapter_5_lesson_1_simulation.parquet\")\n\n\nstandard_errors &lt;- parameter_est |&gt;\n  summarize(\n    alpha0 = sd(alpha0), \n    alpha1 = sd(alpha1)\n  )\n\n\n\n\n\nTable 1: Autocorrelation function and partial autorcorrelation function of the residuals from the fitted linear model for the simulated data. Several realizations of the time series were simulated, and the standard deviation of the estimated parameters from the simulation is compared against the standard errors obtained by ordinary least squares regression.\n\n\n\n\n\nParameter\nEstimate\nComputed SE\nSimulated SE\n\n\n\n\n\\(\\alpha_0\\)\n58.551\n4.88\n17.798\n\n\n\\(\\alpha_1\\)\n3.063\n0.084\n0.315\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nNote that the simulated standard errors are much larger than those obtained by the lm function in R. The standard errors reported by R will lead to unduly small confidence intervals. This illustrates the problem of applying the standard least-squares estimates to time series data.\n\n\nAfter fitting a regression model, it is appropriate to review the relevant diagnostic plots. Here are the correlogram and partial correlogram of the residuals.\n\n\nShow the code\nacf_plot &lt;- residuals(dat_lm) |&gt; feasts::ACF() |&gt; autoplot()\n\npacf_plot &lt;- residuals(dat_lm) |&gt; feasts::PACF() |&gt; autoplot()\n\nacf_plot | pacf_plot\n\n\n\n\n\n\n\n\nFigure 2: Autocorrelation function and partial autorcorrelation function of the residuals from the fitted linear model for the simulated data.\n\n\n\n\n\nRecall that in our simulation, the residuals were modeled by an \\(AR(1)\\) process. So, it is not surprising that the residuals are correlated and that the partial autocorrelation function is only significant for the first lag.\n\n\nFitting a Regression Model to the Global Temperature Time Series\nIn Chapter 4 Lesson 2, we fit AR models to data representing the change in the Earth’s mean annual temperature from 1880 to 2023. These values represent the deviation of the mean global surface temperature from the long-term average from 1951 to 1980. (Source: NASA/GISS.) We will consider the portion of the time series beginning in 1970.\n\n\nShow the code\ntemps_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/global_temparature.csv\") |&gt;\n  as_tsibble(index = year) |&gt;\n  filter(year &gt;= 1970)\n\ntemps_ts |&gt; autoplot(.vars = change) +\n    labs(\n      x = \"Year\",\n      y = \"Temperature Change (Celsius)\",\n      title = paste0(\"Change in Mean Annual Global Temperature (\", min(temps_ts$year), \"-\", max(temps_ts$year), \")\")\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    ) + \n  geom_smooth(method='lm', formula= y~x)\n\n\n\n\n\n\n\n\nFigure 3: Time plot of the mean annual temperature globally for select years.\n\n\n\n\n\nVisually, it is easy to spot a positive trend in these data. Nevertheless, the ordinary least squares technique underestimates the standard error of the constant and slope term. This can lead to errant conclusions that a linear relationship is statistically significant when it is not.\n\n\nShow the code\nglobal &lt;- tibble(x = scan(\"https://byuistats.github.io/timeseries/data/global.dat\")) |&gt;\n    mutate(\n        date = seq(\n            ymd(\"1856-01-01\"),\n            by = \"1 months\",\n            length.out = n()),\n        year = year(date),\n        year_month = tsibble::yearmonth(date),\n        stats_time = year + c(0:11)/12)\n\nglobal_ts &lt;- global |&gt;\n    as_tsibble(index = year_month)\n\ntemp_tidy &lt;- global_ts |&gt; filter(year &gt;= 1970)\ntemp_lm &lt;- temp_tidy |&gt;\n  model(lm = TSLM(x ~ stats_time ))\n# tidy(temp_lm) |&gt; pull(estimate)\n\ntidy(temp_lm) |&gt;\n  mutate(\n    lower = estimate + qnorm(0.025) * std.error,\n    upper = estimate + qnorm(0.975) * std.error\n  ) |&gt;\n  display_table()\n\n\n\n\nTable 2: Parameter estimates for the fitted global temperature time series.\n\n\n\n\n\n\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\nlower\nupper\n\n\n\n\nlm\n(Intercept)\n-34.920409\n1.164899\n-29.97721\n0\n-37.2035680\n-32.6372500\n\n\nlm\nstats_time\n0.017654\n0.000586\n30.12786\n0\n0.0165055\n0.0188025\n\n\n\n\n\n\n\n\n\n\nThe 95% confidence interval for the slope does not contain zero. If the errors were independent, this would be conclusive evidence that there is a significant linear relationship between the year and the global temperature.\nNote that there is significant positive autocorrelation in the residual series for short lags. This implies that the standard errors will be underestimated, and the confidence interval is inappropriately narrow.\nHere are the values of the autocorrelation function of the residuals:\n\n\n\n\nTable 3: Autocorrelation function of the residuals for the global temperature time series.\n\n\n\n\n\n\n1M\n2M\n3M\n4M\n5M\n6M\n7M\n8M\n9M\n10M\n\n\n\n\n0.706\n0.638\n0.5\n0.447\n0.375\n0.307\n0.254\n0.206\n0.12\n0.078\n\n\n\n\n\n\n\n\n\n\nThe autocorrelation for \\(k=1\\) is 0.706. We will use this value when we apply the Generalized Least Squares method in the next section.\nTo get an intuitive idea of why ordinary least squares underestimates the standard errors of the estimates, the positive autocorrelation of adjacent observations leads to a series of data that have an effective record length that is shorter than the number of observations. This happens because similar observations tend to be clumped together, and the overall variation is thereby understated.\n\n\nApplying Generalized Least Squares, GLS\nThe autocorrelation in the data make ordinary least squares estimation inappropriate. What caped superhero comes to our rescue? None other than Captain GLS – the indominable Generalized Least Squares algorithm!\nThis fitting procedure handles the autocorrelation by maximizing the likelihood of the data, taking into account the autocorrelation in the residuals. This leads to much more appropriate standard errors for the parameter estimates. We will pass the value of the acf at lag \\(k=1\\) into the regression function.\n\n# Load additional packages\npacman::p_load(tidymodels, multilevelmod,\n  nlme, broom.mixed)\n\ntemp_spec &lt;- linear_reg() |&gt;\n  set_engine(\"gls\", correlation = nlme::corAR1(0.706))\n\ntemp_gls &lt;- temp_spec |&gt;\n  fit(x ~ stats_time, data = global_ts)\n\ntidy(temp_gls) |&gt;\n  mutate(\n    lower = estimate + qnorm(0.025) * std.error,\n    upper = estimate + qnorm(0.975) * std.error\n  )\n\n\n\n\n\nTable 4: Parameter estimates for the Generalized Least Squares estimates of the fit for the global temperature time series.\n\n\n\n\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\nlower\nupper\n\n\n\n\n(Intercept)\n-9.1361773\n0.4547490\n-20.09059\n0\n-10.0274689\n-8.2448856\n\n\nstats_time\n0.0046599\n0.0002354\n19.79209\n0\n0.0041985\n0.0051214\n\n\n\n\n\n\n\n\n\n\nAs we observed visually, there is evidence to suggest that there is a linear trend in the data.",
    "crumbs": [
      "Lesson 1",
      "Linear Models, GLS, and Seasonal Indicator Variables"
    ]
  },
  {
    "objectID": "chapter_5_lesson_1.html#class-activity-linear-models-with-seasonal-variables-10-min",
    "href": "chapter_5_lesson_1.html#class-activity-linear-models-with-seasonal-variables-10-min",
    "title": "Linear Models, GLS, and Seasonal Indicator Variables",
    "section": "Class Activity: Linear Models with Seasonal Variables (10 min)",
    "text": "Class Activity: Linear Models with Seasonal Variables (10 min)\nSome time series involve seasonal variables. In this section, we will address one way to include seasonality in a regression analysis of a time series. In the next lesson, we will explore another method.\n\nAdditive Seasonal Indicator Variables\nRecall the time series representing the monthly relative number of Google searches for the term “chocolate.” Here is a time plot of the data from 2021 to 2023. Month 1 is January 2021, month 13 is January 2022, and month 25 is January 2023.\n\n\n\n\n\n\n\n\nFigure 4: Time plot of a portion of the chocolate search data.\n\n\n\n\n\nWe fit a regression line to these data.\n\n\n\n\n\n\n\n\nFigure 5: The regression line is superimposed on the chocolate search time series.\n\n\n\n\n\nNow, we draw a line that is parallel to the regression line (has the same slope) but has a Y-intercept of 0.\n\n\n\n\n\n\n\n\nFigure 6: A line parallel to the regression line that passes through the origin is added to the chocolate search time plot.\n\n\n\n\n\nFor each month, we find the average amount that the relative number of google seachers that month deviates from the orange line (that is parallel to the regression line and passes through the origin). So, the length of the green line is the same for every January, etc.\n\n\n\n\n\n\n\n\nFigure 7: Representation of a linear model with seasonal indicator variables for the chocolate search time series data.\n\n\n\n\n\nWhen the bottom of the green arrow is on the orange line, the top of the green arrow is the estimate of the value of the time series for that month.\nWe will create a linear model that includes a constant term for each month. This constant monthly term is called a seasonal indicator variable. This name is derived from the fact that each variable indicates (either as 1 or 0) whether a given month is represented. For example, one of the seasonal indicator variables will represent January. It will be equal to 1 for any value of \\(t\\) representing an observation drawn in January and 0 otherwise. Indicator variables are also called dummy varaibles.\nThis additive model with seasonal indicator variables can be perceived similarly to other additive models with a seasonal component:\n\\[\n  x_t = m_t + s_t + z_t\n\\] where \\[\n  s_t =\n    \\begin{cases}\n      \\beta_1, & t ~\\text{falls in season}~ 1 \\\\\n      \\beta_2, & t ~\\text{falls in season}~ 2 \\\\\n      ⋮~~~~ & ~~~~~~~~~~~~⋮ \\\\\n      \\beta_s, & t ~\\text{falls in season}~ s\n    \\end{cases}\n\\] and \\(s\\) is the number of seasons in one cycle/period, and \\(n\\) is the number of observations, so \\(t = 1, 2, \\ldots, n\\) and \\(i = 1, 2, \\ldots, s\\), and \\(z_t\\) is the residual error series, which can be autocorrelated.\nIt is important to note that \\(m_t\\) does not need to be a constant. It can be a linear trend: \\[\n  m_t = \\alpha_0 + \\alpha_1 t\n\\] or quadratic: \\[\n  m_t = \\alpha_0 + \\alpha_1 t + \\alpha_2 t^2\n\\] a polynomial of degree \\(p\\): \\[\n  m_t = \\alpha_0 + \\alpha_1 t + \\alpha_2 t^2 + \\cdots + \\alpha_p t^p\n\\] or any other function of \\(t\\).\nIf \\(s_t\\) has the same value for all corresponding seasons, then we can write the model as: \\[\n  x_t = m_t + \\beta_{1 + [(t-1) \\mod s]} + z_t\n\\]\nPutting this all together, if we have a time series that has a linear trend and monthly observations where \\(t=1\\) corresponds to January, then the model becomes: \\[\\begin{align*}\n  x_t\n    &= ~~~~ \\alpha_1 t + s_t + z_t \\\\\n    &=\n      \\begin{cases}\n        \\alpha_1 t + \\beta_1 + z_t, & t = 1, 13, 25, \\ldots ~~~~ ~~(January) \\\\\n        \\alpha_1 t + \\beta_2 + z_t, & t = 2, 14, 26, \\ldots ~~~~ ~~(February) \\\\\n        ~~~~~~~~⋮ & ~~~~~~~~~~~~⋮ \\\\\n        \\alpha_1 t + \\beta_{12} + z_t, & t = 12, 24, 36, \\ldots ~~~~ (December)\n      \\end{cases}\n\\end{align*}\\]\nThis is the model illustrated in Figure 7. The orange line represents the term \\(\\alpha_1 t\\) and the green arrows represent the values of \\(\\beta_1, ~ \\beta_2, ~ \\ldots, ~ \\beta_{12}\\).\nThe folded chunk of code below fits the model to the data and computes the estimated parameter values.\n\n\nShow the code\nchocolate_month &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/chocolate.csv\") |&gt;\n  mutate(\n    dates = yearmonth(ym(Month)),\n    month = month(dates),\n    year = year(dates),\n    stats_time = year + (month - 1) / 12,\n    month_seq = 1:n()\n  ) |&gt;\n  mutate(month = factor(month)) |&gt;\n  as_tsibble(index = dates)\n\n# Fit regression model\nchocolate_lm &lt;- chocolate_month |&gt;\n  model(TSLM(chocolate ~ 0 + stats_time + month))\n\n# Estimated parameter values\nparam_est &lt;- chocolate_lm |&gt;\n  tidy() |&gt;\n  pull(estimate)\n\n\nThe estimated value of \\(\\alpha_1\\) is 1.1316416. The estimated values for the \\(\\beta_i\\) parameters are:\n\n\n\n\nTable 5: Estimated values of \\(\beta_i\\) for the fitted model for chocolate search time series.\n\n\n\n\n\n\nJan\nFeb\nMar\nApr\nMay\nJun\n\n\n\n\n-2227.91\n-2219.305\n-2233.149\n-2233.443\n-2234.838\n-2237.932\n\n\n\n\n\n\n\n\n\n\nJul\nAug\nSep\nOct\nNov\nDec\n\n\n\n\n-2236.476\n-2237.37\n-2237.515\n-2232.659\n-2223.003\n-2200.098\n\n\n\n\n\n\n\n\n\n\nNow, we compute forecasted (future) values for the relative number of Google searches for the word “Chocolate.”\n\n\nShow the code\nnum_years_to_forecast &lt;- 5\n\nnew_dat &lt;- chocolate_month |&gt;\n  as_tibble() |&gt;\n  tail(num_years_to_forecast * 12) |&gt;\n  dplyr::select(stats_time, month) |&gt;\n  mutate(\n    stats_time = stats_time + num_years_to_forecast,\n    alpha = tidy(chocolate_lm) |&gt; slice(1) |&gt; pull(estimate),\n    beta = rep(tidy(chocolate_lm) |&gt; slice(2:13) |&gt; pull(estimate), num_years_to_forecast)\n  )\n\nchocolate_forecast &lt;- chocolate_lm |&gt;\n  forecast(new_data=as_tsibble(new_dat, index = stats_time)) \n\nchocolate_forecast |&gt; \n  autoplot(chocolate_month, level = 95) +\n  labs(\n      x = \"Month\",\n      y = \"Relative Count of Google Searches\",\n      title = paste0(\"Google Searches for 'Chocolate' (\", min(chocolate_month$year), \"-\", max(chocolate_month$year), \")\"),\n      subtitle = paste0(num_years_to_forecast, \"-Year Forecast Based on a Linear Model with Seasonal Indicator Variables\")\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5),\n      plot.subtitle = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\nFigure 8: Forecasted values of the relative number of Google searches for the word ‘chocolate.’\n\n\n\n\n\n\n\n\n\n\nShow the code\nnew_dat |&gt;\n  mutate(estimate = alpha * stats_time + beta) |&gt;\n  display_partial_table(nrow_head = 4, nrow_tail = 3, decimals = 3)\n\n\n\n\nTable 6: Select forecasted values of the relative count of Google searches for ‘chocolate’.\n\n\n\n\n\n\nstats_time\nmonth\nalpha\nbeta\nestimate\n\n\n\n\n2024\n1\n1.132\n-2227.91\n62.532\n\n\n2024.083\n2\n1.132\n-2219.305\n71.232\n\n\n2024.167\n3\n1.132\n-2233.149\n57.482\n\n\n2024.25\n4\n1.132\n-2233.443\n57.282\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n2028.75\n10\n1.132\n-2232.659\n63.159\n\n\n2028.833\n11\n1.132\n-2223.003\n72.909\n\n\n2028.917\n12\n1.132\n-2200.098\n95.909\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 6 gives a few of the forecasted values illustrated in Figure 8.",
    "crumbs": [
      "Lesson 1",
      "Linear Models, GLS, and Seasonal Indicator Variables"
    ]
  },
  {
    "objectID": "chapter_5_lesson_1.html#small-group-activity-application-to-retail-sales-data-15-min",
    "href": "chapter_5_lesson_1.html#small-group-activity-application-to-retail-sales-data-15-min",
    "title": "Linear Models, GLS, and Seasonal Indicator Variables",
    "section": "Small Group Activity: Application to Retail Sales Data (15 min)",
    "text": "Small Group Activity: Application to Retail Sales Data (15 min)\nRecall the monthly U.S. retail sales data. We will consider the total retail sales for women’s clothing for 2004-2006. You can use the code below to download the data into a tsibble.\n\n# Read in Women's Clothing Retail Sales data\nretail_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/retail_by_business_type.parquet\") |&gt;\n  filter(naics == 44812) |&gt;\n  filter(month &gt;= yearmonth(my(\"Jan 2004\")) & month &lt;= yearmonth(my(\"Dec 2006\"))) |&gt;\n  mutate(month_seq = 1:n()) |&gt;\n  mutate(year = year(month)) |&gt;\n  mutate(month_num = month(month)) |&gt;\n  as_tsibble(index = month)\n\n\n\n\n\n\n\n\n\nFigure 9: Time plot of the aggregated monthly sales for women’s clothing stores in the United States.\n\n\n\n\n\nWe define the model as : \\[\\begin{align*}\n  x_t\n    &= ~~~~ \\alpha_1 t + s_t + z_t \\\\\n    &=\n      \\begin{cases}\n        \\alpha_1 t + \\beta_1 + z_t, & t = 1, 13, 25 ~~~~ ~~(January) \\\\\n        \\alpha_1 t + \\beta_2 + z_t, & t = 2, 14, 26 ~~~~ ~~(February) \\\\\n        ~~~~~~~~⋮ & ~~~~~~~~~~~~⋮ \\\\\n        \\alpha_1 t + \\beta_{12} + z_t, & t = 12, 24, 36 ~~~~ (December)\n      \\end{cases}\n\\end{align*}\\]\nWhen we regress the women’s clothing retail sales in millions, \\(x_t\\), on the month number, \\(t\\), the estimated simple linear regression equation is: \\[\n  \\hat x_t = 2635 + 24 ~ t\n\\] where \\(t = 1, 2, 3, \\ldots, 35, 36\\).\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUse the estimated regression equation \\[\n  \\hat x_t = 2635 + 24 ~ t\n\\] to do the following:\n\nFind the equation for the line parallel to the regression line that passes through the origin.\nCompute the deviation of each observed point from the line you just obtained.\nCompute the value of \\(\\hat \\beta_i\\) for each month, where \\(i = 1, 2, \\ldots, 12\\).\nCompute the estimate of the time series for \\(t = 1, 2, \\ldots, 36\\).\nPredict the value of the time series for the next six months.\n\n\n\n\n\n\n\n\n\n\n\n\nTable 7: Table used to compute the estimate of the women’s clothing retail sales time series using a linear model with seasonal indicator variables.\n\n\n\n\n\n$$Date$$\n$$t$$\n$$x_t$$\n$$\\hat \\alpha_1 t$$\n$$x_t - \\hat \\alpha_1 t$$\n$$\\hat\\beta_i$$\n$$\\hat x_t$$\n\n\n\n\nJan 2004\n1\n2267\n24\n2243\n2062.33\n2086.33\n\n\nFeb 2004\n2\n2402\n48\n2354\n\n\n\n\nMar 2004\n3\n2915\n\n\n\n\n\n\nApr 2004\n4\n3023\n\n\n\n\n\n\nMay 2004\n5\n3005\n\n\n\n\n\n\nJun 2004\n6\n2777\n144\n2633\n2569.67\n2713.67\n\n\nJul 2004\n7\n2595\n168\n2427\n2297\n2465\n\n\nAug 2004\n8\n2644\n192\n2452\n2345\n2537\n\n\nSep 2004\n9\n2774\n216\n2558\n2463\n2679\n\n\nOct 2004\n10\n2990\n240\n2750\n2574.33\n2814.33\n\n\nNov 2004\n11\n3090\n264\n2826\n2730\n2994\n\n\nDec 2004\n12\n4472\n288\n4184\n4173\n4461\n\n\nJan 2005\n13\n2338\n312\n2026\n2062.33\n2374.33\n\n\nFeb 2005\n14\n2453\n336\n2117\n\n\n\n\nMar 2005\n15\n3105\n\n\n\n\n\n\nApr 2005\n16\n3177\n\n\n\n\n\n\nMay 2005\n17\n3164\n\n\n\n\n\n\nJun 2005\n18\n3045\n432\n2613\n2569.67\n3001.67\n\n\nJul 2005\n19\n2741\n456\n2285\n2297\n2753\n\n\nAug 2005\n20\n2871\n480\n2391\n2345\n2825\n\n\nSep 2005\n21\n2963\n504\n2459\n2463\n2967\n\n\nOct 2005\n22\n3120\n528\n2592\n2574.33\n3102.33\n\n\nNov 2005\n23\n3342\n552\n2790\n2730\n3282\n\n\nDec 2005\n24\n4756\n576\n4180\n4173\n4749\n\n\nJan 2006\n25\n2518\n600\n1918\n2062.33\n2662.33\n\n\nFeb 2006\n26\n2518\n624\n1894\n\n\n\n\nMar 2006\n27\n3194\n\n\n\n\n\n\nApr 2006\n28\n3379\n\n\n\n\n\n\nMay 2006\n29\n3340\n\n\n\n\n\n\nJun 2006\n30\n3183\n720\n2463\n2569.67\n3289.67\n\n\nJul 2006\n31\n2923\n744\n2179\n2297\n3041\n\n\nAug 2006\n32\n2960\n768\n2192\n2345\n3113\n\n\nSep 2006\n33\n3164\n792\n2372\n2463\n3255\n\n\nOct 2006\n34\n3197\n816\n2381\n2574.33\n3390.33\n\n\nNov 2006\n35\n3414\n840\n2574\n2730\n3570\n\n\nDec 2006\n36\n5019\n864\n4155\n4173\n5037\n\n\nJan 2007\n37\n—\n888\n—\n2062.33\n2950.33\n\n\nFeb 2007\n38\n—\n912\n—\n\n\n\n\nMar 2007\n39\n—\n\n—\n\n\n\n\nApr 2007\n40\n—\n\n—\n\n\n\n\nMay 2007\n41\n—\n\n—\n\n\n\n\nJun 2007\n42\n—\n1008\n—\n2569.67\n3577.67",
    "crumbs": [
      "Lesson 1",
      "Linear Models, GLS, and Seasonal Indicator Variables"
    ]
  },
  {
    "objectID": "chapter_5_lesson_1.html#homework-preview-5-min",
    "href": "chapter_5_lesson_1.html#homework-preview-5-min",
    "title": "Linear Models, GLS, and Seasonal Indicator Variables",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_5_1.qmd \n\n\nRetail Sales\n\n\n\n\n\n\n$$Date$$\n$$t$$\n$$x_t$$\n$$\\hat \\alpha_1 t$$\n$$x_t - \\hat \\alpha_1 t$$\n$$\\hat\\beta_i$$\n$$\\hat x_t$$\n\n\n\n\nJan 2004\n1\n2267\n24\n2243\n2062.33\n2086.33\n\n\nFeb 2004\n2\n2402\n48\n2354\n2121.67\n2169.67\n\n\nMar 2004\n3\n2915\n72\n2843\n2711.33\n2783.33\n\n\nApr 2004\n4\n3023\n96\n2927\n2809\n2905\n\n\nMay 2004\n5\n3005\n120\n2885\n2761.67\n2881.67\n\n\nJun 2004\n6\n2777\n144\n2633\n2569.67\n2713.67\n\n\nJul 2004\n7\n2595\n168\n2427\n2297\n2465\n\n\nAug 2004\n8\n2644\n192\n2452\n2345\n2537\n\n\nSep 2004\n9\n2774\n216\n2558\n2463\n2679\n\n\nOct 2004\n10\n2990\n240\n2750\n2574.33\n2814.33\n\n\nNov 2004\n11\n3090\n264\n2826\n2730\n2994\n\n\nDec 2004\n12\n4472\n288\n4184\n4173\n4461\n\n\nJan 2005\n13\n2338\n312\n2026\n2062.33\n2374.33\n\n\nFeb 2005\n14\n2453\n336\n2117\n2121.67\n2457.67\n\n\nMar 2005\n15\n3105\n360\n2745\n2711.33\n3071.33\n\n\nApr 2005\n16\n3177\n384\n2793\n2809\n3193\n\n\nMay 2005\n17\n3164\n408\n2756\n2761.67\n3169.67\n\n\nJun 2005\n18\n3045\n432\n2613\n2569.67\n3001.67\n\n\nJul 2005\n19\n2741\n456\n2285\n2297\n2753\n\n\nAug 2005\n20\n2871\n480\n2391\n2345\n2825\n\n\nSep 2005\n21\n2963\n504\n2459\n2463\n2967\n\n\nOct 2005\n22\n3120\n528\n2592\n2574.33\n3102.33\n\n\nNov 2005\n23\n3342\n552\n2790\n2730\n3282\n\n\nDec 2005\n24\n4756\n576\n4180\n4173\n4749\n\n\nJan 2006\n25\n2518\n600\n1918\n2062.33\n2662.33\n\n\nFeb 2006\n26\n2518\n624\n1894\n2121.67\n2745.67\n\n\nMar 2006\n27\n3194\n648\n2546\n2711.33\n3359.33\n\n\nApr 2006\n28\n3379\n672\n2707\n2809\n3481\n\n\nMay 2006\n29\n3340\n696\n2644\n2761.67\n3457.67\n\n\nJun 2006\n30\n3183\n720\n2463\n2569.67\n3289.67\n\n\nJul 2006\n31\n2923\n744\n2179\n2297\n3041\n\n\nAug 2006\n32\n2960\n768\n2192\n2345\n3113\n\n\nSep 2006\n33\n3164\n792\n2372\n2463\n3255\n\n\nOct 2006\n34\n3197\n816\n2381\n2574.33\n3390.33\n\n\nNov 2006\n35\n3414\n840\n2574\n2730\n3570\n\n\nDec 2006\n36\n5019\n864\n4155\n4173\n5037\n\n\nJan 2007\n37\n—\n888\n—\n2062.33\n2950.33\n\n\nFeb 2007\n38\n—\n912\n—\n2121.67\n3033.67\n\n\nMar 2007\n39\n—\n936\n—\n2711.33\n3647.33\n\n\nApr 2007\n40\n—\n960\n—\n2809\n3769\n\n\nMay 2007\n41\n—\n984\n—\n2761.67\n3745.67\n\n\nJun 2007\n42\n—\n1008\n—\n2569.67\n3577.67",
    "crumbs": [
      "Lesson 1",
      "Linear Models, GLS, and Seasonal Indicator Variables"
    ]
  },
  {
    "objectID": "chapter_4_lesson_4.html",
    "href": "chapter_4_lesson_4.html",
    "title": "Fitted AR Models",
    "section": "",
    "text": "Fit time series models to data and interpret fitted parameters\n\n\nFit an \\(AR(p)\\) model to simulated data\nExplain the difference between parameters of the data generating process and estimates\nCalculate confidence intervals for AR coefficient estimates\nInterpret AR coefficient estimates in the context of the source and nature of historical data\n\n\n\n\nCheck model adequacy using diagnostic plots like correlograms of residuals\n\n\nCompare AR fitted models to an underlying data generating process\nExplain the limitations of stochastic model fitting as evidence in favor or against real world arguments.",
    "crumbs": [
      "Lesson 4",
      "Fitted AR Models"
    ]
  },
  {
    "objectID": "chapter_4_lesson_4.html#learning-outcomes",
    "href": "chapter_4_lesson_4.html#learning-outcomes",
    "title": "Fitted AR Models",
    "section": "",
    "text": "Fit time series models to data and interpret fitted parameters\n\n\nFit an \\(AR(p)\\) model to simulated data\nExplain the difference between parameters of the data generating process and estimates\nCalculate confidence intervals for AR coefficient estimates\nInterpret AR coefficient estimates in the context of the source and nature of historical data\n\n\n\n\nCheck model adequacy using diagnostic plots like correlograms of residuals\n\n\nCompare AR fitted models to an underlying data generating process\nExplain the limitations of stochastic model fitting as evidence in favor or against real world arguments.",
    "crumbs": [
      "Lesson 4",
      "Fitted AR Models"
    ]
  },
  {
    "objectID": "chapter_4_lesson_4.html#preparation",
    "href": "chapter_4_lesson_4.html#preparation",
    "title": "Fitted AR Models",
    "section": "Preparation",
    "text": "Preparation\n\nRead Sections 4.6-4.7",
    "crumbs": [
      "Lesson 4",
      "Fitted AR Models"
    ]
  },
  {
    "objectID": "chapter_4_lesson_4.html#learning-journal-exchange-10-min",
    "href": "chapter_4_lesson_4.html#learning-journal-exchange-10-min",
    "title": "Fitted AR Models",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading another student’s?\nWhat would you recommend the other student add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 4",
      "Fitted AR Models"
    ]
  },
  {
    "objectID": "chapter_4_lesson_4.html#class-activity-fitting-a-simulated-ar1-model-with-zero-mean-5-min",
    "href": "chapter_4_lesson_4.html#class-activity-fitting-a-simulated-ar1-model-with-zero-mean-5-min",
    "title": "Fitted AR Models",
    "section": "Class Activity: Fitting a Simulated \\(AR(1)\\) Model with Zero Mean (5 min)",
    "text": "Class Activity: Fitting a Simulated \\(AR(1)\\) Model with Zero Mean (5 min)\nWe will demonstrate how AR models are fitted via simulation. We will fit two different \\(AR(1)\\) models and an \\(AR(2)\\) model. The advantage of using simulation is that we know how the time series was constructed. So, we know the model that was used and the actual values of the parameters in that model. We can then see how close our estimated parameter values are to the true values.\n\nSimulate an \\(AR(1)\\) Time Series\nIn this simulation, we first simulate data from the \\(AR(1)\\) model \\[\n  x_t = 0.75 ~ x_{t-1} + w_t\n\\] where \\(w_t\\) is a white noise process with variance 1.\n\n\nShow the code\nset.seed(123)\nn_rep &lt;- 1000\nalpha1 &lt;- 0.75\n\ndat_ts &lt;- tibble(w = rnorm(n_rep)) |&gt;\n  mutate(\n    index = 1:n(),\n    x = purrr::accumulate2(\n      lag(w), w, \n      \\(acc, nxt, w) alpha1 * acc + w,\n      .init = 0)[-1]) |&gt;\n  tsibble::as_tsibble(index = index)\n\ndat_ts |&gt; \n  autoplot(.vars = x) +\n    labs(\n      x = \"Time\",\n      y = \"Simulated Time Series\",\n      title = \"Simulated Values from an AR(1) Process\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nThe R command mean(dat_ts$x) gives the mean of the \\(x_t\\) values as 0.067.\n\n\nFit an \\(AR(1)\\) Model with Zero Mean\n\n\nShow the code\n# Fit the AR(1) model\nfit_ar &lt;- dat_ts |&gt;\n  model(AR(x ~ order(1)))\ntidy(fit_ar)\n\n\n# A tibble: 1 × 6\n  .model           term  estimate std.error statistic   p.value\n  &lt;chr&gt;            &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 AR(x ~ order(1)) ar1      0.720    0.0220      32.8 2.07e-160\n\n\nThe estimate of the parameter \\(\\alpha_1\\) (i.e. the fitted value of the parameter \\(\\alpha_1\\)) is \\(\\hat \\alpha_1 = 0.72\\).\nWhen R fits an AR model, the mean of the time series is subtracted from the data before the parameter values are estimated. If R detects that the mean of the time series is not significantly different from zero, it is omitted from the output.\nBecause the mean is subtracted from the time series before the parameter values are estimated, R is using the model \\[\n  z_t = \\alpha_1 ~ z_{t-1} + w_t\n\\] where \\(z_t = x_t - \\mu\\) and \\(\\mu\\) is the mean of the time series.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nAnswer the following questions with your partner.\n\nUse the expression for \\(z_t\\) above to solve for \\(x_t\\) in terms of \\(x_{t-1}\\), \\(\\mu\\), \\(\\alpha_1\\), and \\(w_t\\).\nWhat does your model reduce to when \\(\\mu = 0\\)?\nExplain to your partner why this correctly models a time series with mean \\(\\mu\\).\n\n\n\nWe replace the parameter \\(\\mu\\) with its estimator \\(\\hat \\mu = \\bar x\\). We also replace \\(\\alpha_1\\) with the fitted value from the output \\(\\hat \\alpha_1\\). This gives us the fitted model: \\[\n  \\hat x_t = \\bar x + \\hat \\alpha_1 ~ (x_{t-1} - \\bar x)\n\\]\nThe fitted model can be expressed as:\n\\[\\begin{align*}\n  \\hat x_t\n    &= 0.067 + 0.72 \\left( x_{t-1} - 0.067 \\right) \\\\\n    &= 0.067 - 0.72 ~ (0.067) + 0.72 ~ \\left( x_{t-1} \\right) \\\\\n    &= 0.019 + 0.72 ~ x_{t-1}\n\\end{align*}\\]\nEven though R does not report the parameter for the mean of the process, \\(\\hat \\mu = 0.019\\), it is not significantly different from zero. One could argue that we should not use a model that contains the mean and instead focus on a simple fitted model that has only one parameter:\n\\[\n  \\hat x_t = 0.72 ~ x_{t-1}\n\\]\n\n\nConfidence Interval for the Model Parameter\nThe P-value given above tests the hypothesis that \\(\\alpha_1=0\\). This is not helpful in this context. We are interested in the plausible values for \\(\\alpha_1\\), not whether or not it is different from zero. For this reason, we consider a confidence interval and disregard the P-value.\nWe can compute an approximate 95% confidence interval for \\(\\alpha_1\\) as: \\[\n  \\left(\n    \\hat \\alpha_1 - 2 \\cdot SE_{\\hat \\alpha_1}\n    , ~\n    \\hat \\alpha_1 + 2 \\cdot SE_{\\hat \\alpha_1}\n  \\right)\n\\] where \\(\\hat \\alpha_1\\) is our parameter estimate and \\(SE_{\\hat \\alpha_1}\\) is the standard error of the estimate. Both of these values are given in the R output.\n\n\nShow the code\nci_summary &lt;- tidy(fit_ar) |&gt;\n    mutate(\n        lower = estimate - 2 * std.error,\n        upper = estimate + 2 * std.error\n    )\n\n\nSo, our 95% confidence interval for \\(\\alpha_1\\) is: \\[\n  \\left(\n  0.72 - 2 \\cdot 0.022\n  , ~\n  0.72 + 2 \\cdot 0.022\n  \\right)\n\\] or \\[\n  \\left(\n  0.676\n  , ~\n  0.764\n  \\right)\n\\] Note that the confidence interval contains \\(\\alpha_1 = 0.75\\), the value of the parameter we used in our simulation. The process of estimating the parameter worked well. In practice, we will not know the value of \\(\\alpha_1\\), but the confidence interval gives us a reasonable estimate of the value.\n\n\nResiduals\nFor an \\(AR(1)\\) model where the mean of the time series is not statistically significantly different from 0, the residuals are computed as \\[\\begin{align*}\n  r_t\n    &= x_t - \\hat x_t \\\\\n    &= x_t - \\left[ 0.72 ~ x_{t-1} \\right]\n\\end{align*}\\]\nWe can easily obtain these residual values in R:\nThe variance of the residuals is \\(0.982\\). This is very close to the actual value used in the simulation: \\(\\sigma^2 = 1\\).",
    "crumbs": [
      "Lesson 4",
      "Fitted AR Models"
    ]
  },
  {
    "objectID": "chapter_4_lesson_4.html#class-activity-fitting-a-simulated-ar1-model-with-non-zero-mean-5-min",
    "href": "chapter_4_lesson_4.html#class-activity-fitting-a-simulated-ar1-model-with-non-zero-mean-5-min",
    "title": "Fitted AR Models",
    "section": "Class Activity: Fitting a Simulated \\(AR(1)\\) Model with Non-Zero Mean (5 min)",
    "text": "Class Activity: Fitting a Simulated \\(AR(1)\\) Model with Non-Zero Mean (5 min)\n\nSimulate an \\(AR(1)\\) Time Series\nIt is easy to conceive situations where the mean of an AR model, \\(\\mu\\), is not zero. The model we have been fitting is \\[\n  x_t = \\mu + \\alpha_1 ~ \\left( x_{t-1} - \\mu \\right) + w_t\n\\] where \\(\\mu\\) and \\(\\alpha_1\\) are constants, and \\(w_t\\) is a white noise process with variance \\(\\sigma^2\\).\nThis model can be simplified by combining like terms. \\[\\begin{align*}\nx_t\n  &= \\mu + \\alpha_1 ~ \\left( x_{t-1} - \\mu \\right) + w_t \\\\\n  &= \\underbrace{\\mu - \\alpha_1 ~ (\\mu)}_{\\alpha_0} + \\alpha_1 ~ \\left( x_{t-1} \\right) + w_t \\\\\n  &= \\alpha_0 + \\alpha_1 ~ \\left( x_{t-1} \\right) + w_t\n\\end{align*}\\]\nSuppose the mean of the \\(AR(1)\\) process is \\(\\mu = 50\\). We will set \\(\\alpha_1 = 0.75\\), and \\(\\sigma^2 = 5\\) for this simulation. After specifying these numbers, the model becomes: \\[\\begin{align*}\n  x_t\n    &= 50 + 0.75 ~ ( x_{t-1} - 50 ) + w_t \\\\\n    &= 50 - 0.75 ~ ( 50 ) + 0.75 ~ x_{t-1} + w_t \\\\\n    &= 12.5 + 0.75 ~ x_{t-1} + w_t\n\\end{align*}\\] where \\(w_t\\) is a white noise process with variance \\(\\sigma^2 = 5\\).\n\n\nShow the code\nset.seed(123)\nn_rep &lt;- 1000\nalpha1 &lt;- 0.75\nsigma_sqr &lt;- 5\n\ndat_ts &lt;- tibble(w = rnorm(n = n_rep, sd = sqrt(sigma_sqr))) |&gt;\n  mutate(\n    index = 1:n(),\n    x = purrr::accumulate2(\n      lag(w), w, \n      \\(acc, nxt, w) alpha1 * acc + w,\n      .init = 0)[-1]) |&gt;\n  mutate(x = x + alpha0) |&gt; \n  tsibble::as_tsibble(index = index)\n\ndat_ts |&gt; \n  autoplot(.vars = x) +\n    labs(\n      x = \"Time\",\n      y = \"Simulated Time Series\",\n      title = \"Simulated Values from an AR(1) Process\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nThe R command mean(dat_ts$x) gives the mean of the \\(x_t\\) values as 50.15.\n\n\nFit an \\(AR(1)\\) Model with Non-Zero Mean\nWe now use R to fit an \\(AR(1)\\) model to the time series data.\n\n\nShow the code\n# Fit the AR(1) model\nfit_ar &lt;- dat_ts |&gt;\n  model(AR(x ~ order(1)))\ntidy(fit_ar)\n\n\n# A tibble: 2 × 6\n  .model           term     estimate std.error statistic   p.value\n  &lt;chr&gt;            &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 AR(x ~ order(1)) constant   14.1      1.11        12.7 1.35e- 34\n2 AR(x ~ order(1)) ar1         0.719    0.0220      32.7 5.95e-160\n\n\nThe estimate of the parameter for the constant (mean) term \\(\\alpha_0\\) is \\(\\hat \\alpha_0 = 14.091\\). The estimate of the parameter \\(\\alpha_1\\) (i.e. the fitted value of the parameter \\(\\alpha_1\\)) is \\(\\hat \\alpha_1 = 0.719\\).\n\nFitting the model \\[\n  x_t = \\alpha_0 + \\alpha_1 ~ x_{t-1} + w_t\n\\] we get \\[\\begin{align*}\n  \\hat x_t\n    &= \\hat \\alpha_0 + \\hat \\alpha_1 ~ x_{t-1} \\\\\n    &= 14.091 +\n      0.719\n         ~ x_{t-1}\n\\end{align*}\\]\n\n\nConfidence Intervals for the Model Parameters\nWe can compute approximate 95% confidence intervals for \\(\\alpha_0\\) and \\(\\alpha_1\\):\n\\[\n  \\left(\n    \\hat \\alpha_i - 2 \\cdot SE_{\\hat \\alpha_i}\n    , ~\n    \\hat \\alpha_i + 2 \\cdot SE_{\\hat \\alpha_i}\n  \\right)\n\\] where \\(\\hat \\alpha_i\\) is our estimate of parameter \\(i \\in \\{0,1\\}\\), and \\(SE_{\\hat \\alpha_i}\\) is the standard error of the respective estimates.\n\n\nShow the code\nci_summary &lt;- tidy(fit_ar) |&gt;\n    mutate(\n        lower = estimate - 2 * std.error,\n        upper = estimate + 2 * std.error\n    )\n\n\n\n\n\n95% Confidence Interval for \\(\\alpha_0\\): \\[\n  \\left(\n    \\hat \\alpha_0 - 2 \\cdot SE_{\\hat \\alpha_0}\n    , ~\n    \\hat \\alpha_0 + 2 \\cdot SE_{\\hat \\alpha_0}\n  \\right)\n\\]\n\\[\n  \\left(\n  14.091 - 2 \\cdot 1.105\n  , ~\n  14.091 + 2 \\cdot 1.105\n  \\right)\n\\]\n\\[\n  \\left(\n  11.88\n  , ~\n  16.301\n  \\right)\n\\] The confidence interval for \\(\\alpha_0\\) contains \\[\\alpha_0 = \\mu - \\alpha_1 ~ (\\mu) = 12.5\\]\n\n\n\n95% Confidence Interval for \\(\\alpha_1\\): \\[\n  \\left(\n    \\hat \\alpha_1 - 2 \\cdot SE_{\\hat \\alpha_1}\n    , ~\n    \\hat \\alpha_1 + 2 \\cdot SE_{\\hat \\alpha_1}\n  \\right)\n\\]\n\\[\n  \\left(\n  0.719 - 2 \\cdot 0.022\n  , ~\n  0.719 + 2 \\cdot 0.022\n  \\right)\n\\]\n\\[\n  \\left(\n  0.675\n  , ~\n  0.763\n  \\right)\n\\] The confidence interval for \\(\\alpha_1\\) contains \\[\\alpha_1 = 0.75\\]\n\n\n\nBoth intervals captured the true value used in the simulation. The process of estimating the parameter worked well. In practice, we will not know the value of \\(\\alpha_1\\), but the confidence interval gives us a reasonable estimate of the value. About 95% of the time, the confidence interval will capture the true parameter value.\n\n\nResiduals\nThe residuals in this model are computed as \\[\\begin{align*}\n  r_t\n    &= x_t - \\hat x_t \\\\\n    &= x_t -\n      \\left[\n        14.091 +\n      0.719\n         ~ x_{t-1}\n      \\right]\n\\end{align*}\\]\nThe variance of the residuals is \\(4.911\\), which is near the actual parameter value: \\(\\sigma^2 = 5\\).",
    "crumbs": [
      "Lesson 4",
      "Fitted AR Models"
    ]
  },
  {
    "objectID": "chapter_4_lesson_4.html#class-activity-fitting-a-simulated-ar2-model-10-min",
    "href": "chapter_4_lesson_4.html#class-activity-fitting-a-simulated-ar2-model-10-min",
    "title": "Fitted AR Models",
    "section": "Class Activity: Fitting a Simulated \\(AR(2)\\) Model (10 min)",
    "text": "Class Activity: Fitting a Simulated \\(AR(2)\\) Model (10 min)\n\nSimulate an \\(AR(2)\\) Time Series\nIn this section, we will simulate data from the following \\(AR(2)\\) process: \\[\n  x_t = 2 + 0.5 ~ x_{t-1} + 0.4 ~ x_{t-2} + w_t\n\\] where \\(w_t\\) is a discrete white noise process with variance \\(\\sigma^2 = 9\\).\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUse the \\(AR(2)\\) process above to answer the following questions.\n\nIs this \\(AR(2)\\) process stationary? (Hint: The characteristic polynomial only includes terms that involve \\(x_t\\).)\nRewrite the model in the form \\[\n  x_t = \\mu + \\alpha_1 ~ ( x_{t-1} - \\mu) + \\alpha_2 ~ ( x_{t-2} - \\mu) + w_t\n\\] Identify the value of each of the coefficients (\\(\\mu\\), \\(\\alpha_1\\), and \\(\\alpha_2\\)).      \nWhat is the mean of this \\(AR(2)\\) process?\n\n\n\nHere is a time plot of the simulated time series.\n\n\nShow the code\nset.seed(123)\nn_rep &lt;- 1000\nalpha0 &lt;- 20\nalpha1 &lt;- 0.5\nalpha2 &lt;- 0.4\nsigma_sqr &lt;- 9\n\ndat_ts &lt;- tibble(w = rnorm(n = n_rep, sd = sqrt(sigma_sqr))) |&gt;\n    mutate(\n      index = 1:n(),\n      x = 0\n    ) |&gt;\n    tsibble::as_tsibble(index = index)\n\n# Simulate x values\ndat_ts$x[1] &lt;- alpha0 + dat_ts$w[1]\ndat_ts$x[2] &lt;- alpha0 + alpha1 * ( dat_ts$x[1] - alpha0 ) + dat_ts$w[2]\nfor (i in 3:nrow(dat_ts)) {\n  dat_ts$x[i] &lt;- alpha0 + \n    alpha1 * ( dat_ts$x[i-1] - alpha0 ) + \n    alpha2 * ( dat_ts$x[i-2] - alpha0 ) + \n    dat_ts$w[i]\n}\n\ndat_ts |&gt; \n  autoplot(.vars = x) +\n    labs(\n      x = \"Time\",\n      y = \"Simulated Time Series\",\n      title = paste(\"Simulated Values from an AR(2) Process with Mean\", alpha0)\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\n\n\nFit an \\(AR(2)\\) Model\nWe fit an \\(AR(2)\\) model to these simulated values.\n\n\nShow the code\n# Fit the AR(2) model\nfit_ar &lt;- dat_ts |&gt;\n    model(AR(x ~ order(2))) \ntidy(fit_ar)\n\n\n# A tibble: 3 × 6\n  .model           term     estimate std.error statistic  p.value\n  &lt;chr&gt;            &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 AR(x ~ order(2)) constant    2.33     0.380       6.14 1.17e- 9\n2 AR(x ~ order(2)) ar1         0.478    0.0289     16.5  1.75e-54\n3 AR(x ~ order(2)) ar2         0.408    0.0289     14.1  1.96e-41\n\n\nThe estimates of the parameter values are:\n\\(\\hat \\alpha_0 = 2.333\\), \\(\\hat \\alpha_1 = 0.478\\), and \\(\\hat \\alpha_2 = 0.408\\). This means that our fitted model can be expressed as:\n\\[\\begin{align*}\n  \\hat x_t\n  &=\n    \\hat \\alpha_0\n    + \\hat \\alpha_1 ~ x_{t-1}\n    + \\hat \\alpha_2 ~ x_{t-2}\n    \\\\\n  &=\n    2.333\n    +\n    0.478\n    ~ x_{t-1}\n    +\n    0.408\n    ~ x_{t-2}\n\\end{align*}\\]\n\n\nConfidence Interval for the Model Parameters\nWe can compute an approximate 95% confidence interval for \\(\\alpha_i\\) as: \\[\n  \\left(\n    \\hat \\alpha_i - 2 \\cdot SE_{\\hat \\alpha_i}\n    , ~\n    \\hat \\alpha_i + 2 \\cdot SE_{\\hat \\alpha_i}\n  \\right)\n\\] where \\(\\hat \\alpha_i\\) is our estimate of the \\(i^{th}\\) parameter and \\(SE_{\\hat \\alpha_i}\\) is the standard error of the respective estimate. These values are given in the R output from the code below.\n\n\nShow the code\nci_summary &lt;- tidy(fit_ar) |&gt;\n    mutate(\n        lower = estimate - 2 * std.error,\n        upper = estimate + 2 * std.error\n    )\n\n\n\n\n\n95% confidence interval for \\(\\alpha_0\\): \\[\n  \\left(\n    \\hat \\alpha_0 - 2 \\cdot SE_{\\hat \\alpha_0}\n    , ~\n    \\hat \\alpha_0 + 2 \\cdot SE_{\\hat \\alpha_0}\n  \\right)\n\\] \\[\n  \\left(\n  2.333 - 2 \\cdot 0.38\n  ,\n  \\right.\n  ~~~~~~~~~~~~~~~~~~~\n\\] \\[\n  ~~~~~~~~~~~~~~~~~~~\n  \\left.\n  2.333 + 2 \\cdot 0.38\n  \\right)\n\\] \\[\n  \\left(\n  1.574\n  , ~\n  3.093\n  \\right)\n\\] This confidence interval contains \\(\\alpha_0 = 2\\).\n\n\n\n95% confidence interval for \\(\\alpha_1\\): \\[\n  \\left(\n    \\hat \\alpha_1 - 2 \\cdot SE_{\\hat \\alpha_1}\n    , ~\n    \\hat \\alpha_1 + 2 \\cdot SE_{\\hat \\alpha_1}\n  \\right)\n\\] \\[\n  \\left(\n  0.478 - 2 \\cdot 0.029\n  ,\n  \\right.\n  ~~~~~~~~~~~~~~~~~~~\n  \\] \\[\n  ~~~~~~~~~~~~~~~~~~~\n  \\left.\n  0.478 + 2 \\cdot 0.029\n  \\right)\n\\] \\[\n  \\left(\n  0.42\n  , ~\n  0.536\n  \\right)\n\\] This confidence interval contains \\(\\alpha_1 = 0.5\\).\n\n\n\n95% confidence interval for \\(\\alpha_2\\): \\[\n  \\left(\n    \\hat \\alpha_2 - 2 \\cdot SE_{\\hat \\alpha_2}\n    , ~\n    \\hat \\alpha_2 + 2 \\cdot SE_{\\hat \\alpha_2}\n  \\right)\n\\] \\[\n  \\left(\n  0.408 - 2 \\cdot 0.029\n  ,\n  \\right.\n  ~~~~~~~~~~~~~~~~~~~\n  \\] \\[\n  ~~~~~~~~~~~~~~~~~~~\n  \\left.\n  0.408 + 2 \\cdot 0.029\n  \\right)\n\\] \\[\n  \\left(\n  0.35\n  , ~\n  0.466\n  \\right)\n\\] This confidence interval contains \\(\\alpha_2 = 0.4\\).\n\n\n\nAll three confidence intervals contain the true parameter values we used for the simulation.\n\n\nResiduals\nWe can compute the residuals in the same manner as we did for the other models.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nWorking with a partner, do the following\n\nWrite the expression used to compute the residuals.\nFind the residuals of this sequence using your expression.\nHere are the first few residuals. Compare these to the values you computed.\n\n\n\nShow the code\nfit_ar |&gt;\n  residuals()\n\n\n# A tsibble: 1,000 x 3 [1]\n# Key:       .model [1]\n   .model           index .resid\n   &lt;chr&gt;            &lt;int&gt;  &lt;dbl&gt;\n 1 AR(x ~ order(2))     1 NA    \n 2 AR(x ~ order(2))     2 NA    \n 3 AR(x ~ order(2))     3  4.60 \n 4 AR(x ~ order(2))     4  0.237\n 5 AR(x ~ order(2))     5  0.330\n 6 AR(x ~ order(2))     6  5.13 \n 7 AR(x ~ order(2))     7  1.46 \n 8 AR(x ~ order(2))     8 -3.78 \n 9 AR(x ~ order(2))     9 -2.13 \n10 AR(x ~ order(2))    10 -1.39 \n# ℹ 990 more rows\n\n\n\nExplain why there are no residuals for times \\(t=1\\) and \\(t=2\\).\n\n\n\nThe variance of the residuals is 8.857. This is close to 9, the parameter used in the simulation.",
    "crumbs": [
      "Lesson 4",
      "Fitted AR Models"
    ]
  },
  {
    "objectID": "chapter_4_lesson_4.html#small-group-activity-global-warming-20-min",
    "href": "chapter_4_lesson_4.html#small-group-activity-global-warming-20-min",
    "title": "Fitted AR Models",
    "section": "Small-Group Activity: Global Warming (20 min)",
    "text": "Small-Group Activity: Global Warming (20 min)\nThe time plot below illustrates the change in global surface temperature compared to the long-term average observed from 1951 to 1980. (Source: NASA/GISS.)\n\n\nShow the code\ntemps_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/global_temparature.csv\") |&gt;\n  as_tsibble(index = year)\n\ntemps_ts |&gt; autoplot(.vars = change) +\n    labs(\n      x = \"Year\",\n      y = \"Temperature Change (Celsius)\",\n      title = paste0(\"Change in Mean Annual Global Temperature (\", min(temps_ts$year), \"-\", max(temps_ts$year), \")\")\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\n\nUsing the PACF to Choose \\(p\\) for an \\(AR(p)\\) Process\nIn the previous lesson, we noted that the partial correlogram can be used to assess the number of parameters in an AR model. Here is a partial correlogram for the change in the mean annual global temperature.\n\n\n\nShow the code\npacf(temps_ts$change)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nWorking with your partner, do the following:\n\nWe will apply an \\(AR(p)\\) model. What value of \\(p\\) is suggested by the pacf?\nUsing the value of \\(p\\) you identified, fit an \\(AR(p)\\) model to the global temperature data. State the fitted \\(AR(p)\\) model in the form \\[\\hat x_t = \\cdots\\]\nObtain 95% confidence intervals for each of the parameters. Which are significantly different from zero?\nGive the first three residual values (skipping the NAs).\nWhat is the estimate of \\(\\sigma^2\\)?\nMake a correlogram for the residuals. Does it appear that your model has fully explained the variation in the temperatures?\n\n\n\n\n\nFitting Models (Dynamic Number of Parameters)\nYou may have concluded that \\(p=3\\) might be insufficient for modeling these data. We now explore a technique that allows R to choose \\(p\\) based on the significance of the parameters.\nIf we specify order(1:9) in the model statement, R returns the largest \\(AR(p)\\) model (up to \\(p=9\\)) for which the parameter \\(\\alpha_p\\) is significant.\n\n\nShow the code\nglobal_ar &lt;- temps_ts |&gt;\n    model(AR(change ~ order(1:9)))\ntidy(global_ar)\n\n\n# A tibble: 7 × 6\n  .model                  term     estimate std.error statistic  p.value\n  &lt;chr&gt;                   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 AR(change ~ order(1:9)) constant   0.0190   0.00881     2.15  3.30e- 2\n2 AR(change ~ order(1:9)) ar1        0.656    0.0841      7.80  1.40e-12\n3 AR(change ~ order(1:9)) ar2       -0.0662   0.100      -0.659 5.11e- 1\n4 AR(change ~ order(1:9)) ar3        0.140    0.0988      1.42  1.58e- 1\n5 AR(change ~ order(1:9)) ar4        0.265    0.0995      2.67  8.58e- 3\n6 AR(change ~ order(1:9)) ar5       -0.163    0.102      -1.60  1.11e- 1\n7 AR(change ~ order(1:9)) ar6        0.206    0.0863      2.38  1.85e- 2\n\n\nR returned an \\(AR(6)\\) model for this time series.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nWorking with your partner, do the following:\n\nState the fitted \\(AR(p)\\) model in the form \\[\\hat x_t = \\cdots\\]\nObtain 95% confidence intervals for each of the parameters. Which are significantly different from zero?\nGive the first three residual values (skipping the NAs).\nWhat is the estimate of \\(\\sigma^2\\)?\nMake a correlogram for the residuals. Does it appear that your model has fully explained the variation in the temperatures? Justify your answer.\n\n\n\n\n\nStationarity of the \\(AR(p)\\) Model\nWith the exception of a lone seemingly spurious autocorrelation, there are no significant values of the acf of the residuals in the \\(AR(6)\\) model. This suggests that the model accounts for the variation in the time series.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWrite the characteristic equation for the \\(AR(6)\\) model you developed.\nClick on the link below to obtain a more precise version of the characteristic equation, then solve the characteristic equation by any method.\n\nCharacteristic Equation\n\n\n\nShow the code\nalphas &lt;- global_ar |&gt; coefficients() |&gt; tail(-1) |&gt; dplyr::select(estimate) |&gt; pull()\ncat(\n  \"0 = 1\", \n        \"- (\", alphas[1], \") * x\",\n        \"- (\", alphas[2], \") * x^2\",\n        \"- (\", alphas[3], \") * x^3\",\n        \"\\n     \",\n        \"- (\", alphas[4], \") * x^4\",\n        \"- (\", alphas[5], \") * x^5\",\n        \"- (\", alphas[6], \") * x^6\"\n)\n\n\n0 = 1 - ( 0.6559292 ) * x - ( -0.06617426 ) * x^2 - ( 0.140204 ) * x^3 \n      - ( 0.2653744 ) * x^4 - ( -0.1627911 ) * x^5 - ( 0.2056924 ) * x^6\n\n\n\n\nIs our \\(AR(6)\\) model stationary?",
    "crumbs": [
      "Lesson 4",
      "Fitted AR Models"
    ]
  },
  {
    "objectID": "chapter_4_lesson_4.html#class-activity-forecasting-with-an-arp-model-5-min",
    "href": "chapter_4_lesson_4.html#class-activity-forecasting-with-an-arp-model-5-min",
    "title": "Fitted AR Models",
    "section": "Class Activity: Forecasting with an \\(AR(p)\\) Model (5 min)",
    "text": "Class Activity: Forecasting with an \\(AR(p)\\) Model (5 min)\nWe now use the model to forecast the mean temperature difference for the next 50 years.\n\n\nShow the code\ntemps_forecast &lt;- global_ar |&gt; forecast(h = \"50 years\")\ntemps_forecast |&gt;\n  autoplot(temps_ts, level = 95) +\n  geom_line(aes(y = .fitted, color = \"Fitted\"),\n    data = augment(global_ar)) +\n  scale_color_discrete(name = \"\") +\n  labs(\n    x = \"Year\",\n    y = \"Temperature Change (Celsius)\",\n    title = paste0(\"Change in Mean Annual Global Temperature (\", min(temps_ts$year), \"-\", max(temps_ts$year), \")\"),\n    subtitle = paste0(\"50-Year Forecast Based on our AR(\", tidy(global_ar) |&gt; as_tibble() |&gt; dplyr::select(term) |&gt; tail(1) |&gt; right(1), \") Model\")\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nDoes this forecast seem appropriate for the data? Why or why not?\n\n\n\n\nClass Activity: Comparison to the Results in Section 4.6.3 of the Book (5 min)\nIn Sections 1.4.5 and 4.6.3 of the textbook, the authors present a similar dataset on the mean annual temperatures on Earth through 2005. Here is a time plot of their data:\n\n\nShow the code\nglobal_ts &lt;- tibble(x = scan(\"data/global.dat\")) |&gt;\n  mutate(\n        date = seq(\n            ymd(\"1856-01-01\"),\n            by = \"1 months\",\n            length.out = n()),\n        year = year(date),\n        year_month = tsibble::yearmonth(date)\n  ) |&gt;\n  summarise(x = mean(x), .by = year) |&gt;\n  as_tsibble(index = year) \nglobal_ts |&gt; autoplot(.vars = x) +\n    labs(\n      x = \"Year\",\n      y = \"Temperature Change (Celsius)\",\n      title = paste0(\"Change in Mean Annual Global Temperature (\", min(global_ts$year), \"-\", max(global_ts$year), \")\"),\n      subtitle = \"Data from Textbook Sections 1.4.5 and 4.6.3\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5),\n      plot.subtitle = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nThe fitted \\(AR(4)\\) model is given below.\n\n\nShow the code\nglobal_ar_book &lt;- global_ts |&gt;\n  model(AR(x ~ order(1:9)))\ntidy(global_ar_book)\n\n\n# A tibble: 4 × 6\n  .model             term  estimate std.error statistic  p.value\n  &lt;chr&gt;              &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 AR(x ~ order(1:9)) ar1     0.582     0.0791     7.36  1.24e-11\n2 AR(x ~ order(1:9)) ar2     0.0216    0.0919     0.236 8.14e- 1\n3 AR(x ~ order(1:9)) ar3     0.107     0.0917     1.17  2.43e- 1\n4 AR(x ~ order(1:9)) ar4     0.267     0.0791     3.38  9.43e- 4\n\n\nLet’s check the stationarity of this model. The characteristic equation is:\n\n\nShow the code\nalphas &lt;- global_ar_book |&gt; coefficients() |&gt; dplyr::select(estimate) |&gt; pull()\ncat(\n  \"0 = 1\", \n        \"- (\", alphas[1], \") * x\",\n        \"- (\", alphas[2], \") * x^2\",\n        \"- (\", alphas[3], \") * x^3\",\n        \"- (\", alphas[4], \") * x^4\"\n)\n\n\n0 = 1 - ( 0.5817607 ) * x - ( 0.02164876 ) * x^2 - ( 0.1074731 ) * x^3 - ( 0.2670716 ) * x^4\n\n\nThe solutions of the characteristic equation are:\n\n\nShow the code\npolyroot(c(1, -alphas)) |&gt; round(3)\n\n\n[1]  1.011+0.000i -1.755+0.000i  0.171-1.443i  0.171+1.443i\n\n\nThe absolute value of the solutions of the characteristic equation are:\n\n\nShow the code\npolyroot(c(1, -alphas)) |&gt; abs() |&gt; round(3)\n\n\n[1] 1.011 1.755 1.453 1.453\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nIs the textbook’s model stationary?\nIn the textbook, the author stated, “The correlogram of the residuals has only one (marginally) significant value at lag 27, so the underlying residual series could be white noise (Fig. 4.14). Thus the fitted AR(4) model (Equation (4.24)) provides a good fit to the data. As the AR model has no deterministic trend component, the trends in the data can be explained by serial correlation and random variation, implying that it is possible that these trends are stochastic (or could arise from a purely stochastic process). Again we emphasise that this does not imply that there is no underlying reason for the trends. If a valid scientific explanation is known, such as a link with the increased use of fossil fuels, then this information would clearly need to be included in any future forecasts of the series.”\n\nWhat is the author saying?\nHow would you respond to this statement?\n\n\n\n\nHere is a plot of the forecasted values for the next 100 years, based on the textbook’s model:\n\n\nShow the code\n# global_ar_book &lt;- global_ts |&gt;\n#   model(AR(x ~ order(4)))\ntemps_forecast_book &lt;- global_ar_book |&gt; forecast(h = \"100 years\")\ntemps_forecast_book |&gt;\n  autoplot(global_ts, level = 95) +\n#   geom_line(aes(y = .mean, color = \"Fitted\"),\n#     data = augment(global_ar_book)) +\n#   scale_color_discrete(name = \"\") +\n    labs(\n      x = \"Year\",\n      y = \"Temperature Change (Celsius)\",\n      title = paste0(\"Change in Mean Annual Global Temperature (\", min(temps_ts$year), \"-\", max(temps_ts$year), \")\"),\n      subtitle = \"100-Year Forecast Based on the Book's AR(4) Model\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5),\n      plot.subtitle = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nCompare and contrast the results you observed in the two global temperature time series.\nWhat conclusions do you draw?",
    "crumbs": [
      "Lesson 4",
      "Fitted AR Models"
    ]
  },
  {
    "objectID": "chapter_4_lesson_4.html#homework-preview-5-min",
    "href": "chapter_4_lesson_4.html#homework-preview-5-min",
    "title": "Fitted AR Models",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_4_4.qmd \n\n\nSmall-Group Activity: Global Warming–PACF\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nWorking with your partner, do the following\n\nWe will apply an \\(AR(p)\\) model. What value of \\(p\\) is suggested by the pacf?\n\nSolution:\n\\[p=3\\]\n\n\n\nUsing the value of \\(p\\) you identified, fit an \\(AR(p)\\) model to the global temperature data. State the fitted \\(AR(p)\\) model in the form \\[\\hat x_t = \\cdots\\]\n\nSolution:\n\n\nShow the code\nglobal_ar &lt;- temps_ts |&gt;\n    model(AR(change ~ order(3)))\ntidy(global_ar)\n\n\n# A tibble: 3 × 6\n  .model                term  estimate std.error statistic  p.value\n  &lt;chr&gt;                 &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 AR(change ~ order(3)) ar1     0.737     0.0823     8.95  1.80e-15\n2 AR(change ~ order(3)) ar2    -0.0350    0.103     -0.339 7.35e- 1\n3 AR(change ~ order(3)) ar3     0.319     0.0839     3.80  2.15e- 4\n\n\nNote that the constant term is not statistically significant. If we ignore this term, we get the fitted model:\n\\[\\begin{align*}\n  \\hat x_t\n  &=\n    0\n    + \\hat \\alpha_1 ~ x_{t-1}\n    + \\hat \\alpha_2 ~ x_{t-2}\n    + \\hat \\alpha_3 ~ x_{t-3}\n    \\\\\n  &=\n    0.737\n    ~ x_{t-1}\n    +\n    (-0.035)\n    ~ x_{t-2}\n    +\n    0.319\n    ~ x_{t-3}\n\\end{align*}\\]\nIf we want to incorporate the constant term in the model, then we need to find the mean of the time series. The mean of the time series is:\n\nmean(temps_ts$change)\n\n[1] 0.06923611\n\n\nThe fitted AR model is\n\\[\\begin{align*}\n  \\hat x_t\n  &=\n    \\hat \\mu\n    + \\hat \\alpha_1 ~ (x_{t-1} - \\mu)\n    + \\hat \\alpha_2 ~ (x_{t-2} - \\mu)\n    + \\hat \\alpha_3 ~ (x_{t-3} - \\mu)\n    \\\\\n  &=\n    \\underbrace{\n    \\hat \\mu - \\hat \\alpha_1 (\\hat \\mu) - \\hat \\alpha_2 (\\hat \\mu) - \\hat \\alpha_3 (\\hat \\mu)\n    }_{\\hat \\alpha_0}\n    + \\hat \\alpha_1 ~ x_{t-1}\n    + \\hat \\alpha_2 ~ x_{t-2}\n    + \\hat \\alpha_3 ~ x_{t-3}\n    \\\\\n  &=\n    \\hat \\alpha_0\n    + \\hat \\alpha_1 ~ x_{t-1}\n    + \\hat \\alpha_2 ~ x_{t-2}\n    + \\hat \\alpha_3 ~ x_{t-3}\n\\end{align*}\\] Or, after substituting the fitted values: \\[\\begin{align*}\n  \\hat x_t\n  &=\n    0.069\n    +\n    0.737\n    ~ ( x_{t-1} - 0.069)\n    +\n    (-0.035)\n    ~ ( x_{t-2} - 0.069)\n    +\n    0.319\n    ~ ( x_{t-3} - 0.069) \\\\\n  &=\n    -0.0014\n    +\n    0.737\n    ~ x_{t-1}\n    +\n    (-0.035)\n    ~ x_{t-2}\n    +\n    0.319\n    ~ x_{t-3}\n\\end{align*}\\]\n\n\n\nObtain 95% confidence intervals for each of the parameters. Which are significantly different from zero?\n\nSolution:\n\n\nShow the code\nci_summary &lt;- tidy(global_ar) |&gt;\n    mutate(\n        lower = estimate - 2 * std.error,\n        upper = estimate + 2 * std.error\n    )\n\n\nThe confidence intervals are: \\[\\begin{align*}\n  \\alpha_1: &&&\n    (\n      0.572\n      ,~\n      0.901\n    )\n    \\\\\n  \\alpha_2: &&&\n    (\n      -0.242\n      ,~\n      0.172\n    )\n    \\\\\n  \\alpha_3: &&&\n    (\n      0.151\n      ,~\n      0.486\n    )\n\\end{align*}\\]\nThe parameters \\(\\alpha_1\\) and \\(\\alpha_3\\) are statistically significantly different from 0.\n\n\n\nGive the first three residual values (skipping the NAs).\n\nSolution:\n\nglobal_ar |&gt; \n  residuals() |&gt;\n  na.omit() |&gt;\n  head(3)\n\n# A tsibble: 3 x 3 [1Y]\n# Key:       .model [1]\n  .model                 year  .resid\n  &lt;chr&gt;                 &lt;int&gt;   &lt;dbl&gt;\n1 AR(change ~ order(3))  1883 -0.0413\n2 AR(change ~ order(3))  1884 -0.130 \n3 AR(change ~ order(3))  1885 -0.105 \n\n\n\n\n\nWhat is the estimate of \\(\\sigma^2\\)?\n\nSolution:\n\n\nShow the code\nresid_df &lt;- global_ar |&gt; \n  residuals() |&gt;\n  as_tibble()\nvar(resid_df$.resid, na.rm = TRUE) \n\n\n[1] 0.01114332\n\n\n\n\n\nMake a correlogram for the residuals. Does it appear that your model has fully explained the variation in the temperatures?\n\nSolution:\n\n\nShow the code\nresiduals(global_ar) |&gt; \n  ACF(lag_max = 50) |&gt; \n  autoplot(.vars = .resid) +\n    labs(\n      title = paste0(\"ACF of the Residuals from the AR(\", tidy(global_ar) |&gt; as_tibble() |&gt; dplyr::select(term) |&gt; tail(1) |&gt; right(1), \") Model\")\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\nResponse variable not specified, automatically selected `var = .resid`\n\n\n\n\n\n\n\n\n\nThere is still a significant autocorrelation at lag \\(k=3\\). This suggests a more sophisticated model may be necessary.\n\n\n\n\n\n\n\nSmall-Group Activity: Global Warming–Dynamic\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nWorking with your partner, do the following:\n\nState the fitted \\(AR(p)\\) model in the form \\[\\hat x_t = \\cdots\\]\n\nSolution:\n\n\nShow the code\nglobal_ar &lt;- temps_ts |&gt;\n    model(AR(change ~ order(1:9)))\ntidy(global_ar)\n\n\n# A tibble: 7 × 6\n  .model                  term     estimate std.error statistic  p.value\n  &lt;chr&gt;                   &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 AR(change ~ order(1:9)) constant   0.0190   0.00881     2.15  3.30e- 2\n2 AR(change ~ order(1:9)) ar1        0.656    0.0841      7.80  1.40e-12\n3 AR(change ~ order(1:9)) ar2       -0.0662   0.100      -0.659 5.11e- 1\n4 AR(change ~ order(1:9)) ar3        0.140    0.0988      1.42  1.58e- 1\n5 AR(change ~ order(1:9)) ar4        0.265    0.0995      2.67  8.58e- 3\n6 AR(change ~ order(1:9)) ar5       -0.163    0.102      -1.60  1.11e- 1\n7 AR(change ~ order(1:9)) ar6        0.206    0.0863      2.38  1.85e- 2\n\n\n\\[\n  \\hat x_t =\n    0.019\n    +\n    0.656 ~ x_{t-1}\n    +\n    (-0.066) ~ x_{t-2}\n    +\n    0.14 ~ x_{t-3}\n    +\n    0.265 ~ x_{t-4}\n    +\n    (-0.163) ~ x_{t-5}\n    +\n    0.206 ~ x_{t-6}\n\\]\n\n\n\nObtain 95% confidence intervals for each of the parameters. Which are significantly different from zero?\n\nSolution:\n\n\nShow the code\nci_summary &lt;- tidy(global_ar) |&gt;\n    mutate(\n        lower = estimate - 2 * std.error,\n        upper = estimate + 2 * std.error\n    )\nci_summary\n\n\n# A tibble: 7 × 8\n  .model             term  estimate std.error statistic  p.value    lower  upper\n  &lt;chr&gt;              &lt;chr&gt;    &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;\n1 AR(change ~ order… cons…   0.0190   0.00881     2.15  3.30e- 2  0.00135 0.0366\n2 AR(change ~ order… ar1     0.656    0.0841      7.80  1.40e-12  0.488   0.824 \n3 AR(change ~ order… ar2    -0.0662   0.100      -0.659 5.11e- 1 -0.267   0.135 \n4 AR(change ~ order… ar3     0.140    0.0988      1.42  1.58e- 1 -0.0574  0.338 \n5 AR(change ~ order… ar4     0.265    0.0995      2.67  8.58e- 3  0.0663  0.464 \n6 AR(change ~ order… ar5    -0.163    0.102      -1.60  1.11e- 1 -0.366   0.0405\n7 AR(change ~ order… ar6     0.206    0.0863      2.38  1.85e- 2  0.0331  0.378 \n\n\nThe constant term, \\(\\alpha_1\\), \\(\\alpha_4\\), and \\(\\alpha_6\\) are all statistically significantly different from zero.\n\n\n\nGive the first three residual values (skipping the NAs).\n\nSolution:\n\n\nShow the code\nresiduals(global_ar) |&gt;\n  na.omit() |&gt;\n  head(3)\n\n\n# A tsibble: 3 x 3 [1Y]\n# Key:       .model [1]\n  .model                   year  .resid\n  &lt;chr&gt;                   &lt;int&gt;   &lt;dbl&gt;\n1 AR(change ~ order(1:9))  1886 -0.0636\n2 AR(change ~ order(1:9))  1887 -0.117 \n3 AR(change ~ order(1:9))  1888  0.139 \n\n\n\n\n\nWhat is the estimate of \\(\\sigma^2\\)?\n\nSolution:\n\n\nShow the code\nresid_var &lt;- global_ar |&gt;\n  residuals() |&gt;\n  as_tibble() |&gt;\n  dplyr::select(.resid) |&gt;\n  pull() |&gt;\n  na.omit() |&gt;\n  var()\nresid_var\n\n\n[1] 0.01006731\n\n\nThe estimate of \\(\\sigma^2\\) is \\(\\hat \\sigma^2 = 0.01\\).\n\n\n\nMake a correlogram for the residuals. Does it appear that your model has fully explained the variation in the temperatures? Justify your answer.\n\nSolution:\n\n\nShow the code\nresiduals(global_ar) |&gt; \n  ACF(lag_max = 50) |&gt; \n  autoplot(.vars = .resid) +\n    labs(\n      title = paste0(\"ACF of the Residuals from the AR(\", tidy(global_ar) |&gt; as_tibble() |&gt; dplyr::select(term) |&gt; tail(1) |&gt; right(1), \") Model\")\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nThere is only one significant autocorrelation: \\(k=34\\). This is probably a Type I error, which should occur 5% of the time. None of the other autocorrelations are significant–particularly among the smaller values of \\(k\\). It appears that this model has fully explained the variation in the temperatures.\n\n\n\n\n\n\nStationarity of the \\(AR(6)\\) model\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWrite the characteristic equation for the \\(AR(6)\\) model you developed.\n\nSolution:\n\n\n0 = 1 - ( 0.656 ) * x - ( -0.066 ) * x^2 - ( 0.14 ) * x^3 \n      - ( 0.265 ) * x^4 - ( -0.163 ) * x^5 - ( 0.206 ) * x^6\n\n\n\n\n\nObtain a more precise version of the characteristic equation, then solve the characteristic equation by any method.\n\nSolution:\n\n\n0 = 1 - ( 0.6559292 ) * x - ( -0.06617426 ) * x^2 - ( 0.140204 ) * x^3 \n      - ( 0.2653744 ) * x^4 - ( -0.1627911 ) * x^5 - ( 0.2056924 ) * x^6\n\n\n\nalphas &lt;- global_ar |&gt; coefficients() |&gt; tail(-1) |&gt; dplyr::select(estimate) |&gt; pull()\npolyroot(c(1, -alphas))\n\n[1]  0.9838806+0.000000i -1.2890454+0.000000i -0.3076674-1.277838i\n[4]  0.8559646-1.219125i -0.3076674+1.277838i  0.8559646+1.219125i\n\n\n\n\n\nIs our \\(AR(6)\\) model stationary?\n\nSolution:\n\nabs(polyroot(c(1, -alphas)))\n\n[1] 0.9838806 1.2890454 1.3143548 1.4896112 1.3143548 1.4896112\n\n\nNot all of the roots are greater than 1 in absolute value. So, this AR process is not stationary.",
    "crumbs": [
      "Lesson 4",
      "Fitted AR Models"
    ]
  },
  {
    "objectID": "chapter_4_lesson_2_handout.html",
    "href": "chapter_4_lesson_2_handout.html",
    "title": "White Noise and Random Walks: Part 2 – Handout",
    "section": "",
    "text": "Difference Operator\nLet \\(\\{x_t\\}\\) be a time series with the following values.\n\n\\(x_{1} = 5\\),\\(~\\) \\(x_{2} = 10\\),\\(~\\) \\(x_{3} = 13\\),\\(~\\) \\(x_{4} = 8\\),\\(~\\) \\(x_{5} = 4\\),\\(~\\) \\(x_{6} = 3\\),\\(~\\) \\(x_{7} = 9\\),\\(~\\) \\(x_{8} = 2\\)\n\n\n\n\n\nFind the first differences, \\(\\nabla x_t\\)\nFind the second differences, \\(\\nabla^2 x_t\\).\nFill in the missing steps: \\[\\begin{align*}\n\\nabla^2 x_8 &= (1-\\mathbf{B} )^2 x_8 \\\\\n  &= (1-\\mathbf{B} ) \\left[ (1-\\mathbf{B} ) x_8 \\right] \\\\\n  &  ~~~~~~~~~~~~~~~~~~~~~~ ⋮ \\\\\n  &= (x_8-x_7)-(x_7-x_6)\n\\end{align*}\\] and check that this is equal to the last term in the sequence of second differences.\n\n\n\n\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$\\nabla x_t$$\n$$\\nabla^2 x_t$$\n\n\n\n\n1\n5\n\n\n\n\n2\n10\n\n\n\n\n3\n13\n\n\n\n\n4\n8\n\n\n\n\n5\n4\n\n\n\n\n6\n3\n\n\n\n\n7\n9\n\n\n\n\n8\n2\n\n\n\n\n\n\n\n\n\n\n\n\n              \n\n\nComputing Differences\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$\\nabla x_t$$\n\n\n\n\n\n\n1\n7.5\n\n\n\n\n\n2\n10\n\n\n\n\n\n3\n12.5\n\n\n\n\n\n4\n15\n\n\n\n\n\n5\n17.5\n\n\n\n\n\n6\n20\n\n\n\n\n\n7\n22.5\n\n\n\n\n\n8\n25\n\n\n\n\n\n9\n27.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$\\nabla x_t$$\n$$\\nabla^2 x_t$$\n\n\n\n\n\n1\n15\n\n\n\n\n\n2\n2\n\n\n\n\n\n3\n-7\n\n\n\n\n\n4\n-12\n\n\n\n\n\n5\n-13\n\n\n\n\n\n6\n-10\n\n\n\n\n\n7\n-3\n\n\n\n\n\n8\n8\n\n\n\n\n\n9\n23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$\\nabla x_t$$\n$$\\nabla^2 x_t$$\n$$\\nabla^3 x_t$$\n\n\n\n\n1\n-2.7\n\n\n\n\n\n2\n0\n\n\n\n\n\n3\n1\n\n\n\n\n\n4\n0.9\n\n\n\n\n\n5\n0.3\n\n\n\n\n\n6\n-0.2\n\n\n\n\n\n7\n0\n\n\n\n\n\n8\n1.5\n\n\n\n\n\n9\n4.9"
  },
  {
    "objectID": "chapter_4_lesson_1_handout.html",
    "href": "chapter_4_lesson_1_handout.html",
    "title": "White Noise and Random Walks: Part 1 – Handout",
    "section": "",
    "text": "Coin Toss Experiment\n\n\n\n\n\n\n\nStart the time series at \\(x_0 = 0\\).\nToss a coin.\n\nIf the coin shows heads, then \\(x_t = x_{t-1}+1\\)\nIf the coin shows tails, then \\(x_t = x_{t-1}-1\\)\n\nPlot the new point on the time plot.\nComplete steps 2 and 3 a total of \\(n=60\\) times.\n\n\n\n\n\n\n\n\n\n\n\n\n\n \n\nBackward Shift Operator\nLet \\(\\{x_t\\}\\) be a time series with the following values.\n\n\\(x_{1} = 5\\),\\(~\\) \\(x_{2} = 10\\),\\(~\\) \\(x_{3} = 13\\),\\(~\\) \\(x_{4} = 8\\),\\(~\\) \\(x_{5} = 4\\),\\(~\\) \\(x_{6} = 3\\),\\(~\\) \\(x_{7} = 9\\),\\(~\\) \\(x_{8} = 2\\)\n\nEvaluate the following.\n\n\\(\\mathbf{B} x_8\\)\n\\(\\mathbf{B}^5 x_8\\)\n\\((\\mathbf{B}^5 - \\mathbf{B} ) x_8\\)\n\\(( \\mathbf{B}^2 - 6 \\mathbf{B} + 9 ) x_8\\)\n\\(( (\\mathbf{B} - 6 )\\mathbf{B} + 9 ) x_8\\)\n\\(( \\mathbf{B} - 3 )^2 x_8 =  ( \\mathbf{B} - 3 ) \\left[ ( \\mathbf{B} - 3 ) x_8 \\right]\\)\n\\(( 1 - \\frac{1}{2} \\mathbf{B} - \\frac{1}{4} \\mathbf{B}^2 - \\frac{1}{8} \\mathbf{B}^3 ) x_8\\)"
  },
  {
    "objectID": "chapter_4.html",
    "href": "chapter_4.html",
    "title": "Lessons & Homework",
    "section": "",
    "text": "Chapter 4\n\n\n\n\n\n\nLessons\n\n\n\n\nLesson 1 - White Noise and Random Walks - Part 1\nLesson 2 - White Noise and Random Walks - Part 2\nLesson 3 - Autoregressive (AR) Models\nLesson 4 - Fitted AR Models\n\n\n\n\n\n\n\n\n\nDownload Homework Assignments\n\n\n\n\n homework_4_1.qmd \n homework_4_2.qmd \n homework_4_3.qmd \n homework_4_4.qmd",
    "crumbs": [
      "Overview",
      "Lessons & Homework"
    ]
  },
  {
    "objectID": "chapter_3_lesson_5_handout.html",
    "href": "chapter_3_lesson_5_handout.html",
    "title": "Holt-Winters Method (Multiplicative Models)",
    "section": "",
    "text": "Figure 1: Time plot of BYU-Idaho campus enrollments\n\n\n\n\n\n\n\n\n\n\\(\\ \\)          \n\n\nTable 1: Holt-Winters smoothing for BYU-Idaho campus enrollments\n\n\n\n\n\n$$Semester$$\n$$t$$\n$$x_t$$\n$$a_t$$\n$$b_t$$\n$$s_t$$\n$$\\hat x_t$$\n\n\n\n\nSP11\n-2\n—\n—\n—\n\n—\n\n\nFA11\n-1\n—\n—\n—\n\n—\n\n\nWI12\n0\n—\n—\n—\n\n—\n\n\nSP12\n1\n13.7\n\n\n\n\n\n\nFA12\n2\n16.2\n14.2\n0.082\n1.03\n14.6\n\n\nWI13\n3\n15.5\n14.5\n0.126\n1.01\n14.6\n\n\nSP13\n4\n14\n14.5\n0.101\n0.99\n14.4\n\n\nFA13\n5\n15.6\n14.7\n0.121\n1.04\n15.3\n\n\nWI14\n6\n15.6\n\n\n\n\n\n\nSP14\n7\n12.9\n\n\n\n\n\n\nFA14\n8\n16.2\n14.8\n0.08\n1.05\n15.5\n\n\nWI15\n9\n16.7\n15.2\n0.144\n1.04\n15.8\n\n\nSP15\n10\n13.7\n15.1\n0.095\n0.96\n14.5\n\n\nFA15\n11\n17.6\n15.5\n0.156\n1.07\n16.6\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\nFA22\n32\n19.4\n17.7\n-0.086\n1.12\n19.8\n\n\nWI23\n33\n17.7\n17.5\n-0.109\n1.04\n18.2\n\n\nSP23\n34\n12.8\n16.9\n-0.207\n0.85\n14.4\n\n\nFA23\n35\n18.7\n16.7\n-0.206\n1.12\n18.7\n\n\nWI24\n36\n17.6\n16.6\n-0.185\n1.04\n17.3\n\n\nSP24\n37\n—\n—\n—\n\n\n\n\nFA24\n38\n—\n—\n—\n\n\n\n\nWI25\n39\n—\n—\n—\n\n\n\n\nSP25\n40\n—\n—\n—\n\n\n\n\nFA25\n41\n—\n—\n—\n1.12\n17.6\n\n\nWI26\n42\n—\n—\n—\n1.04\n16.1\n\n\nSP26\n43\n—\n—\n—\n0.85\n13\n\n\nFA26\n44\n—\n—\n—\n1.12\n16.9\n\n\nWI27\n45\n—\n—\n—\n1.04\n15.5"
  },
  {
    "objectID": "chapter_3_lesson_4_handout.html",
    "href": "chapter_3_lesson_4_handout.html",
    "title": "Holt-Winters Method (Additive Models) - Part 2",
    "section": "",
    "text": "Figure 1: Quarterly U.S. natural gas consumption (Bcf)\n\n\n\n\n\n\n\n\n\n\\(\\ \\)            \n\n\nTable 1: Holt-Winters filter for the U.S. quarterly natural gas consumption (in Bcf)\n\n\n\n\n\n$$Quarter$$\n$$t$$\n$$x_t$$\n$$a_t$$\n$$b_t$$\n$$s_t$$\n$$\\hat x_t$$\n\n\n\n\n2016-01-07\n-3\n—\n—\n—\n1000\n—\n\n\n2016-04-06\n-2\n—\n—\n—\n-1000\n—\n\n\n2016-07-05\n-1\n—\n—\n—\n-1000\n—\n\n\n2016-10-03\n0\n—\n—\n—\n1000\n—\n\n\n2017-01-01\n1\n1990\n\n\n\n\n\n\n2017-04-01\n2\n603\n\n\n\n\n\n\n2017-07-01\n3\n326\n\n\n\n\n\n\n2017-10-01\n4\n1495\n\n\n\n\n\n\n2018-01-01\n5\n2331\n1508\n-58\n805\n2313\n\n\n2018-04-01\n6\n729\n1519\n-44\n-1012\n507\n\n\n2018-07-01\n7\n318\n1464\n-46\n-1110\n354\n\n\n2018-10-01\n8\n1620\n1301\n-69\n693\n1994\n\n\n2019-01-01\n9\n2451\n1315\n-52\n871\n2186\n\n\n2019-04-01\n10\n670\n1347\n-35\n-945\n402\n\n\n2019-07-01\n11\n323\n1336\n-30\n-1091\n245\n\n\n2019-10-01\n12\n1573\n1221\n-47\n625\n1846\n\n\n2020-01-01\n13\n2089\n1183\n-45\n878\n2061\n\n\n2020-04-01\n14\n751\n1250\n-23\n-856\n394\n\n\n2020-07-01\n15\n354\n1271\n-14\n-1056\n215\n\n\n2020-10-01\n16\n1481\n1177\n-30\n561\n1738\n\n\n2021-01-01\n17\n2345\n1211\n-17\n929\n2140\n\n\n2021-04-01\n18\n690\n1264\n-3\n-800\n464\n\n\n2021-07-01\n19\n338\n1288\n2\n-1035\n253\n\n\n2021-10-01\n20\n1344\n1189\n-18\n480\n1669\n\n\n2022-01-01\n21\n2337\n1218\n-9\n967\n2185\n\n\n2022-04-01\n22\n710\n1269\n3\n-752\n517\n\n\n2022-07-01\n23\n327\n1290\n7\n-1021\n269\n\n\n2022-10-01\n24\n1590\n1260\n0\n450\n1710\n\n\n2023-01-01\n25\n2115\n1238\n-4\n949\n2187\n\n\n2023-04-01\n26\n663\n1270\n3\n-723\n\n\n\n2023-07-01\n27\n329\n1288\n6\n-1021\n\n\n\n2023-09-29\n28\n—\n—\n—\n450\n\n\n\n2023-12-28\n29\n—\n—\n—\n949\n\n\n\n2024-03-27\n30\n—\n—\n—\n-723\n\n\n\n2024-06-25\n31\n—\n—\n—\n-1021\n\n\n\n2024-09-23\n32\n—\n—\n—\n450\n\n\n\n2024-12-22\n33\n—\n—\n—\n949\n\n\n\n2025-03-22\n34\n—\n—\n—\n-723\n\n\n\n2025-06-20\n35\n—\n—\n—\n-1021\n\n\n\n2025-09-18\n36\n—\n—\n—\n450"
  },
  {
    "objectID": "chapter_3_lesson_3.html",
    "href": "chapter_3_lesson_3.html",
    "title": "Holt-Winters Method (Additive Models) - Part 1",
    "section": "",
    "text": "Implement the Holt-Winter method to forecast time series\n\n\nJustify the need for the Holt-Winters method\nDescribe how to obtain initial parameters for the Holt-Winters algorithm\nExplain the Holt-Winters update equations for additive decomposition models\nExplain the purpose of the parameters \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\)\nInterpret the coefficient estimates \\(a_t\\), \\(b_t\\), and \\(s_t\\) of the Holt-Winters algorithm\nExplain the Holt-Winters forecasting equation for additive decomposition models, Equation (3.22)",
    "crumbs": [
      "Lesson 3",
      "Holt-Winters Method (Additive Models) - Part 1"
    ]
  },
  {
    "objectID": "chapter_3_lesson_3.html#learning-outcomes",
    "href": "chapter_3_lesson_3.html#learning-outcomes",
    "title": "Holt-Winters Method (Additive Models) - Part 1",
    "section": "",
    "text": "Implement the Holt-Winter method to forecast time series\n\n\nJustify the need for the Holt-Winters method\nDescribe how to obtain initial parameters for the Holt-Winters algorithm\nExplain the Holt-Winters update equations for additive decomposition models\nExplain the purpose of the parameters \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\)\nInterpret the coefficient estimates \\(a_t\\), \\(b_t\\), and \\(s_t\\) of the Holt-Winters algorithm\nExplain the Holt-Winters forecasting equation for additive decomposition models, Equation (3.22)",
    "crumbs": [
      "Lesson 3",
      "Holt-Winters Method (Additive Models) - Part 1"
    ]
  },
  {
    "objectID": "chapter_3_lesson_3.html#preparation",
    "href": "chapter_3_lesson_3.html#preparation",
    "title": "Holt-Winters Method (Additive Models) - Part 1",
    "section": "Preparation",
    "text": "Preparation\n\nRead Section 3.4.2 (Page 59 - top of page 60 only)",
    "crumbs": [
      "Lesson 3",
      "Holt-Winters Method (Additive Models) - Part 1"
    ]
  },
  {
    "objectID": "chapter_3_lesson_3.html#learning-journal-exchange-10-min",
    "href": "chapter_3_lesson_3.html#learning-journal-exchange-10-min",
    "title": "Holt-Winters Method (Additive Models) - Part 1",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading another student’s?\nWhat would you recommend the other student add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 3",
      "Holt-Winters Method (Additive Models) - Part 1"
    ]
  },
  {
    "objectID": "chapter_3_lesson_3.html#introduction-to-the-holt-winters-methodadditive-model-35-min",
    "href": "chapter_3_lesson_3.html#introduction-to-the-holt-winters-methodadditive-model-35-min",
    "title": "Holt-Winters Method (Additive Models) - Part 1",
    "section": "Introduction to the Holt-Winters Method–Additive Model (35 min)",
    "text": "Introduction to the Holt-Winters Method–Additive Model (35 min)\nWe will describe the historical progression that led to the Holt-Winters method.\n\nReview: Exponentially Weighted Moving Average (EWMA) or Simple Exponential Smoothing\nThe exponential weighted moving average (EWMA) is a simple method for smoothing (or filtering) a time series. The update equation for the estimate of the level of the time series is\n\\[\n  a_t = \\alpha x_t + (1-\\alpha) a_{t-1}\n\\]\nwhere \\(a_t\\) is the estimate of the level of the time series at time \\(t\\) and \\(0 \\le \\alpha \\le 1\\) is the smoothing paramter.\nThis is known as the level update equation, because at each time step, we can update our estimate of the level (or the center) of the time series. It is called exponential smoothing, because at each preceding value has exponentially decreasing influence on the estimate.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nExplain the level update equation to your partner.\n\n\n\nThese computations are based on previous values and \\(a_1 = x_1\\). The number \\(0 \\le \\alpha \\le 1\\) is a smoothing parameter. This determines how much weight is given to previous values when creating the updated level estimate.\nIf you were to use this model for forecasting, you would not be able to consider any trend or seasonality in the forecast. Hence, the future values would all be forecasted as the last value of \\(a_n\\):\n\\[\n  \\hat x_{n+k|n} = a_n\n\\] where \\(\\hat x_{n+k \\mid n}\\) is the estimate of the time series \\(k\\) time units in the future past time \\(t=n\\). Frankly, this is not very useful, because many time series have trends or seasonality.\n\n\nHolt’s Exponential Smoothing\nIn 1957, Charles Holt published a new procedure that introduced a trend into this model. The forecasted values were:\n\\[\n  \\hat x_{n+k|n} = a_n + k b_n\n\\]\nwhere \\(b_n\\) is the slope indicating how much the time series changes on average from one time point to another and \\(k\\) is the number of time periods past \\(t=n\\) you are forecasting.\nThis method uses the same level update equation as EMWA. The slope update equation is:\n\\[\n  b_t = \\beta \\left( a_t - a_{t-1} \\right) + (1-\\beta) b_{t-1}\n\\]\nwhere \\(0 \\le \\beta \\le 1\\) is a smoothing parameter, \\(b_t\\) is the slope estimate at time \\(t\\), and \\(a_t\\) is the estimate of the level of the time series at time \\(t\\).\n\n\nHolt-Winters Filtering (Winters’ Exponential Smoothing)\nPeter Winters, a colleague of Holt’s at the Carnagie Institute of Technology, published an enhancement of Winters’ technique in 1960 that allowed for seasonal variation. This is known as the Holt-Winters Method or Holt-Winters Filtering.\n\nForecast Equation\nThe forecast equation is:\n\\[\n  \\hat x_{n+k|n} = a_n + k b_n + s_{n+k-p}\n\\]\nwhere \\(\\hat x_{n+k|n}\\) is the forecasted value of the time series \\(k\\) units in the future after time \\(t=n\\), and the time series is assumed to have seasonality with a period of \\(p\\) time units; \\(a_n\\) is the level of the time series at time \\(t=n\\); \\(b_n\\) is the slope of the time series at time \\(t=n\\); and \\(s_{n+k-p}\\) is the estimated seasonal component at time \\(t=n+k-p\\). Note that we must look back one full period to get the estimated seasonal component.\n\n\nUpdate Equations\nThere are three update equations, one each for \\(a_t\\) (level), \\(b_t\\) (slope), and \\(s_t\\) (seasonal component).\n\\[\\begin{align*}\n  a_t &= \\alpha \\left( x_t - s_{t-p} \\right) + (1-\\alpha) \\left( a_{t-1} + b_{t-1} \\right) && \\text{Level} \\\\\n  b_t &= \\beta \\left( a_t - a_{t-1} \\right) + (1-\\beta) b_{t-1} && \\text{Slope} \\\\\n  s_t &= \\gamma \\left( x_t - a_t \\right) + (1-\\gamma) s_{t-p} && \\text{Seasonal}\n\\end{align*}\\]\nwhere \\(\\{x_t\\}\\) is a time series from \\(t=1\\) to \\(t=n\\) that has seasonality with a period of \\(p\\) time units; at time \\(t\\), \\(a_t\\) is the estimated level of the time series, \\(b_t\\) is the estimated slope, and \\(s_t\\) is the estimated seasonal component; and \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\) are parameters (all between 0 and 1).\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nWork with your partner to answer the following questions about the update equations.\n\\[\na_t = \\alpha \\cdot \\underbrace{ \\left( x_t - s_{t-p} \\right) }_{A} + (1-\\alpha) \\cdot  \\underbrace{ \\left( a_{t-1} + b_{t-1} \\right) }_{B}\n~~~~~~~~~~~~~~~~~~~~ \\text{Level}\n\\]\n\nInterpret the term \\(A = x_t - s_{t-p}\\).\nInterpret the term \\(B = a_{t-1} + b_{t-1}\\).\nExplain why this expression for \\(a_t\\) estimates the level of the time series at time \\(t\\).\n\n\\[\nb_t = \\beta \\cdot \\underbrace{ \\left( a_t - a_{t-1} \\right) }_{C} + (1-\\beta) \\cdot \\underbrace{ b_{t-1} }_{D}\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\text{Slope}\n\\]\n\nInterpret the term \\(C = a_t - a_{t-1}\\).\nInterpret the term \\(D = b_{t-1}\\).\nExplain why this expression for \\(b_t\\) estimates the slope of the time series at time \\(t\\).\n\n\\[\ns_t = \\gamma \\cdot \\underbrace{ \\left( x_t - a_t \\right) }_{E} + (1-\\gamma) \\cdot \\underbrace{ s_{t-p} }_{F}\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\text{Seasonal}\n\\]\n\nInterpret the term \\(E = x_t - a_t\\).\nInterpret the term \\(F = s_{t-p}\\).\nExplain why this expression for \\(s_t\\) estimates the seasonal component of the time series at time \\(t\\).\nWhen the seasonal component appears on the right-hand side of the update equations, it always given as \\(s_{t-p}\\). Why do we use the estimate of the seasonal effect \\(p\\) periods ago? Why not apply a more recent value?\nWhat do the following sets of terms have in common?\n\n\\(\\{A, C, E \\}\\)\n\\(\\{B, D, F \\}\\)\n\nExplain why the Holt-Winters method works.\n\n\n\n\n\nInitial Estimates of \\(a_t\\), \\(b_t\\), and \\(s_t\\)\nWe can use the update equations to compute the next value of \\(a_t\\), \\(b_t\\), and \\(s_t\\), once we get going. Yet, how do we get started? What are the initial values of these estimates?\n\nEstimating \\(a_1\\):\nIt is reasonable to let \\(a_1 = x_t\\). We simply start our estimate of the level of the time series at the initial data value.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nExplain why it is reasonable to let \\(a_1 = x_1\\).\n\n\n\n\n\nEstimating \\(b_1\\):\nFor the value of \\(b_1\\), the Cowpertwait textbook vaguely suggests estimating this from the data or setting it to zero. Setting \\(b_1\\) to zero is problematic, because it adversely affects the level and slope estimates at the beginning of the time series. We need a better way.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nInterpret the quantity \\[\n  \\dfrac{x_{p+1} - x_{1}}{p}\n\\]\n\n\n\nWe will approximate \\(b_1\\) by averaging the slope between pairs of points one period apart. Recall that \\(p\\) is the number of observations per period. (Monthly data which have an annual cycle would have \\(p=12\\). Daily data with a weekly cycle have \\(p=7\\).) Note that the fraction in the “Check Your Understanding” box above is an estimate of the slope of the time series as it moves from time \\(1\\) to time \\(p+1\\). These are the first observations in the first two cycles. We compute these estimated slopes for all the paired observations in the first two cycles, then we compute the mean of these slopes. This is reflected in the expression for \\(b_1\\):\n\\[\n  b_1 =\n    \\frac{\n      \\left(\n        \\dfrac{x_{p+1} - x_{1}}{p} +\n        \\dfrac{x_{p+2} - x_{2}}{p} +\n        \\dfrac{x_{p+3} - x_{3}}{p} +\n        \\cdots +\n        \\dfrac{x_{2p-1} - x_{p-1}}{p} +\n        \\dfrac{x_{2p} - x_{p}}{p}\n      \\right)\n    }{p}\n\\]\n\n\nEstimating \\(s_1, s_2, \\ldots s_p\\):\nThe initial \\(p\\) values of the seasonal effects, \\(s_1, s_2, \\ldots s_p\\), can be determined either by estimating based on the data or your prior experience; alternatively, they could be set to 0. For \\(t=1, 2, 3, \\ldots, p\\) iterations, use your estimate for \\(s_t\\) when the formulas call for \\(s_{t-p}\\).\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat does the term \\[\n  \\dfrac{x_{p+1} - x_{1}}{p}\n\\] estimate?\nWhy does the average of the values \\[\n  \\left\\{\n      \\dfrac{x_{p+1} - x_{1}}{p}, ~\n      \\dfrac{x_{p+2} - x_{2}}{p}, ~\n      \\dfrac{x_{p+3} - x_{3}}{p}, ~\n      \\cdots, ~\n      \\dfrac{x_{2p-1} - x_{p-1}}{p}, ~\n      \\dfrac{x_{2p} - x_{p}}{p}\n  \\right\\}\n\\] give a good estimate of the slope at the beginning of the time series?\nSuppose you needed to estimate \\(s_1, s_2, s_3, \\ldots, s_p\\) for monthly sales data, where sales are highest in the summer months and lowest in the winter months. If January corresponds to month 1, which values of \\(s_t\\) would you set to be positive? negative? near zero?\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nTo implement the initial seasonality estimates, we include them in the model as \\[\n  s_{1-p}, ~ s_{2-p}, ~ s_{3-p}, ~ \\ldots, ~ s_{p-p}\n\\] This allows us to begin using the data to update the \\(s_p\\) values in the first cycle of seasonality (\\(t = 1, 2, 3, \\ldots, p\\)).",
    "crumbs": [
      "Lesson 3",
      "Holt-Winters Method (Additive Models) - Part 1"
    ]
  },
  {
    "objectID": "chapter_3_lesson_3.html#application-of-the-holt-winters-method-to-a-sample-time-series-10-min",
    "href": "chapter_3_lesson_3.html#application-of-the-holt-winters-method-to-a-sample-time-series-10-min",
    "title": "Holt-Winters Method (Additive Models) - Part 1",
    "section": "Application of the Holt-Winters Method to a Sample Time Series (10 min)",
    "text": "Application of the Holt-Winters Method to a Sample Time Series (10 min)\nThe Holt-Winters method provides a way to model a time series in which we consider the time series in layers. first, there is the level (the smoothed \\(x_t\\) values from the time series) at time \\(t\\). We will denote the level by \\(a_t\\). The level can change from time to time. We introduce a value \\(b_n\\), which we call the slope. This is the change in the level of the series from one time period to another. (As the book points out, R and many textbooks call the slope the trend.) Finally, we include a seasonal estimate, \\(s_t\\), which indicates how much the time series rises or falls above the level and trend values at time \\(t\\).\nTo visualize these terms, it can be helpful to consider the forecasting model. Suppose we have computed that Holt-Winters estimate of a time series with \\(n\\) observations. In other words, we have just fit a curve to the entire time series. We will use a very simple time series for this illustration.\nFirst, consider a time series that has a seasonal pattern of (100, 104, 100, 100) with zero trend and random components. This is illustrated in Figure 1.\n\n\n\n\n\n\n\n\nFigure 1: Sample time series with zero trend and random components\n\n\n\n\n\nNow, we add a slope of \\(\\frac{1}{2}\\) to this time series. In Figure 2, we observe the same sample time series, but with the added component of a slope.\n\n\n\n\n\n\n\n\nFigure 2: Sample time series with a trend\n\n\n\n\n\nNext, we apply the Holt-Winters method to these data. Figure 3 illustrates the Holt-Winters filter plotted against the data.\n\n\n\n\n\n\n\n\nFigure 3: Sample time series illustrating the Holt-Winters filter\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do you observe?",
    "crumbs": [
      "Lesson 3",
      "Holt-Winters Method (Additive Models) - Part 1"
    ]
  },
  {
    "objectID": "chapter_3_lesson_3.html#simulation-of-holt-winters-filtering-10-min",
    "href": "chapter_3_lesson_3.html#simulation-of-holt-winters-filtering-10-min",
    "title": "Holt-Winters Method (Additive Models) - Part 1",
    "section": "Simulation of Holt-Winters Filtering (10 min)",
    "text": "Simulation of Holt-Winters Filtering (10 min)",
    "crumbs": [
      "Lesson 3",
      "Holt-Winters Method (Additive Models) - Part 1"
    ]
  },
  {
    "objectID": "chapter_3_lesson_3.html#homework-preview-5-min",
    "href": "chapter_3_lesson_3.html#homework-preview-5-min",
    "title": "Holt-Winters Method (Additive Models) - Part 1",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_3_3.qmd",
    "crumbs": [
      "Lesson 3",
      "Holt-Winters Method (Additive Models) - Part 1"
    ]
  },
  {
    "objectID": "chapter_3_lesson_3.html#references",
    "href": "chapter_3_lesson_3.html#references",
    "title": "Holt-Winters Method (Additive Models) - Part 1",
    "section": "References",
    "text": "References\n\nC. C. Holt (1957) Forecasting seasonals and trends by exponentially weighted moving averages, ONR Research Memorandum, Carnegie Institute of Technology 52. (Reprint at https://doi.org/10.1016/j.ijforecast.2003.09.015).\nP. R. Winters (1960). Forecasting sales by exponentially weighted moving averages. Management Science, 6, 324–342. (Reprint at https://doi.org/10.1287/mnsc.6.3.324.)",
    "crumbs": [
      "Lesson 3",
      "Holt-Winters Method (Additive Models) - Part 1"
    ]
  },
  {
    "objectID": "chapter_3_lesson_2.html",
    "href": "chapter_3_lesson_2.html",
    "title": "Exponential Smoothing (EWMA)",
    "section": "",
    "text": "Implement simple exponential smoothing to estimate local mean levels\n\n\nExplain forecasting by extrapolation\nState the assumptions of exponential smoothing\nDefine exponential weighted moving average (EWMA)\nState the exponential smoothing forecasting equation\nState the EWMA in geometric series form (in terms of x_t only Eq 3.18)\nExplain the EWMA intuitively\nDefine the one-step-ahead prediction error (1PE)\nState the SS1PE used to estimate the smoothing parameter of a EWMA\nIndicate when the EWMA smoothing parameter is optimally set as 1/n",
    "crumbs": [
      "Lesson 2",
      "Exponential Smoothing (EWMA)"
    ]
  },
  {
    "objectID": "chapter_3_lesson_2.html#learning-outcomes",
    "href": "chapter_3_lesson_2.html#learning-outcomes",
    "title": "Exponential Smoothing (EWMA)",
    "section": "",
    "text": "Implement simple exponential smoothing to estimate local mean levels\n\n\nExplain forecasting by extrapolation\nState the assumptions of exponential smoothing\nDefine exponential weighted moving average (EWMA)\nState the exponential smoothing forecasting equation\nState the EWMA in geometric series form (in terms of x_t only Eq 3.18)\nExplain the EWMA intuitively\nDefine the one-step-ahead prediction error (1PE)\nState the SS1PE used to estimate the smoothing parameter of a EWMA\nIndicate when the EWMA smoothing parameter is optimally set as 1/n",
    "crumbs": [
      "Lesson 2",
      "Exponential Smoothing (EWMA)"
    ]
  },
  {
    "objectID": "chapter_3_lesson_2.html#preparation",
    "href": "chapter_3_lesson_2.html#preparation",
    "title": "Exponential Smoothing (EWMA)",
    "section": "Preparation",
    "text": "Preparation\n\nRead Section 3.4.1",
    "crumbs": [
      "Lesson 2",
      "Exponential Smoothing (EWMA)"
    ]
  },
  {
    "objectID": "chapter_3_lesson_2.html#learning-journal-exchange-10-min",
    "href": "chapter_3_lesson_2.html#learning-journal-exchange-10-min",
    "title": "Exponential Smoothing (EWMA)",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading another student’s?\nWhat would you recommend the other student add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 2",
      "Exponential Smoothing (EWMA)"
    ]
  },
  {
    "objectID": "chapter_3_lesson_2.html#class-discussion-theory-supporting-the-ewma",
    "href": "chapter_3_lesson_2.html#class-discussion-theory-supporting-the-ewma",
    "title": "Exponential Smoothing (EWMA)",
    "section": "Class Discussion: Theory Supporting the EWMA",
    "text": "Class Discussion: Theory Supporting the EWMA\nOur objective is to predict a future value given the first \\(n\\) observations of a time series. One example would be to forecast sales of an existing product in a stable market.\nWe assume the following about the time series:\n\nThere is no systematic trend or seasonal effects (or that these have been removed).\nThe mean is non-stationary (can change), but we have no idea about the direction.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nMatch the following definitions to the appropriate term.\n\n\n\nA future observation of a time series\nModel assumed in EWMA agorithm\nNon-stationary mean\nRandom correlated error terms\nA fixed constant between 0 and 1\n\n\n\n\n\\(\\alpha\\)\n\\(\\mu_t\\)\n\\(w_t\\)\n\\(x_{n+k}\\)\n\\(x_t = \\mu_t + w_t\\)\n\n\n\n\n\nThe idea is that we will use past observations to predict future observations. Our best prediction of future observations under this model is the mean of the estimate at the time of the last observation. In symbols, if we want to predict the value of the time series at time \\(n+k\\), we use our estimate of the time series at time \\(n\\):\n\\[\n\\hat x_{n+k \\mid n} = a_n, ~~~~~~~~~~~~~~~~~~\\text{where}~~ k = 1, 2, 3, \\ldots ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ (3.16)\n\\]\nHere are three equivalent expressions that define the EWMA estimation technique:\n\\[\\begin{align}\n  a_t &= \\alpha x_t + (1-\\alpha) a_{t-1} & (3.15) \\\\\n  a_t &= \\alpha (x_t - a_{t-1} ) + a_{t-1} & (3.17) \\\\\n  a_t &= \\alpha x_t + \\alpha(1-\\alpha) x_{t-1} + \\alpha(1-\\alpha)^2 x_{t-2} + \\alpha(1-\\alpha)^3 x_{t-3} + \\cdots & (3.18)\n\\end{align}\\]\nWe assume that \\(a_1 = x_1\\).\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat happens if \\(\\alpha\\) is close to 1?\nWhat happens if \\(\\alpha\\) is close to 0?\nWhat is a common “compromise” value for \\(\\alpha\\)?\nShow that Equation (3.15) and (3.17) are equivalent.\nHow would you show that Equation (3.18) is equivalent to Equation (3.17)?",
    "crumbs": [
      "Lesson 2",
      "Exponential Smoothing (EWMA)"
    ]
  },
  {
    "objectID": "chapter_3_lesson_2.html#class-activity-practice-applying-the-ewma-20-min",
    "href": "chapter_3_lesson_2.html#class-activity-practice-applying-the-ewma-20-min",
    "title": "Exponential Smoothing (EWMA)",
    "section": "Class Activity: Practice Applying the EWMA (20 min)",
    "text": "Class Activity: Practice Applying the EWMA (20 min)\n Tables-Handout-Excel \nWe will use the following data to practice applying the Exponentially Weighted Moving Average (EWMA) technique. Use \\(\\alpha = 0.2\\).\n\n\nx &lt;- c( 4.4, 4.2, 4.2, 4, 4.4, 4.7, 4.9, 5.3, 5.4, 5.5 )\n\n\nStart filling Table 1 by calculating the values of \\(a_t\\) for each value of \\(x_t\\).\n\n\n\n\nTable 1: Calculation of \\(a_t\\) and \\(e_t\\) for a sample time series\n\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$a_t$$\n$$e_t$$\n\n\n\n\n1\n4.4\n\n\n\n\n2\n4.2\n\n\n\n\n3\n4.2\n\n\n\n\n4\n4\n\n\n\n\n5\n4.4\n\n\n\n\n6\n4.7\n\n\n\n\n7\n4.9\n\n\n\n\n8\n5.3\n\n\n\n\n9\n5.4\n\n\n\n\n10\n5.5\n\n\n\n\n\n\n\n\n\n\n\n\n\nOne-Step-Ahead Prediction Errors\n\nDefinition\nIf we have a time series \\(\\{x_1, x_2, \\ldots, x_n\\}\\) and start with \\(a_1 = x_1\\), we can compute the value of \\(a_t\\) if \\(2 \\le t \\le n\\). We define the one-step-ahead prediction error, \\(e_t\\) as\n\\[\n  e_t = x_t - \\hat x_{t|t-1} = x_t - a_{t-1}\n\\]\n\n\nCalculation of \\(\\alpha\\) in R\nR uses the one-step-ahead prediction error to estimate \\(\\alpha\\). It chooses \\(\\alpha\\) to minimize\n\\[\n  SS1PE = \\sum_{t=2}^n e_t^2 = e_2^2 + e_3^2 + e_4^2 + \\cdots + e_n^2\n\\]\nwhere \\(a_1 = x_1\\).\nIf the mean of a long time series has not changed much, then this will produce a value of \\(\\alpha\\) that is unduly small. A small value of \\(\\alpha\\) prevents the model from responding to rapid future changes in the time series.\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nIf the mean of the process does not change much, what is the optimal value of \\(\\alpha\\)?\n\n\n\n\n\n\nComputation of the One-Step-Ahead Prediction Errors\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nCompute \\(e_t\\) for the 10 values of \\(x\\) given in the table above. Add these values to the table.\nCompute SS1PE for these data.",
    "crumbs": [
      "Lesson 2",
      "Exponential Smoothing (EWMA)"
    ]
  },
  {
    "objectID": "chapter_3_lesson_2.html#class-activity-highway-speeds-in-new-york-city-15-min",
    "href": "chapter_3_lesson_2.html#class-activity-highway-speeds-in-new-york-city-15-min",
    "title": "Exponential Smoothing (EWMA)",
    "section": "Class Activity: Highway Speeds in New York City (15 min)",
    "text": "Class Activity: Highway Speeds in New York City (15 min)\n\n\n\nThe Bruckner Expressway (I-278) cuts through the Bronx in New York City. Traffic sensors record the speed of cars every 5 minutes between Stratford Avenue and Castle Hill Avenue.\nThe data from 7/2/2022 through 7/17/2022 are contained in the file ny_speeds.csv. (Source: City of New York Department of Transportation. Online Links: http://a841‐dotweb01.nyc.gov/datafeeds.) These data have been cleaned to align the observations with five minute intervals.\nWe will read these data into a tsibble.\n\n\n\n\n\n\n\n\n\nFigure 1: Google Map showing the location of the data collection.\n\n\n\n\n\n\n\nShow the code\n# Read the ny_speeds data\nny_speeds_dat &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/ny_speeds.csv\")\n\nny_speeds_tb &lt;- ny_speeds_dat |&gt;\n  mutate(\n    year = lubridate::year(date),\n    month = lubridate::month(date),\n    #\n    day = lubridate::day(date),\n    hour = lubridate::hour(date),\n    min = lubridate::minute(date)\n  ) |&gt;\n  rename(speed = Speed) |&gt;\n  dplyr::select(date, year, month, day, hour, min, speed) |&gt;\n  tibble()\n\nny_speeds_ts &lt;- ny_speeds_tb |&gt;\n  mutate(index = date) |&gt;\n  as_tsibble(index = index)\n\n\nHere are the first few lines of the tsibble:\n\n\n# A tsibble: 10 x 8 [5m] &lt;UTC&gt;\n   date                 year month   day  hour   min speed index              \n   &lt;dttm&gt;              &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;dttm&gt;             \n 1 2022-07-02 00:00:00  2022     7     2     0     0  52.2 2022-07-02 00:00:00\n 2 2022-07-02 00:05:00  2022     7     2     0     5  53.4 2022-07-02 00:05:00\n 3 2022-07-02 00:10:00  2022     7     2     0    10  55.9 2022-07-02 00:10:00\n 4 2022-07-02 00:15:00  2022     7     2     0    15  56.5 2022-07-02 00:15:00\n 5 2022-07-02 00:20:00  2022     7     2     0    20  56.5 2022-07-02 00:20:00\n 6 2022-07-02 00:25:00  2022     7     2     0    25  53.4 2022-07-02 00:25:00\n 7 2022-07-02 00:30:00  2022     7     2     0    30  55.9 2022-07-02 00:30:00\n 8 2022-07-02 00:35:00  2022     7     2     0    35  54.7 2022-07-02 00:35:00\n 9 2022-07-02 00:40:00  2022     7     2     0    40  53.4 2022-07-02 00:40:00\n10 2022-07-02 00:45:00  2022     7     2     0    45  54.0 2022-07-02 00:45:00\n\n\nHere is a time plot of the data:\n\n\nShow the code\nautoplot(ny_speeds_ts, .vars = speed) +\n  labs(\n    x = \"Date / Time\",\n    y = \"Traffic Speed (mph)\",\n    title = \"Bruckner Expressway Traffic Speed\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFigure 2: Time series plot of the traffic speed data from July 2 through July 17, 2022\n\n\n\n\n\nThe data set spans 16 days. There is a strong daily pattern, which can be seen by the 16 pairs of peaks and valleys. It might be easier to see if we look at the observations from the first 2 days.\n\n\n\n\n\n\n\n\nFigure 3: Time series plot of the first two days of the traffic speed data\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do you no notice about this time series?\nDuring what parts of the day does the traffic flow tend to be slowest? fastest? What is the reason for this behavior?\n\n\n\nNext, we compute the decomposition. We expect that that traffic will fluctuate with daily seasonality. The number of five-minute periods in a day is:\n\\[\n\\underbrace{\\frac{60}{5}}_{\\substack{\\text{Number of} \\\\ \\text{5-minute periods} \\\\ \\text{per hour}}} \\cdot \\underbrace{24}_{\\substack{\\text{Number of} \\\\ \\text{hours} \\\\ \\text{per day}}} = ~~ 288~~\\text{five-minute periods per day}\n\\]\nThat is, the time series will follow a cycle that repeats every 288 five-minute intervals. Due to the complexity of the time series, we will indicate this to R in our function call. (Try replacing “speed ~ season(288)” in the classical_decomposition command with “speed” and see what happens!)\n\nny_speeds_decompose &lt;- ny_speeds_ts |&gt;\n    model(feasts::classical_decomposition(speed ~ season(288),\n        type = \"add\"))  |&gt;\n    components()\n\nautoplot(ny_speeds_decompose)\n\n\n\n\n\n\n\nFigure 4: Additive decomposition of the traffic speed time series\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWorking with your partner, identify evidence of the daily seasonal component of the speed data.\nHow do you explain the sharp downward spikes in the data?\nDoes there seem to be a trend in the data? Justify your answer.\nWhy is there no estimate for the trend or the random component for the first half of the first day and the last half of the last day?\n\n\n\nHere is a plot of the decomposition for the first 4 days of the time series (July 2, 2022 at 0:00 to July 6, 2022 at 0:00).\n\n\nShow the code\nny_speeds_decompose |&gt; \n  filter((index) &lt;= mdy_hm(\"7/6/2022 0:00\")) |&gt; \n  autoplot()\n\n\n\n\n\n\n\n\nFigure 5: Additive decomposition of the first four days of the traffic speed time series\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nConsider the seasonal variation. Around what time of day are the speeds highest? When are the speeds lowest?\n\n\n\n\n\n\n\n\n\nApplying the EWMA when there is a trend and/or seasonal variation\n\n\n\nOne assumption for the EWMA is that there is no trend or seasonal variation. If present, we need to eliminate the trend and seasonal variation before applying the EWMA algorithm. In this case, we decompose the time series and compute the EWMA for the random component. Finally, we add the trend and seasonal components back in to get the EWMA estimate for the time series.\n\n\nThese data demonstrate distinct seasonal variation. We will need to apply the EWMA algorithm to the random component and later add the seasonal component and trend to our EWMA estimate.\nWe demonstrate the computation of the EWMA for a few choices of \\(\\alpha\\).\n\nEWMA with \\(\\alpha = 0.01\\)EWMA with \\(\\alpha = 0.05\\)EWMA with \\(\\alpha = 0.2\\)EWMA with \\(\\alpha = 0.75\\)\n\n\n\n\n\n\n\n\nEWMA\n\n\n\n\n\n\n\nTable 2: Illustration of the computation of the EWMA\n\n\n\n\n\n\nDate-Time\nRandom $$ \\hat z_t $$\nComputation of $$ a_t $$\n$$a_t$$\n\n\n\n\n7/2/2022 12:00\n-18.258\n-18.258 =\n-18.258\n\n\n7/2/2022 12:05\n-17.597\n0.01 * (-17.597) + (1 - 0.01) * (-18.258) =\n-18.251\n\n\n7/2/2022 12:10\n-17.095\n0.01 * (-17.095) + (1 - 0.01) * (-18.251) =\n-18.24\n\n\n7/2/2022 12:15\n-16.342\n0.01 * (-16.342) + (1 - 0.01) * (-18.24) =\n-18.221\n\n\n7/2/2022 12:20\n-17.627\n0.01 * (-17.627) + (1 - 0.01) * (-18.221) =\n-18.215\n\n\n7/2/2022 12:25\n-14.03\n0.01 * (-14.03) + (1 - 0.01) * (-18.215) =\n-18.173\n\n\n\n\n\n\n\n\n\n\nHere is a plot showing the EWMA for the random component for all the data.\n\n\n\n\n\n\n\n\nFigure 6: EWMA superimposed on the random component of the traffic data time series\n\n\n\n\n\nHere is a plot of the EWMA against the random component for four hours:\n\n\n\n\n\n\n\n\nFigure 7: EWMA superimposed on the random component of the traffic data for a four-hour period\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEWMA\n\n\n\n\n\n\n\nTable 3: Illustration of the computation of the EWMA\n\n\n\n\n\n\nDate-Time\nRandom $$ \\hat z_t $$\nComputation of $$ a_t $$\n$$a_t$$\n\n\n\n\n7/2/2022 12:00\n-18.258\n-18.258 =\n-18.258\n\n\n7/2/2022 12:05\n-17.597\n0.05 * (-17.597) + (1 - 0.05) * (-18.258) =\n-18.225\n\n\n7/2/2022 12:10\n-17.095\n0.05 * (-17.095) + (1 - 0.05) * (-18.225) =\n-18.168\n\n\n7/2/2022 12:15\n-16.342\n0.05 * (-16.342) + (1 - 0.05) * (-18.168) =\n-18.077\n\n\n7/2/2022 12:20\n-17.627\n0.05 * (-17.627) + (1 - 0.05) * (-18.077) =\n-18.054\n\n\n7/2/2022 12:25\n-14.03\n0.05 * (-14.03) + (1 - 0.05) * (-18.054) =\n-17.853\n\n\n\n\n\n\n\n\n\n\nHere is a plot showing the EWMA for the random component for all the data.\n\n\n\n\n\n\n\n\nFigure 8: EWMA superimposed on the random component of the traffic data time series\n\n\n\n\n\nHere is a plot of the EWMA against the random component for four hours:\n\n\n\n\n\n\n\n\nFigure 9: EWMA superimposed on the random component of the traffic data for a four-hour period\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEWMA\n\n\n\n\n\n\n\nTable 4: Illustration of the computation of the EWMA\n\n\n\n\n\n\nDate-Time\nRandom $$ \\hat z_t $$\nComputation of $$ a_t $$\n$$a_t$$\n\n\n\n\n7/2/2022 12:00\n-18.258\n-18.258 =\n-18.258\n\n\n7/2/2022 12:05\n-17.597\n0.2 * (-17.597) + (1 - 0.2) * (-18.258) =\n-18.126\n\n\n7/2/2022 12:10\n-17.095\n0.2 * (-17.095) + (1 - 0.2) * (-18.126) =\n-17.919\n\n\n7/2/2022 12:15\n-16.342\n0.2 * (-16.342) + (1 - 0.2) * (-17.919) =\n-17.604\n\n\n7/2/2022 12:20\n-17.627\n0.2 * (-17.627) + (1 - 0.2) * (-17.604) =\n-17.608\n\n\n7/2/2022 12:25\n-14.03\n0.2 * (-14.03) + (1 - 0.2) * (-17.608) =\n-16.893\n\n\n\n\n\n\n\n\n\n\nHere is a plot showing the EWMA for the random component for all the data.\n\n\n\n\n\n\n\n\nFigure 10: EWMA superimposed on the random component of the traffic data time series\n\n\n\n\n\nHere is a plot of the EWMA against the random component for four hours:\n\n\n\n\n\n\n\n\nFigure 11: EWMA superimposed on the random component of the traffic data for a four-hour period\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nEWMA\n\n\n\n\n\n\n\nTable 5: Illustration of the computation of the EWMA\n\n\n\n\n\n\nDate-Time\nRandom $$ \\hat z_t $$\nComputation of $$ a_t $$\n$$a_t$$\n\n\n\n\n7/2/2022 12:00\n-18.258\n-18.258 =\n-18.258\n\n\n7/2/2022 12:05\n-17.597\n0.75 * (-17.597) + (1 - 0.75) * (-18.258) =\n-17.762\n\n\n7/2/2022 12:10\n-17.095\n0.75 * (-17.095) + (1 - 0.75) * (-17.762) =\n-17.261\n\n\n7/2/2022 12:15\n-16.342\n0.75 * (-16.342) + (1 - 0.75) * (-17.261) =\n-16.572\n\n\n7/2/2022 12:20\n-17.627\n0.75 * (-17.627) + (1 - 0.75) * (-16.572) =\n-17.363\n\n\n7/2/2022 12:25\n-14.03\n0.75 * (-14.03) + (1 - 0.75) * (-17.363) =\n-14.864\n\n\n\n\n\n\n\n\n\n\nHere is a plot showing the EWMA for the random component for all the data.\n\n\n\n\n\n\n\n\nFigure 12: EWMA superimposed on the random component of the traffic data time series\n\n\n\n\n\nHere is a plot of the EWMA against the random component for four hours:\n\n\n\n\n\n\n\n\nFigure 13: EWMA superimposed on the random component of the traffic data for a four-hour period\n\n\n\n\n\n\n\n\n\n\nOnce we have obtained the EWMA for the random component, we can add to it the trend and seasonality to get the EWMA for the original speed data. The raw data are plotted in black, the trend in blue, the seasonal component in yellow, and the EWMA is plotted in orange. The green curve is the sum of EWMA, the seasonal component, and the trend.\n\n\n\n\n\n\n\n\nFigure 14: Components of the EWMA superimposed on the random component of the New York traffic data\n\n\n\n\n\nHere is the plot for four hours of data:\n\n\n\n\n\n\n\n\nFigure 15: Components of the EWMA superimposed on the random component of the New York traffic data, four hours only",
    "crumbs": [
      "Lesson 2",
      "Exponential Smoothing (EWMA)"
    ]
  },
  {
    "objectID": "chapter_3_lesson_2.html#class-activity-ewma-in-r-10-min",
    "href": "chapter_3_lesson_2.html#class-activity-ewma-in-r-10-min",
    "title": "Exponential Smoothing (EWMA)",
    "section": "Class Activity: EWMA in R (10 min)",
    "text": "Class Activity: EWMA in R (10 min)\nWe will compute the EWMA for the random component of the New York highway speed data in R. We have already decomposed the model, so we have isolated the estimated values for the random component. This is a portion of the tsibble ny_speeds_decompose:\n\n\n\n\nTable 6: A portion of the tsibble containing the EWMA for the random component of the traffic data\n\n\n\n\n\n\n.model\nindex\nspeed\ntrend\nseasonal\nrandom\nseason_adjust\n\n\n\n\nfeasts::classical_decomposition(speed ~ season(288), type = \"add\")\n2022-07-02\n52.19\nNA\n2.557\nNA\n49.633\n\n\nfeasts::classical_decomposition(speed ~ season(288), type = \"add\")\n2022-07-02 00:05:00\n53.43\nNA\n2.884\nNA\n50.546\n\n\nfeasts::classical_decomposition(speed ~ season(288), type = \"add\")\n2022-07-02 00:10:00\n55.92\nNA\n2.463\nNA\n53.457\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\nfeasts::classical_decomposition(speed ~ season(288), type = \"add\")\n2022-07-02 11:55:00\n30.44\nNA\n-4.195\nNA\n34.635\n\n\nfeasts::classical_decomposition(speed ~ season(288), type = \"add\")\n2022-07-02 12:00:00\n26.71\n51.969\n-7.002\n-18.258\n33.712\n\n\nfeasts::classical_decomposition(speed ~ season(288), type = \"add\")\n2022-07-02 12:05:00\n25.47\n51.973\n-8.906\n-17.597\n34.376\n\n\nfeasts::classical_decomposition(speed ~ season(288), type = \"add\")\n2022-07-02 12:10:00\n25.47\n51.969\n-9.405\n-17.095\n34.875\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\nfeasts::classical_decomposition(speed ~ season(288), type = \"add\")\n2022-07-17 11:45:00\n53.43\n54.612\n-4.153\n2.971\n57.583\n\n\nfeasts::classical_decomposition(speed ~ season(288), type = \"add\")\n2022-07-17 11:50:00\n54.05\n54.616\n-4.485\n3.918\n58.535\n\n\nfeasts::classical_decomposition(speed ~ season(288), type = \"add\")\n2022-07-17 11:55:00\n53.43\n54.626\n-4.195\n2.999\n57.625\n\n\nfeasts::classical_decomposition(speed ~ season(288), type = \"add\")\n2022-07-17 12:00:00\n50.33\nNA\n-7.002\nNA\n57.332\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\nfeasts::classical_decomposition(speed ~ season(288), type = \"add\")\n2022-07-17 23:45:00\n55.3\nNA\n-1.237\nNA\n56.537\n\n\nfeasts::classical_decomposition(speed ~ season(288), type = \"add\")\n2022-07-17 23:50:00\n55.92\nNA\n-1.905\nNA\n57.825\n\n\nfeasts::classical_decomposition(speed ~ season(288), type = \"add\")\n2022-07-17 23:55:00\n57.16\nNA\n1.528\nNA\n55.632\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWarning\n\n\n\nWhen applying EWMA to the random component of the time series, delete the variable .model and all NA values from the data frame. If you neglect this step, there will be errors in the code below.\n\n\nWe drop the variable .model and omit all NA values. (Remember the first half-day and the last half-day in the time series do not yield estimates for the random component.)\n\nny_speeds_random &lt;- ny_speeds_decompose |&gt;\n  na.omit() |&gt;                                # Omit all NA values\n  select(-.model)                             # Eliminate the variable .model\n\nThe following code can be used to execute the EWMA algorithm on the random component using \\(\\alpha = 0.2\\).\n\nny_speeds_model1 &lt;- ny_speeds_random |&gt;\n  model(Additive = ETS(random ~\n                         trend(\"A\", alpha = 0.2, beta = 0) +\n                         error(\"A\") + season(\"N\"),\n                       opt_crit = \"amse\", nmse = 1))\nreport(ny_speeds_model1)\n\nSeries: random \nModel: ETS(A,A,N) \n  Smoothing parameters:\n    alpha = 0.2 \n    beta  = 0 \n\n  Initial states:\n      l[0]        b[0]\n -13.52924 0.003643793\n\n  sigma^2:  22.2348\n\n     AIC     AICc      BIC \n49563.94 49563.94 49583.05 \n\n\nNow, we calculate the value of SS1PE.\n\nsum(components(ny_speeds_model1)$remainder^2, na.rm = TRUE)\n\n[1] 95965.59\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nModify the code analyzing the New York traffic speed data by deleting the argument alpha = 0.2. \n\nWhat value of \\(\\alpha\\) is obtained?\nWhat is the value of the SS1PE for this value of \\(\\alpha\\)?\n\nChoose values of \\(\\alpha\\) slightly less than and slightly greater than 0.6722.\n\nCan you find a value of \\(\\alpha\\) that gives you a smaller SS1PE than 77661.59?",
    "crumbs": [
      "Lesson 2",
      "Exponential Smoothing (EWMA)"
    ]
  },
  {
    "objectID": "chapter_3_lesson_2.html#homework-preview-5-min",
    "href": "chapter_3_lesson_2.html#homework-preview-5-min",
    "title": "Exponential Smoothing (EWMA)",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_3_2.qmd \n\n\n Handout & Matching Activity\n\n Tables-Handout-Excel-key \nSolutions to matching activity:\n\nD\nE\nB\nC\nA\n\n\nClass Activity: Practice Applying the EWMA\n\n\n\n\n\nTable 7: Calculation of \\(a_t\\) and \\(e_t\\) for a sample time series: Solution for Table 1\n\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$a_t$$\n$$e_t$$\n\n\n\n\n$$ 1 $$\n$$ 4.4 $$\n$$4.4$$\n\n\n\n$$ 2 $$\n$$ 4.2 $$\n$$ 0.2 \\cdot 4.2 + (1 - 0.2 ) \\cdot 4.4 = 4.36 $$\n$$ 4.2 - 4.4 = -0.2 $$\n\n\n$$ 3 $$\n$$ 4.2 $$\n$$ 0.2 \\cdot 4.2 + (1 - 0.2 ) \\cdot 4.36 = 4.328 $$\n$$ 4.2 - 4.36 = -0.16 $$\n\n\n$$ 4 $$\n$$ 4 $$\n$$ 0.2 \\cdot 4 + (1 - 0.2 ) \\cdot 4.328 = 4.262 $$\n$$ 4 - 4.328 = -0.328 $$\n\n\n$$ 5 $$\n$$ 4.4 $$\n$$ 0.2 \\cdot 4.4 + (1 - 0.2 ) \\cdot 4.262 = 4.29 $$\n$$ 4.4 - 4.262 = 0.138 $$\n\n\n$$ 6 $$\n$$ 4.7 $$\n$$ 0.2 \\cdot 4.7 + (1 - 0.2 ) \\cdot 4.29 = 4.372 $$\n$$ 4.7 - 4.29 = 0.41 $$\n\n\n$$ 7 $$\n$$ 4.9 $$\n$$ 0.2 \\cdot 4.9 + (1 - 0.2 ) \\cdot 4.372 = 4.478 $$\n$$ 4.9 - 4.372 = 0.528 $$\n\n\n$$ 8 $$\n$$ 5.3 $$\n$$ 0.2 \\cdot 5.3 + (1 - 0.2 ) \\cdot 4.478 = 4.642 $$\n$$ 5.3 - 4.478 = 0.822 $$\n\n\n$$ 9 $$\n$$ 5.4 $$\n$$ 0.2 \\cdot 5.4 + (1 - 0.2 ) \\cdot 4.642 = 4.794 $$\n$$ 5.4 - 4.642 = 0.758 $$\n\n\n$$ 10 $$\n$$ 5.5 $$\n$$ 0.2 \\cdot 5.5 + (1 - 0.2 ) \\cdot 4.794 = 4.935 $$\n$$ 5.5 - 4.794 = 0.706 $$\n\n\n\n\n\n\n\n\n\n\nThe value of SS1PE is \\[\n  \\sum\\limits_{t=2}^n e_t^2\n    = (-0.2)^2 + (-0.16)^2 + \\cdots + (0.758)^2 + (0.706)^2\n    = 2.389\n\\]",
    "crumbs": [
      "Lesson 2",
      "Exponential Smoothing (EWMA)"
    ]
  },
  {
    "objectID": "chapter_3_lesson_1.html",
    "href": "chapter_3_lesson_1.html",
    "title": "Leading Variables and Associated Variables",
    "section": "",
    "text": "Explain the purpose and limitations of forecasting\n\n\nDefine lead time\nDefine forecasting\nDifferentiate causation from correlation\n\n\n\n\nExplain why there is not one correct model to describe a time series\n\n\nExplain why there can be several suitable models for a given time series\n\n\n\n\nUse cross-correlation analysis to quantify lead/lag relationships\n\n\nExplain forecasting by leading indicators\nDefine the population k-lag ccvf\nDefine the population k-lag ccf\nDefine the sample k-lag ccvf\nDefine the sample k-lag ccf\nEstimate an ccf for two time series\nInterpret whether a variable is a leading indicator using a cross-correlogram\n\n\n\n\nEvaluate the limitations of forecasting models based on past trends\n\n\nExplain how unexpected future events may invalidate forecast trends\nAvoid over-extrapolation of fitted trends beyond reasonable time horizons",
    "crumbs": [
      "Lesson 1",
      "Leading Variables and Associated Variables"
    ]
  },
  {
    "objectID": "chapter_3_lesson_1.html#learning-outcomes",
    "href": "chapter_3_lesson_1.html#learning-outcomes",
    "title": "Leading Variables and Associated Variables",
    "section": "",
    "text": "Explain the purpose and limitations of forecasting\n\n\nDefine lead time\nDefine forecasting\nDifferentiate causation from correlation\n\n\n\n\nExplain why there is not one correct model to describe a time series\n\n\nExplain why there can be several suitable models for a given time series\n\n\n\n\nUse cross-correlation analysis to quantify lead/lag relationships\n\n\nExplain forecasting by leading indicators\nDefine the population k-lag ccvf\nDefine the population k-lag ccf\nDefine the sample k-lag ccvf\nDefine the sample k-lag ccf\nEstimate an ccf for two time series\nInterpret whether a variable is a leading indicator using a cross-correlogram\n\n\n\n\nEvaluate the limitations of forecasting models based on past trends\n\n\nExplain how unexpected future events may invalidate forecast trends\nAvoid over-extrapolation of fitted trends beyond reasonable time horizons",
    "crumbs": [
      "Lesson 1",
      "Leading Variables and Associated Variables"
    ]
  },
  {
    "objectID": "chapter_3_lesson_1.html#preparation",
    "href": "chapter_3_lesson_1.html#preparation",
    "title": "Leading Variables and Associated Variables",
    "section": "Preparation",
    "text": "Preparation\n\nRead Sections 3.1-3.2\n\nNote: There is a typo in the book on page 47. Equation (3.5) gives the sample ccf, not the sample acf.",
    "crumbs": [
      "Lesson 1",
      "Leading Variables and Associated Variables"
    ]
  },
  {
    "objectID": "chapter_3_lesson_1.html#learning-journal-exchange-10-min",
    "href": "chapter_3_lesson_1.html#learning-journal-exchange-10-min",
    "title": "Leading Variables and Associated Variables",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading another student’s?\nWhat would you recommend the other student add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 1",
      "Leading Variables and Associated Variables"
    ]
  },
  {
    "objectID": "chapter_3_lesson_1.html#small-group-discussion-why-forecast-5-min",
    "href": "chapter_3_lesson_1.html#small-group-discussion-why-forecast-5-min",
    "title": "Leading Variables and Associated Variables",
    "section": "Small Group Discussion: Why Forecast? (5 min)",
    "text": "Small Group Discussion: Why Forecast? (5 min)\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhy are we interested in forecasting a time series?\nGive an example of a forecast for a time series that you have been interested in lately.\nExplain why there can be several suitable models for a given time series.",
    "crumbs": [
      "Lesson 1",
      "Leading Variables and Associated Variables"
    ]
  },
  {
    "objectID": "chapter_3_lesson_1.html#class-discussion-definition-of-the-sample-ccvf-and-sample-ccf-5-min",
    "href": "chapter_3_lesson_1.html#class-discussion-definition-of-the-sample-ccvf-and-sample-ccf-5-min",
    "title": "Leading Variables and Associated Variables",
    "section": "Class Discussion: Definition of the Sample CCVF and Sample CCF (5 min)",
    "text": "Class Discussion: Definition of the Sample CCVF and Sample CCF (5 min)\n\nSample Cross-Covariance Function (ccvf)\nIn Chapter 2, Lesson 2, we explored the covariance of a time series with itself, shifted by \\(k\\) units of time. Now, we will consider a similar idea, where we compare one time series as being related to a shift of \\(k\\) time units relative to another time series. When one time series leads another, we can sometimes use the one that leads to predict the one that lags–at least in the short term.\nThe sample cross-covariance function (ccvf) is defined as:\n\\[\n  c_k(x,y) = \\frac{1}{n} \\sum\\limits_{t=1}^{n-k} \\left(x_{t+k} - \\bar x \\right) \\left( y_t - \\bar y \\right)\n\\]\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat does the notation \\(x_{t+k}\\) mean?\nExplain the concept of the ccvf.\nCompare and contrast the ccvf to the acvf.\nDescribe the quantity \\(c_k(x,y)\\) to your partner.\n\nExplain every component of the expression.\nDiscuss how to compute each part.\nIn your own words, explain what the result means.\n\nGive the expression for the ccvf of a time series with itself, \\(c_k(x,x)\\).\n\nWhat is this function called?\n\nFind an expression for \\(c_0(x,x)\\).\n\nSelected Solutions\n\nWe can compute the ccvf of a time series with itself:\n\\[\n  c_k(x,x) = \\frac{1}{n} \\sum_{t=1}^{n-k} \\left( x_{t+k} - \\bar x \\right) \\left( x_t - \\bar x \\right)\n\\]\nIn particular, if \\(k=0\\), this reduces to:\n\\[\n  c_0(x,x) = \\frac{1}{n} \\sum_{t=1}^{n-k} \\left( x_{t} - \\bar x \\right)^2\n\\]\n\n\n\n\n\nSample Cross-Correlation Function (ccf)\nWe define the sample cross-correlation function to help us identify when one variable leads another and by how many time periods. Note that \\(r_k\\), given in the book as Equation (3.5), is misidentified there as the sample acf.\n\\[\n  r_k(x,y) =\n    \\frac{\n        c_k(x,y)\n    }{\n        \\sqrt{ c_0(x,x) \\cdot c_0(y,y) }\n    }\n\\]\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nWork with your assigned partner to do the following:\n\nDescribe the quantity \\(r_k(x,y)\\) to your partner.\n\nExplain every component of the expression.\nDiscuss how to compute each part.\nIn your own words, explain what the result means.\n\nWhat are the bounds on the values of \\(r_k\\)?",
    "crumbs": [
      "Lesson 1",
      "Leading Variables and Associated Variables"
    ]
  },
  {
    "objectID": "chapter_3_lesson_1.html#small-group-activity-computing-the-sample-ccf-20-min",
    "href": "chapter_3_lesson_1.html#small-group-activity-computing-the-sample-ccf-20-min",
    "title": "Leading Variables and Associated Variables",
    "section": "Small Group Activity: Computing the Sample CCF (20 min)",
    "text": "Small Group Activity: Computing the Sample CCF (20 min)\nSuppose we have collected 10 values each from two time series. You can use the following command to read the values into R.\n\n\nsample_df &lt;- data.frame( \n  x = c(21, 20, 17, 15, 18, 21, 21, 24, 22, 21),  \n  y = c(14, 16, 18, 17, 12, 10, 11, 16, 14, 22)  \n) \n\n\n\nFigure 1: Superimposed plot of two simulated time series\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nComplete Tables 1 and 2 to calculate \\(c_k\\) for the given values of \\(k\\).\n\nTable 1: Computation of squared deviations\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$y_t$$\n$$x_t - \\bar x$$\n$$(x_t - \\bar x)^2$$\n$$y_t - \\bar y$$\n$$(y_t - \\bar y)^2$$\n\n\n\n\n1\n21\n14\n1\n1\n\n\n\n\n2\n20\n16\n0\n0\n\n\n\n\n3\n17\n18\n-3\n9\n\n\n\n\n4\n15\n17\n-5\n25\n\n\n\n\n5\n18\n12\n-2\n4\n\n\n\n\n6\n21\n10\n1\n1\n\n\n\n\n7\n21\n11\n1\n1\n\n\n\n\n8\n24\n16\n4\n16\n\n\n\n\n9\n22\n14\n2\n4\n\n\n\n\n10\n21\n22\n1\n1\n\n\n\n\nSum\n200\n150\n0\n62\n\n\n\n\n\n\n\n\n\nTable 2: Computation of \\(c_k\\) and \\(r_k\\) for select values of \\(k\\)\n\n\n\n\n\n$$t$$\n$$x_t - \\bar x$$\n$$y_t - \\bar y$$\n$$~k=-4~$$\n$$~k=-3~$$\n$$~k=-2~$$\n$$~k=-1~$$\n$$~k=0~$$\n$$~k=1~$$\n$$~k=2~$$\n$$~k=3~$$\n$$~k=4~$$\n\n\n\n\n1\n1\n-1\n\n\n\n\n-1\n\n3\n5\n2\n\n\n2\n0\n1\n\n\n\n\n\n\n-5\n-2\n1\n\n\n3\n-3\n3\n\n\n\n\n\n\n-6\n3\n3\n\n\n4\n-5\n2\n\n\n\n\n\n\n2\n2\n8\n\n\n5\n-2\n-3\n\n\n\n\n\n\n-3\n-12\n-6\n\n\n6\n1\n-5\n\n\n\n\n\n\n-20\n-10\n-5\n\n\n7\n1\n-4\n\n\n\n\n\n\n-8\n-4\n—\n\n\n8\n4\n1\n\n\n\n\n\n\n1\n—\n—\n\n\n9\n2\n-1\n\n\n\n\n\n\n—\n—\n—\n\n\n10\n1\n7\n\n\n\n\n\n\n—\n—\n—\n\n\nSum\n0\n0\n\n\n\n\n\n\n-36\n-18\n3\n\n\n$$c_k$$\n\n\n\n\n\n\n\n\n-3.6\n-1.8\n0.3\n\n\n$$r_k$$\n\n\n\n\n\n\n\n\n-0.424\n-0.212\n0.035\n\n\n\n\n\n\nUse the figure below as a guide to plot the ccf values.\n\n\n\nFigure 2: Plot of the Sample CCF\n\n\n\n\n\n\n\n\n\n\nAre any of the ccf values statistically significant? If so, which one(s)?",
    "crumbs": [
      "Lesson 1",
      "Leading Variables and Associated Variables"
    ]
  },
  {
    "objectID": "chapter_3_lesson_1.html#small-group-activity-single-family-housing-starts-and-completions-10-min",
    "href": "chapter_3_lesson_1.html#small-group-activity-single-family-housing-starts-and-completions-10-min",
    "title": "Leading Variables and Associated Variables",
    "section": "Small-Group Activity: Single Family Housing Starts and Completions (10 min)",
    "text": "Small-Group Activity: Single Family Housing Starts and Completions (10 min)\nThe U. S. Census Bureau publishes monthly counts of the number of building permits issued for new housing construction, the number of housing units started, and the number of housing units completed. We will consider the number of single-family units started and completed each month.\n\nFigure 3: Time series plot of new single housing starts and completions in the U.S.\n\n\nShow the code\n# read and clean the data\nhousing &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/housing.csv\") |&gt;\n  rename(\n    permits = permits_single_family,\n    starts = starts_single_family,\n    completions = completions_single_family\n  ) |&gt;\n  mutate(date = my(month)) |&gt;\n  mutate(month = yearmonth(date)) |&gt;\n  dplyr::select(month, permits, starts, completions)  |&gt;\n  as_tsibble(index = month)\n\nautoplot(housing, .vars = starts) +\n  geom_line(data = housing, aes(x = month, y = completions), color = \"#E69F00\") +\n  labs(\n    x = \"Month\",\n    y = \"Housing Units (Thousands)\",\n    title = \"U.S. Single-Family Housing Starts and Completions\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nThe plot above illustrates the number of single-family dwellings started and completed each month from January 1968 through December 2023.\nWe decompose these time series to remove the trend and seasonal component. Then, we compute the sample ccf for the random components.\n\n\nFigure 4: CCF plot of random component of new single housing starts and completions in the U.S.\n\n\nShow the code\nstarts_r_ts &lt;- model(housing, feasts::classical_decomposition(starts)) |&gt;\n  components() |&gt;\n  select(month, random) |&gt;\n  rename(random_starts = random)\n\ncompletions_r_ts &lt;- model(housing, feasts::classical_decomposition(completions)) |&gt;\n    components() |&gt;\n  select(month, random) |&gt;\n  rename(random_completions = random)\n\nrandom_joint &lt;- starts_r_ts |&gt;\n  right_join(completions_r_ts, by = join_by(month))\n\nautoplot(random_joint, .vars = random_starts) +\n  geom_line(data = random_joint, aes(x = month, y = random_completions), color = \"#E69F00\") +\n  labs(\n    x = \"Month\",\n    y = \"Random Component (Thousands)\",\n    title = \"Random Component\",\n    subtitle = \"U.S. Single-Family Housing Starts and Completions\"\n  ) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: ACF for the random components of the housing starts and completions\n\n\nShow the code\nacf_starts &lt;- ACF(random_joint, y = random_starts) |&gt; autoplot() +\n    labs(title = \"ACF of Random Component of Single-Family Housing Starts\")\nacf_completions &lt;- ACF(random_joint, y = random_completions) |&gt; autoplot() +\n    labs(title = \"ACF of Random Component of Single-Family Housing Completions\")\nacf_starts / acf_completions\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do you observe in the acf plots for the two variables?\n\nDoes this fit your understanding of the autocorrelation that will exist in these variables? Why or why not?\n\n\n\n\n\n\nFigure 6: CCF for random components of the housing starts and completions\n\n\nShow the code\nrandom_joint %&gt;%\n  CCF(y = random_completions, x = random_starts) %&gt;%\n  autoplot() +\n  labs(\n    title = \"CCF for Random Component of Housing Starts (x) and Completions (y)\",\n    subtitle = \"Single-Family Units in the U.S.\"\n  ) +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\n\nTable 3: CCF for Random Components of Housing Starts (x) and Housing Completions (y)\n\n\nShow the code\nrandom_joint |&gt;\n  CCF(y = random_completions, x = random_starts)\n\n\n\n\n\n\n\nlag\n-12M\n-11M\n-10M\n-9M\n-8M\n-7M\n-6M\n-5M\n-4M\n-3M\n-2M\n-1M\n0M\n\n\n\n\nccf\n0.025\n-0.178\n-0.249\n-0.274\n-0.263\n-0.055\n0.067\n0.19\n0.285\n0.254\n0.278\n0.193\n0.152\n\n\n\n\n\n\n\n\nlag\n0M\n1M\n2M\n3M\n4M\n5M\n6M\n7M\n8M\n9M\n10M\n11M\n12M\n\n\n\n\nccf\n0.152\n-0.124\n-0.245\n-0.318\n-0.311\n-0.216\n-0.073\n0.069\n0.178\n0.256\n0.26\n0.165\n0.042\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do you observe in the ccf?\n\nFor what value of \\(k\\) is the ccf maximized?\n\nUsing any tool, except pre-defined functions in R, verify the values of the ccf for the housing starts and completions data for \\(k=-2\\), \\(k=-1\\), and \\(k=0\\) months.",
    "crumbs": [
      "Lesson 1",
      "Leading Variables and Associated Variables"
    ]
  },
  {
    "objectID": "chapter_3_lesson_1.html#class-activity-maximum-solar-angle-and-daily-high-temperature-5-min",
    "href": "chapter_3_lesson_1.html#class-activity-maximum-solar-angle-and-daily-high-temperature-5-min",
    "title": "Leading Variables and Associated Variables",
    "section": "Class Activity: Maximum Solar Angle and Daily High Temperature (5 min)",
    "text": "Class Activity: Maximum Solar Angle and Daily High Temperature (5 min)\nIn this example, we examine the relationship between the maximum angle the sun makes with the horizon (at midday) and the daily high temperature in Rexburg, Idaho. The maximum angle of the sun is related to the amount of heat a given area on the earth is able to absorb. If the angle is higher, we would expect warmer temperatures.\nWe can compute the maximum angle the sun makes with the horizon (the angle at solar noon) for any given day.\n\n\nShow the code\n# functions for angle conversions\ndeg2rad &lt;- function (x) {x / 180 * base::pi}\nrad2deg &lt;- function (x) {x / base::pi * 180}\n\n# Read and clean rexburg weather data\nrexburg_daily_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/rexburg_weather.csv\") |&gt;\n  mutate(year_month_day = ymd(dates)) |&gt;\n  mutate(\n    days_since_ref_date = as.integer(year_month_day - mdy(\"12/31/2010\")),\n    declination = 23.45 * sin(deg2rad(360 * (284+days_since_ref_date)/365.25)),\n    max_solar_angle = (sin(deg2rad(43.825386)) * sin(deg2rad(declination)) \n                       + cos(deg2rad(43.825386)) * cos(deg2rad(declination)) * cos(0)) \n                      |&gt; asin() \n                      |&gt; rad2deg()\n  ) |&gt;\n  rename(high_temp = rexburg_airport_high) |&gt;\n  select(year_month_day, max_solar_angle, high_temp) |&gt;\n  as_tsibble(index = year_month_day)\n\nrexburg_daily_ts %&gt;% head\n\n\n# A tsibble: 6 x 3 [1D]\n  year_month_day max_solar_angle high_temp\n  &lt;date&gt;                   &lt;dbl&gt;     &lt;int&gt;\n1 1999-01-02                23.2        30\n2 1999-01-03                23.3        25\n3 1999-01-04                23.4        26\n4 1999-01-05                23.5        29\n5 1999-01-06                23.6        32\n6 1999-01-07                23.7        31\n\n\nThe angle of the sun at solar noon is based on a deterministic formula, not daily measurements. Consequently, it is composed only of a seasonal component with period 365.25 days. If we remove the seasonal component before computing the ccf, the random component will be zero. So, even though we usually remove the trend and seasonal component before computing the ccf, we will not do it in this example.\nThe figure below illustrates the daily high temperature in Rexburg, Idaho (in black) and the angle of the sun with the horizon at solar noon (in red) over a 7-year span.\n\nFigure 7: Daily high temperature in Rexburg, Idaho and the maximum solar angle\n\n\nShow the code\nrexburg_daily_ts |&gt;\n  filter(year(year_month_day) &gt; 2016) |&gt;\n  autoplot(.vars = high_temp) +\n  geom_line(aes(x = year_month_day, y = max_solar_angle), color = \"#D55E00\", linewidth = 2) +\n  scale_y_continuous(sec.axis = sec_axis(~., name = \"Max Solar Angle (in degrees)\")) +\n  labs(\n    x = \"Date\",\n    y = \"High Temp (F)\",\n    title = \"Daily High Temperature in Rexburg, Idaho\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nNotice that the orange curve “leads” the black time series. The peaks and valleys first occur in the maximum solar angle and then days later in the daily high temperatures.\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nPlot the decomposition of the Rexburg temperature data. (Hint: include the model statement high_temp ~ season(365.25) to help R identify the seasonality.)\n\nSelected Solutions\n\n\nFigure 8: Decomposition of Rexburg’s daily high temperature time series\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: ACF and CCF plots for maximum solar angle and daily high temperature in Rexburg, Idaho\n\nacf_solar &lt;- ACF(rexburg_daily_ts, y = max_solar_angle) |&gt; autoplot() +\n    labs(title = \"Maximum Solar Angle\")\nacf_temp &lt;- ACF(rexburg_daily_ts, y = high_temp) |&gt; autoplot() +\n    labs(title = \"Daily High Temperature\")\njoint_ccf_plot &lt;- rexburg_daily_ts |&gt;\n  CCF(x = max_solar_angle, y = high_temp) |&gt; autoplot() +   # Note: x lags y; x predicts y\n  labs(title = \"CCF Plot\")\n(acf_solar + acf_temp) / joint_ccf_plot\n\n\n\n\n\n\n\n\n\n\nTable 4: CCF for maximum solar angle and daily high temperature in Rexburg, Idaho\n\n\n\n\nValues of ccf\n\n\n\n\n\n\n\n\nlag\nccf\n\n\n\n\n\n-35D\n0.9029\n\n\n\n-34D\n0.9046\n\n\n\n-33D\n0.906\n\n\n\n-32D\n0.9072\n\n\n\n-31D\n0.9081\n\n\n\n-30D\n0.9088\n\n\n\n-29D\n0.9092\n\n\n\n-28D\n0.9093\n\n\n\n-27D\n0.9092\n\n\n\n-26D\n0.9089\n\n\n\n-25D\n0.9082\n\n\n\n-24D\n0.9073\n\n\n\n-23D\n0.9061\n\n\n\n-22D\n0.9046\n\n\n\n-21D\n0.9028\n\n\n\n-20D\n0.9008\n\n\n\n-19D\n0.8985\n\n\n\n-18D\n0.8959\n\n\n\n\n\n\n\n\n\nlag\nccf\n\n\n\n\n\n-17D\n0.893\n\n\n\n-16D\n0.8898\n\n\n\n-15D\n0.8864\n\n\n\n-14D\n0.8827\n\n\n\n-13D\n0.8788\n\n\n\n-12D\n0.8746\n\n\n\n-11D\n0.8701\n\n\n\n-10D\n0.8654\n\n\n\n-9D\n0.8605\n\n\n\n-8D\n0.8552\n\n\n\n-7D\n0.8498\n\n\n\n-6D\n0.8441\n\n\n\n-5D\n0.8381\n\n\n\n-4D\n0.8319\n\n\n\n-3D\n0.8255\n\n\n\n-2D\n0.8189\n\n\n\n-1D\n0.812\n\n\n\n0D\n0.8048\n\n\n\n\n\n\n\n\n\nlag\nccf\n\n\n\n\n\n1D\n0.7971\n\n\n\n2D\n0.7892\n\n\n\n3D\n0.781\n\n\n\n4D\n0.7726\n\n\n\n5D\n0.7639\n\n\n\n6D\n0.755\n\n\n\n7D\n0.7458\n\n\n\n8D\n0.7365\n\n\n\n9D\n0.727\n\n\n\n10D\n0.7173\n\n\n\n11D\n0.7073\n\n\n\n12D\n0.697\n\n\n\n13D\n0.6866\n\n\n\n14D\n0.6761\n\n\n\n15D\n0.6654\n\n\n\n16D\n0.6545\n\n\n\n17D\n0.6433\n\n\n\n18D\n0.6319\n\n\n\n\n\n\n\n\n\nlag\nccf\n\n\n\n\n\n19D\n0.6204\n\n\n\n20D\n0.6086\n\n\n\n21D\n0.5966\n\n\n\n22D\n0.5845\n\n\n\n23D\n0.5723\n\n\n\n24D\n0.5598\n\n\n\n25D\n0.5472\n\n\n\n26D\n0.5344\n\n\n\n27D\n0.5215\n\n\n\n28D\n0.5084\n\n\n\n29D\n0.4952\n\n\n\n30D\n0.4819\n\n\n\n31D\n0.4684\n\n\n\n32D\n0.4548\n\n\n\n33D\n0.4411\n\n\n\n34D\n0.4273\n\n\n\n35D\n0.4134\n\n\n\n36D\n0.3994\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do you observe in the acf plots for the two variables?\n\nDoes this fit your understanding of the autocorrelation that will exist in these variables? Why or why not?\n\nWhat do you observe in the ccf?\n\nFor what value of \\(k\\) is the ccf maximized?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCompare the two plots below. The tab on the left shows the relationship between the maximum solar angle on a specific day with the high temperature for that day. The tab on the right provides a scatter plot of the maximum solar angle from 28 days ago and the daily high temperature for the current day.\n\n\nFigure 10: Scatter plots of maximum solar angle and daily high temperatures for Rexburg Idaho showing the difference in the correlation when the data are lagged\n\nNot LaggedLagged",
    "crumbs": [
      "Lesson 1",
      "Leading Variables and Associated Variables"
    ]
  },
  {
    "objectID": "chapter_3_lesson_1.html#summary",
    "href": "chapter_3_lesson_1.html#summary",
    "title": "Leading Variables and Associated Variables",
    "section": "Summary",
    "text": "Summary\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nWorking with your partner, prepare to explain the following concepts to the class:\n\nCross-covariance\nCross-correlation\nCross-correlation function\nWhy do we care about the cross-correlation function? When would it be used?\nHow is the cross-correlation function related to the autocorrelation function?",
    "crumbs": [
      "Lesson 1",
      "Leading Variables and Associated Variables"
    ]
  },
  {
    "objectID": "chapter_3_lesson_1.html#homework-preview-5-min",
    "href": "chapter_3_lesson_1.html#homework-preview-5-min",
    "title": "Leading Variables and Associated Variables",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions",
    "crumbs": [
      "Lesson 1",
      "Leading Variables and Associated Variables"
    ]
  },
  {
    "objectID": "chapter_3_lesson_1.html#homework",
    "href": "chapter_3_lesson_1.html#homework",
    "title": "Leading Variables and Associated Variables",
    "section": "Homework",
    "text": "Homework\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_3_1.qmd \n\n\n\n\n\n\nClass Activity",
    "crumbs": [
      "Lesson 1",
      "Leading Variables and Associated Variables"
    ]
  },
  {
    "objectID": "chapter_3_lesson_1.html#solutions-to-class-activity",
    "href": "chapter_3_lesson_1.html#solutions-to-class-activity",
    "title": "Leading Variables and Associated Variables",
    "section": "Solutions to Class Activity",
    "text": "Solutions to Class Activity\n\nTable 1: Computation of squared deviations\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$y_t$$\n$$x_t - \\bar x$$\n$$(x_t - \\bar x)^2$$\n$$y_t - \\bar y$$\n$$(y_t - \\bar y)^2$$\n\n\n\n\n1\n21\n14\n1\n1\n\n\n\n\n2\n20\n16\n0\n0\n\n\n\n\n3\n17\n18\n-3\n9\n\n\n\n\n4\n15\n17\n-5\n25\n\n\n\n\n5\n18\n12\n-2\n4\n\n\n\n\n6\n21\n10\n1\n1\n\n\n\n\n7\n21\n11\n1\n1\n\n\n\n\n8\n24\n16\n4\n16\n\n\n\n\n9\n22\n14\n2\n4\n\n\n\n\n10\n21\n22\n1\n1\n\n\n\n\nSum\n200\n150\n0\n62\n\n\n\n\n\n\n\n\n\nTable 2: Computation of \\(c_k\\) and \\(r_k\\) for select values of \\(k\\)\n\n\n\n\n\n$$t$$\n$$x_t - \\bar x$$\n$$y_t - \\bar y$$\n$$~k=-4~$$\n$$~k=-3~$$\n$$~k=-2~$$\n$$~k=-1~$$\n$$~k=0~$$\n$$~k=1~$$\n$$~k=2~$$\n$$~k=3~$$\n$$~k=4~$$\n\n\n\n\n1\n1\n-1\n—\n—\n—\n—\n-1\n0\n3\n5\n2\n\n\n2\n0\n1\n—\n—\n—\n1\n0\n-3\n-5\n-2\n1\n\n\n3\n-3\n3\n—\n—\n3\n0\n-9\n-15\n-6\n3\n3\n\n\n4\n-5\n2\n—\n2\n0\n-6\n-10\n-4\n2\n2\n8\n\n\n5\n-2\n-3\n-3\n0\n9\n15\n6\n-3\n-3\n-12\n-6\n\n\n6\n1\n-5\n0\n15\n25\n10\n-5\n-5\n-20\n-10\n-5\n\n\n7\n1\n-4\n12\n20\n8\n-4\n-4\n-16\n-8\n-4\n—\n\n\n8\n4\n1\n-5\n-2\n1\n1\n4\n2\n1\n—\n—\n\n\n9\n2\n-1\n2\n-1\n-1\n-4\n-2\n-1\n—\n—\n—\n\n\n10\n1\n7\n7\n7\n28\n14\n7\n—\n—\n—\n—\n\n\nSum\n0\n0\n13\n41\n73\n27\n-14\n-45\n-36\n-18\n3\n\n\n$$c_k$$\n\n\n1.3\n4.1\n7.3\n2.7\n-1.4\n-4.5\n-3.6\n-1.8\n0.3\n\n\n$$r_k$$\n\n\n0.153\n0.483\n0.861\n0.318\n-0.165\n-0.531\n-0.424\n-0.212\n0.035\n\n\n\n\n\n\n\nFigure 2: Plot of the Sample CCF",
    "crumbs": [
      "Lesson 1",
      "Leading Variables and Associated Variables"
    ]
  },
  {
    "objectID": "chapter_2_lesson_3.html",
    "href": "chapter_2_lesson_3.html",
    "title": "Exploration of Autocorrelation Concepts",
    "section": "",
    "text": "Explain the theoretical implications of autocorrelation for the estimation of time series statistics\n\n\nExplain how positive autocorrelation leads to underestimation of variance in short time series\nExplain how negative autocorrelation can improve efficiency of sample mean estimate\n\n\n\n\nInterpret correlograms to identify significant lags, correlations, trends, and seasonality\n\n\nCreate a correlogram\nInterpret a correlogram\nDefine a sampling distribution\nState the sampling distribution of rk\nExplain the concept of a confidence interval\nConduct a single hypothesis test using a correlogram\nDescribe the problems associated with multiple hypothesis testing in a correlogram\nDifferentiate statistical and practical significance\nDiagnose non-stationarity using a correlogram",
    "crumbs": [
      "Lesson 3",
      "Exploration of Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "chapter_2_lesson_3.html#learning-outcomes",
    "href": "chapter_2_lesson_3.html#learning-outcomes",
    "title": "Exploration of Autocorrelation Concepts",
    "section": "",
    "text": "Explain the theoretical implications of autocorrelation for the estimation of time series statistics\n\n\nExplain how positive autocorrelation leads to underestimation of variance in short time series\nExplain how negative autocorrelation can improve efficiency of sample mean estimate\n\n\n\n\nInterpret correlograms to identify significant lags, correlations, trends, and seasonality\n\n\nCreate a correlogram\nInterpret a correlogram\nDefine a sampling distribution\nState the sampling distribution of rk\nExplain the concept of a confidence interval\nConduct a single hypothesis test using a correlogram\nDescribe the problems associated with multiple hypothesis testing in a correlogram\nDifferentiate statistical and practical significance\nDiagnose non-stationarity using a correlogram",
    "crumbs": [
      "Lesson 3",
      "Exploration of Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "chapter_2_lesson_3.html#preparation",
    "href": "chapter_2_lesson_3.html#preparation",
    "title": "Exploration of Autocorrelation Concepts",
    "section": "Preparation",
    "text": "Preparation\n\nRead Sections 2.3-2.5",
    "crumbs": [
      "Lesson 3",
      "Exploration of Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "chapter_2_lesson_3.html#learning-journal-exchange-10-min",
    "href": "chapter_2_lesson_3.html#learning-journal-exchange-10-min",
    "title": "Exploration of Autocorrelation Concepts",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading your partner’s?\nWhat would you recommend your partner add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 3",
      "Exploration of Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "chapter_2_lesson_3.html#correlograms-10-min",
    "href": "chapter_2_lesson_3.html#correlograms-10-min",
    "title": "Exploration of Autocorrelation Concepts",
    "section": "Correlograms (10 min)",
    "text": "Correlograms (10 min)\nIn the previous lesson, we used the following time series as an example. Here are the values in that time series:\n\n\nx &lt;- c( 4.4, 4.2, 4.2, 4, 4.4, 4.7, 4.9, 5.3, 5.4, 5.5 )\n\n\n\nThe table below gives the sample autocorrelation function, acf, for this data set. You may recognize some of these values from the previous lesson.\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n1\n0.763\n0.448\n0.074\n-0.237\n-0.419\n-0.47\n-0.344\n-0.226\n-0.089\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nUse the acf values to sketch the correlogram for these data in your Learning Journal. The figure below can help you begin.\n\n\n\n\n\n\n\n\n\n\n\n\nAre any of the autocorrelations statistically significant? If so, which one(s)?",
    "crumbs": [
      "Lesson 3",
      "Exploration of Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "chapter_2_lesson_3.html#application-chocolate-search-trends-10-min",
    "href": "chapter_2_lesson_3.html#application-chocolate-search-trends-10-min",
    "title": "Exploration of Autocorrelation Concepts",
    "section": "Application: Chocolate Search Trends (10 min)",
    "text": "Application: Chocolate Search Trends (10 min)\nRecall the Google Trends data for the term “chocolate” from the last lesson. The cleaned data are available in the file chocolate.csv.\n\nImport the chocolate search data and convert to tsibble format\nUse the code below to import the data and convert it into a time series (tsibble) object.\n\n# load packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\"tsibble\", \"fable\",\n               \"feasts\", \"tsibbledata\",\n               \"fable.prophet\", \"tidyverse\",\n               \"patchwork\", \"rio\")\n\n# read in the data from a csv and make the tsibble\n# change the line below to include your file path\nchocolate_month_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/chocolate.csv\") |&gt;\n  mutate(\n    dates = yearmonth(ym(Month)),\n    month = month(dates),\n    year = year(dates),\n    value = chocolate\n  ) |&gt; \n  dplyr::select(dates, month, year, value) |&gt;\n  as_tsibble(index = dates)\n\nchoc_decompose &lt;- chocolate_month_ts |&gt;\n    model(feasts::classical_decomposition(value,\n        type = \"mult\"))  |&gt;\n    components()\n\nautoplot(choc_decompose)\n\n\n\n\n\n\n\n\nHere are the values of the acf for the chocolate search data:\n\nacf(chocolate_month_ts$value, plot=FALSE, type = \"correlation\", lag.max = 25)\n\n\nAutocorrelations of series 'chocolate_month_ts$value', by lag\n\n     0      1      2      3      4      5      6      7      8      9     10 \n 1.000  0.522  0.440  0.159  0.041 -0.018 -0.081 -0.024  0.020  0.121  0.386 \n    11     12     13     14     15     16     17     18     19     20     21 \n 0.425  0.814  0.426  0.357  0.103 -0.001 -0.051 -0.114 -0.057 -0.003  0.104 \n    22     23     24     25 \n 0.358  0.398  0.768  0.389 \n\n\nHere is the associated correlogram:\n\nacf(chocolate_month_ts$value, plot=TRUE, type = \"correlation\", lag.max = 25)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat does the information displayed in this correlogram suggest?\n\n\n\nIf we consider only the random component of this time series, the correlogram is:\n\nacf(choc_decompose$random |&gt; na.omit(), plot=TRUE, type = \"correlation\", lag.max = 25)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do the spikes in the correlogram tell us about this time series?\nIs there evidence of autocorrelation in the data after removing the trend and seasonal variation?\nWhat would happen to the correlogram of the random component if we used an additive decomposition?\n\n\n\n\n\n\n\n\n\nStatistical vs Practical Significance\n\n\n\nAn estimate for the lag k autocorrelation can be statistically significant, but practically insignificant. The percentage of variation in \\(x_t\\) explained by \\(x_{t+k}\\) equals \\(r_k^2\\). In the example above, \\(r_3\\approx r_4\\approx r_5 \\approx 0.2\\), which mean that each of the lagged variables estimates explain around 4% of the variation of the chocolate series at their respective lag values. All the estimates are statistically significant, but we should put a lot of practical weight in their significance.",
    "crumbs": [
      "Lesson 3",
      "Exploration of Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "chapter_2_lesson_3.html#small-group-activity-byu-idaho-on-campus-enrollment-25-min",
    "href": "chapter_2_lesson_3.html#small-group-activity-byu-idaho-on-campus-enrollment-25-min",
    "title": "Exploration of Autocorrelation Concepts",
    "section": "Small Group Activity: BYU-Idaho On-Campus Enrollment (25 min)",
    "text": "Small Group Activity: BYU-Idaho On-Campus Enrollment (25 min)\nThe official number of on-campus BYU-Idaho students each semester is given in the file byui_enrollment.csv.\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nDo the following:\n\nCreate a tsibble with the BYU-Idaho enrollment data. (Hint: There are three semesters in a year, so treat the enrollments as observations taken every four months in January, May, and September.)\nPlot the decomposition of this time series.\nDescribe the trend.\nDescribe the seasonal component.\nIs there evidence of seasonal variation? If so, propose an explanation for the seasonal variation.\nCreate the correlogram for these data.\n\nWhat do you observe?\nDoes the correlogram support the statement you made about the seasonal component?\n\nIs there evidence of autocorrelation in the data after removing the trend and seasonal variation?",
    "crumbs": [
      "Lesson 3",
      "Exploration of Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "chapter_2_lesson_3.html#homework-preview-5-min",
    "href": "chapter_2_lesson_3.html#homework-preview-5-min",
    "title": "Exploration of Autocorrelation Concepts",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions",
    "crumbs": [
      "Lesson 3",
      "Exploration of Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "chapter_2_lesson_3.html#homework",
    "href": "chapter_2_lesson_3.html#homework",
    "title": "Exploration of Autocorrelation Concepts",
    "section": "Homework",
    "text": "Homework\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_2_3.qmd \n\n\nCorrelograms\n\nSolutions to correlogram activity\n\nx &lt;- c( 4.4, 4.2, 4.2, 4, 4.4, 4.7, 4.9, 5.3, 5.4, 5.5 )\nacf(x, plot=FALSE, type = \"correlation\")\n\n\nAutocorrelations of series 'x', by lag\n\n     0      1      2      3      4      5      6      7      8      9 \n 1.000  0.763  0.448  0.074 -0.237 -0.419 -0.470 -0.344 -0.226 -0.089 \n\nacf(x, plot=TRUE, type = \"correlation\")\n\n\n\n\n\n\n\n\n\nBYU-Idaho Enrollment\n\nSolutions to BYU-Idaho Enrollment Activity\n\n# read in the data from a csv and make the tsibble\n\n# Method 1:\nenrollment_df &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/byui_enrollment.csv\")\nstart_date &lt;- lubridate::ymd(\"2019-05-01\")\ndate_seq &lt;- seq(start_date,\n                start_date + months(nrow(enrollment_df)-1) * 4,\n                by = \"4 months\")\nenrollment_ts &lt;- tibble(\n    dates = tsibble::yearmonth(date_seq),\n    semester = pull(enrollment_df, semester),\n    enrollment = pull(enrollment_df, enrollment)\n  ) |&gt;\n  dplyr::select(semester, dates, enrollment) |&gt;\n  as_tsibble(index = dates)\n\n# Method 2:\nenrollment_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/byui_enrollment.csv\") |&gt;\n  mutate(\n    dates = yearmonth(ym(paste(year, term * 4 - 3)))\n  ) |&gt;\n  dplyr::select(semester, dates, enrollment) |&gt;\n  as_tsibble(index = dates) \n\n# Compute and plot the decomposition\nenrollment_decompose &lt;- enrollment_ts |&gt;\n    model(feasts::classical_decomposition(enrollment,\n        type = \"add\"))  |&gt;\n    components()\nautoplot(enrollment_decompose)\n\n\n\n\n\n\n\n\n\nacf(enrollment_decompose$enrollment, type = \"correlation\")\n\n\n\n\n\n\n\n\n\nacf(enrollment_decompose$enrollment, plot=FALSE, type = \"correlation\")\n\n\nAutocorrelations of series 'enrollment_decompose$enrollment', by lag\n\n     0      1      2      3      4      5      6      7      8      9     10 \n 1.000 -0.333 -0.346  0.707 -0.298 -0.312  0.436 -0.278 -0.265  0.349 -0.153 \n    11 \n-0.134 \n\n\n\nacf(enrollment_decompose$enrollment, plot=TRUE, type = \"correlation\")\n\n\n\n\n\n\n\n\n\nacf(enrollment_decompose$random |&gt; na.omit(), plot=TRUE, type = \"correlation\")",
    "crumbs": [
      "Lesson 3",
      "Exploration of Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "chapter_2_lesson_2.html",
    "href": "chapter_2_lesson_2.html",
    "title": "Autocorrelation Concepts",
    "section": "",
    "text": "Define key terms in time series analysis\n\n\nDefine the ensemble of a time series\nDefine the expected value (or mean function) of a time series model\nDefine the sample estimate of the population mean of a time series\nDefine the variance function of a time series model\nState the constant variance estimator for a time series model\nExplain the stationarity assumption\nExplain the stationary variance assumption\nDefine lag\nDefine autocorrelation\nDefine the second-order stationary time series\nExplain the autocovariance function in Equation (2.11)\nExplain the lag k autocorrelation function in Equation (2.12)\nDefine the autocovariance function, acvf\nDefine the sample autocorrelation function, acf\n\n\n\n\nCalculate sample estimates of autocovariance and autocorrelation functions from time series data\n\n\nDefine the sample autocovariance function, c_k\nDefine the sample autocorrelation function, r_k",
    "crumbs": [
      "Lesson 2",
      "Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "chapter_2_lesson_2.html#learning-outcomes",
    "href": "chapter_2_lesson_2.html#learning-outcomes",
    "title": "Autocorrelation Concepts",
    "section": "",
    "text": "Define key terms in time series analysis\n\n\nDefine the ensemble of a time series\nDefine the expected value (or mean function) of a time series model\nDefine the sample estimate of the population mean of a time series\nDefine the variance function of a time series model\nState the constant variance estimator for a time series model\nExplain the stationarity assumption\nExplain the stationary variance assumption\nDefine lag\nDefine autocorrelation\nDefine the second-order stationary time series\nExplain the autocovariance function in Equation (2.11)\nExplain the lag k autocorrelation function in Equation (2.12)\nDefine the autocovariance function, acvf\nDefine the sample autocorrelation function, acf\n\n\n\n\nCalculate sample estimates of autocovariance and autocorrelation functions from time series data\n\n\nDefine the sample autocovariance function, c_k\nDefine the sample autocorrelation function, r_k",
    "crumbs": [
      "Lesson 2",
      "Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "chapter_2_lesson_2.html#preparation",
    "href": "chapter_2_lesson_2.html#preparation",
    "title": "Autocorrelation Concepts",
    "section": "Preparation",
    "text": "Preparation\n\nRead Sections 2.2.5",
    "crumbs": [
      "Lesson 2",
      "Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "chapter_2_lesson_2.html#learning-journal-exchange-10-min",
    "href": "chapter_2_lesson_2.html#learning-journal-exchange-10-min",
    "title": "Autocorrelation Concepts",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading your partner’s?\nWhat would you recommend your partner add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 2",
      "Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "chapter_2_lesson_2.html#hands-on-exercise-exploring-sample-autocorrelation-40-min",
    "href": "chapter_2_lesson_2.html#hands-on-exercise-exploring-sample-autocorrelation-40-min",
    "title": "Autocorrelation Concepts",
    "section": "Hands-on Exercise – Exploring Sample Autocorrelation (40 min)",
    "text": "Hands-on Exercise – Exploring Sample Autocorrelation (40 min)\n\n\nComparison of Independent and Autocorrelated Error Terms\nIn the previous lesson, we computed the sample covariance and sample correlation coefficient between two independent variables. When working with time series, the observations are not independent. There is often a relationship between sequential observations. We will compute the autocovariance function and autocorrelation function for a time series. Note: the prefix “auto” comes from a Greek root meaning “self.”\nThe figure below illustrates the difference between a series of data, where the residuals are independent compared to a series with autocorrelated data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nThe variances of the residuals for these two series are approximately equal. What characteristics distinguish the two series in the figure above?\n\n\n\n\n\nAutocovariance and Autocorrelation\nWe will use the following data to explore the concepts of autovariance and autocorrelation.\n\n\n\n\n\nt\n$$ x_t $$\n\n\n\n\n1\n4.4\n\n\n2\n4.2\n\n\n3\n4.2\n\n\n4\n4.0\n\n\n5\n4.4\n\n\n6\n4.7\n\n\n7\n4.9\n\n\n8\n5.3\n\n\n9\n5.4\n\n\n10\n5.5\n\n\n\n\n\n\n\nYou can use this R command to read in the observations.\n\n\nx &lt;- c( 4.4, 4.2, 4.2, 4, 4.4, 4.7, 4.9, 5.3, 5.4, 5.5 )\n\n\n\n\n\n\n\n\n\n\n\nWe will use the sample mean of these data repeatedly. The value of \\(\\bar x\\) is:\n\\[\n  \\bar x\n    = \\frac{1}{n} \\sum\\limits_{t=1}^{n} x_t\n    = \\frac{1}{10} \\cdot 47\n    = 4.7\n\\]\nWe will be finding the autocovariance and correlation of a time series with itself. First, we start with a lag of 1. With a lag of 1 the corresponding values of the time series that are being compared are shifted by one time unit. Then, we will consider any integer lag: lag \\(k\\).\n\n\nLag \\(k\\) Sample Autocovariance Function (acvf), \\(c_k\\)\nThe lag \\(k\\) sample autocovariance function, acvf, denoted \\(c_k\\), is defined as\n\\[\n  c_k = \\frac{1}{n} \\sum\\limits_{t=1}^{n-k}(x_t-\\bar x)(x_{t+k}-\\bar x)\n\\]\nWe denote the lag by the letter \\(k\\), where \\(k \\ge 0\\). This is the number of values the data set is shifted to compute the autocovariance.\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nExplain the equation for \\(c_k\\) to your partner.\nWhat is the equation for \\(c_0\\), the value of the autocovariance function with lag \\(k=0\\)?\n\nThis expression is very similar to a definition we have encountered previously. What is it?\n\n\n\n\n\nLag \\(k=1\\) Sample Autocovariance Function, \\(c_1\\)\nWe will now find the autocovariance between the values in a time series (\\(x = x_t\\)) and the same values, shifted by one unit of time (\\(y = x_{t+1}\\)).\n\n\n\n\n\nt\n$$ x_t $$\n$$ x_{t+1} $$\n$$ x_t-\\bar x $$\n$$ (x_t-\\bar x)^2 $$\n$$ x_{t+1}-\\bar x$$\n$$ (x-\\bar x)(x_{t+1}-\\bar x) $$\n\n\n\n\n1\n4.4\n4.2\n-0.3\n0.09\n-0.5\n0.15\n\n\n2\n4.2\n4.2\n-0.5\n0.25\n-0.5\n0.25\n\n\n3\n4.2\n4\n-0.5\n0.25\n-0.7\n0.35\n\n\n4\n4\n4.4\n-0.7\n0.49\n-0.3\n0.21\n\n\n5\n4.4\n4.7\n-0.3\n0.09\n0\n0\n\n\n6\n4.7\n4.9\n0\n0\n0.2\n0\n\n\n7\n4.9\n5.3\n0.2\n0.04\n0.6\n0.12\n\n\n8\n5.3\n5.4\n0.6\n0.36\n0.7\n0.42\n\n\n9\n5.4\n5.5\n0.7\n0.49\n0.8\n0.56\n\n\n10\n5.5\n—\n0.8\n0.64\n—\n—\n\n\nsum\n47\n42.6\n0\n2.7\n0.3\n2.06\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWorking with your assigned partner, compute each of the values in row 1 by hand. Recall that \\(\\bar x = 4.7\\).\nWith your partner, add up the values in the last column to verify that the sum is 2.06.\n\n\n\nThe scatterplot below illustrates the relationship between the observed data (\\(x_t\\)) and the next observation (\\(x_{t+1}\\)).\n\n\n\n\n\n\n\n\n\nIn this example, the second variable is \\(x_{t+1}\\), where \\(t&gt;1\\). the autocovariance of \\(x_t\\) and \\(x_{t+1}\\) is:\n\\[\n  c_1\n    = \\frac{1}{n} \\sum\\limits_{t=1}^{n-1}(x_t-\\bar x)(x_{t+1}-\\bar x)\n    = \\frac{1}{10} \\sum\\limits_{t=1}^{9}(x_t-\\bar x)(x_{t+1}-\\bar x)\n  = \\frac{1}{10} \\cdot 2.06\n  = 0.206\n\\]\nThis is the (auto)covariance of \\(x\\) with itself, but with a lag of 1 time unit. This is the value of the lag \\(k=1\\) autocovariance function, acvf_1.\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat does the lag 1 autocovariance measure?\n\n\n\n\n\n\nLag \\(k\\) Sample Autocorrelation Function (acf), \\(r_k\\)\nThe sample autocorrelation function, acf, denoted \\(r_k\\), where \\(k\\) is the lag, is defined as\n\\[\n  r_k\n    = \\frac{c_k}{c_0}\n    = \\frac{ \\frac{1}{n} \\sum\\limits_{t=1}^{n-k}(x_t-\\bar x)(x_{t+k}-\\bar x) }{ \\frac{1}{n} \\sum\\limits_{t=1}^{n}(x_t-\\bar x)^2 }\n    = \\frac{ \\sum\\limits_{t=1}^{n-k}(x_t-\\bar x)(x_{t+k}-\\bar x) }{ \\sum\\limits_{t=1}^{n}(x_t-\\bar x)^2 }\n\\]\nNote that \\(c_0\\) is the variance of \\(x\\), but computed by dividing by \\(n\\), instead of \\(n-1\\).\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nInterpret the components of the numerator and the denominator of the expression for \\(r_k\\) to your partner.\n\n\n\n\nLag \\(k=1\\) Sample Autocorrelation Function, \\(r_1\\)\nWe can compute the lag 1 autocorrelation or the autocorrelation of \\(x\\) with lag 1 as the quotient \\(r_1 = \\frac{c_1}{c_0}\\). We have already determined that \\(c_1 = 0.206\\). We now compute \\(c_0\\):\n\\[\n  c_0 = \\frac{1}{n} \\sum\\limits_{t=1}^{n-0} (x_t-\\bar x)(x_{t+0}-\\bar x)\n    = \\frac{1}{n} \\sum\\limits_{t=1}^{n} (x_t-\\bar x)^2\n    = \\frac{1}{10} \\cdot 2.7\n    = 0.27\n\\]\nWe use \\(c_0\\) and \\(c_1\\) to compute \\(r_1\\). Here are two ways we can compute this value:\n\\[\\begin{align*}\n  r_1\n    &= \\frac{c_1}{c_0}\n    =\n    \\frac{ \\frac{1}{n} \\sum\\limits_{t=1}^{9}(x_t-\\bar x)(x_{t+1}-\\bar x)\n        }{\n            \\frac{1}{n} \\sum\\limits_{t=1}^{10}(x_t-\\bar x)^2\n        }  \n    =\n    \\frac{\n        \\frac{1}{10} \\cdot 2.06\n      }{\n        \\frac{1}{10} \\cdot 2.7\n      }\n    = \\frac{0.206}{0.27}\n    = 0.763\n    \\\\\n    &=\n    \\frac{ \\sum\\limits_{t=1}^{9}(x_t-\\bar x)(x_{t+1}-\\bar x)\n        }{\n           \\sum\\limits_{t=1}^{10}(x_t-\\bar x)^2\n        }  \n    = \\frac{2.06}{2.7}\n    = 0.763\n    \n\\end{align*}\\]\n\nWhat does the lag 1 autocorrelation, \\(c_1\\), measure?\n\n\n\nLag \\(k = 2\\)\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWorking with your assigned partner, fill in the blanks in the following table. Use the results to compute \\(c_2\\) and \\(r_2\\).\n\n Tables-Handout-Excel \n\n\n\n\n\n\n\nt\n$$ x_t $$\n$$ x_{t+k} $$\n$$ x_t-\\bar x $$\n$$ (x_t-\\bar x)^2 $$\n$$ x_{t+k}-\\bar x$$\n$$ (x-\\bar x)(x_{t+k}-\\bar x) $$\n\n\n\n\n1\n4.4\n4.2\n-0.3\n0.09\n-0.5\n0.15\n\n\n2\n4.2\n4\n-0.5\n0.25\n-0.7\n0.35\n\n\n3\n4.2\n4.4\n-0.5\n0.25\n-0.3\n0.15\n\n\n4\n4\n\n\n\n\n\n\n\n5\n4.4\n\n\n\n\n\n\n\n6\n4.7\n\n\n\n\n\n\n\n7\n4.9\n5.4\n0.2\n0.04\n0.7\n0.14\n\n\n8\n5.3\n5.5\n0.6\n0.36\n0.8\n0.48\n\n\n9\n5.4\n—\n0.7\n0.49\n—\n—\n\n\n10\n5.5\n—\n0.8\n0.64\n—\n—\n\n\nsum\n47\n\n\n\n\n\n\n\n\n\n\n\n\nThe figure below illustrates the relationship between \\(x_t\\) and \\(x_{t+2}\\).\n\n\n\n\n\n\n\n\n\n\n\nLag \\(k = 3\\)\n\n\n\n\n\nt\n$$ x_t $$\n$$ x_{t+k} $$\n$$ x_t-\\bar x $$\n$$ (x_t-\\bar x)^2 $$\n$$ x_{t+k}-\\bar x$$\n$$ (x-\\bar x)(x_{t+k}-\\bar x) $$\n\n\n\n\n1\n4.4\n4\n-0.3\n0.09\n-0.7\n0.21\n\n\n2\n4.2\n4.4\n-0.5\n0.25\n-0.3\n0.15\n\n\n3\n4.2\n4.7\n-0.5\n0.25\n0\n0\n\n\n4\n4\n4.9\n-0.7\n0.49\n0.2\n-0.14\n\n\n5\n4.4\n5.3\n-0.3\n0.09\n0.6\n-0.18\n\n\n6\n4.7\n5.4\n0\n0\n0.7\n0\n\n\n7\n4.9\n5.5\n0.2\n0.04\n0.8\n0.16\n\n\n8\n5.3\n—\n0.6\n0.36\n—\n—\n\n\n9\n5.4\n—\n0.7\n0.49\n—\n—\n\n\n10\n5.5\n—\n0.8\n0.64\n—\n—\n\n\nsum\n47\n34.2\n0\n2.7\n1.3\n0.2\n\n\n\n\n\n\n\nThe figure below illustrates the correlations between \\(x_t\\) and \\(x_{t+3}\\). Note that \\(c_3 = \\dfrac{0.2}{10} = 0.02\\) and \\(r_3 = \\dfrac{0.02}{0.27} = 0.0741\\).\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nDoes the value of \\(r_3 = 0.0741\\) seem reasonable, given the pattern in this plot?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLag \\(k = 4\\)\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nCompute \\(c_4\\) and \\(r_4\\) using R (but not automated functions), Excel, or hand calculations.\n\n\n\n\n\n\n\n\nt\n$$ x_t $$\n$$ x_{t+k} $$\n$$ x_t-\\bar x $$\n$$ (x_t-\\bar x)^2 $$\n$$ x_{t+k}-\\bar x$$\n$$ (x-\\bar x)(x_{t+k}-\\bar x) $$\n\n\n\n\n1\n4.4\n\n\n\n\n\n\n\n2\n4.2\n\n\n\n\n\n\n\n3\n4.2\n\n\n\n\n\n\n\n4\n4\n\n\n\n\n\n\n\n5\n4.4\n\n\n\n\n\n\n\n6\n4.7\n\n\n\n\n\n\n\n7\n4.9\n\n\n\n\n\n\n\n8\n5.3\n\n\n\n\n\n\n\n9\n5.4\n\n\n\n\n\n\n\n10\n5.5\n\n\n\n\n\n\n\nsum\n47\n\n\n\n\n\n\n\n\n\n\n\n\nThe figure below illustrates the correlations between \\(x_t\\) and \\(x_{t+4}\\).\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nDoes the value of \\(r_4\\) you computed seem reasonable, given the pattern in this plot?",
    "crumbs": [
      "Lesson 2",
      "Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "chapter_2_lesson_2.html#class-activity-using-r-to-compute-the-acvf-and-acf-5-min",
    "href": "chapter_2_lesson_2.html#class-activity-using-r-to-compute-the-acvf-and-acf-5-min",
    "title": "Autocorrelation Concepts",
    "section": "Class Activity: Using R to compute the acvf and acf (5 min)",
    "text": "Class Activity: Using R to compute the acvf and acf (5 min)\nWe will continue to use the following sample data.\n\n\n x &lt;- c( 4.4, 4.2, 4.2, 4, 4.4, 4.7, 4.9, 5.3, 5.4, 5.5 ) \n df &lt;- data.frame(x = x)\n\n\n\nacvf\nThis code gives the values of the acvf.\n\nacf(df$x, plot=FALSE, type = \"covariance\")\n\n\nAutocovariances of series 'df$x', by lag\n\n     0      1      2      3      4      5      6      7      8      9 \n 0.270  0.206  0.121  0.020 -0.064 -0.113 -0.127 -0.093 -0.061 -0.024 \n\n\n\n\nacf\nWe can obtain the acf by changing the argument for the paramter type to \"correlation\".\n\nacf(df$x, plot=FALSE, type = \"correlation\")\n\n\nAutocorrelations of series 'df$x', by lag\n\n     0      1      2      3      4      5      6      7      8      9 \n 1.000  0.763  0.448  0.074 -0.237 -0.419 -0.470 -0.344 -0.226 -0.089",
    "crumbs": [
      "Lesson 2",
      "Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "chapter_2_lesson_2.html#homework-preview-5-min",
    "href": "chapter_2_lesson_2.html#homework-preview-5-min",
    "title": "Autocorrelation Concepts",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions",
    "crumbs": [
      "Lesson 2",
      "Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "chapter_2_lesson_2.html#homework",
    "href": "chapter_2_lesson_2.html#homework",
    "title": "Autocorrelation Concepts",
    "section": "Homework",
    "text": "Homework\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_2_2.qmd \n\n\nClass Activity: k=2\n\n Tables-Handout-Excel-key \nSolutions to Class Activity: \\(k=2\\)\n\n\n\n\n\nt\nx_t\nx_{t+k}\nx_t-mean(x)\n(x_t-mean(x))^2\nx_{t+k}-mean(x)\n(x-mean(x))(x_{t+k}-mean(x))\n\n\n\n\n1\n4.4\n4.2\n-0.3\n0.09\n-0.5\n0.15\n\n\n2\n4.2\n4\n-0.5\n0.25\n-0.7\n0.35\n\n\n3\n4.2\n4.4\n-0.5\n0.25\n-0.3\n0.15\n\n\n4\n4\n4.7\n-0.7\n0.49\n0\n0\n\n\n5\n4.4\n4.9\n-0.3\n0.09\n0.2\n-0.06\n\n\n6\n4.7\n5.3\n0\n0\n0.6\n0\n\n\n7\n4.9\n5.4\n0.2\n0.04\n0.7\n0.14\n\n\n8\n5.3\n5.5\n0.6\n0.36\n0.8\n0.48\n\n\n9\n5.4\n—\n0.7\n0.49\n—\n—\n\n\n10\n5.5\n—\n0.8\n0.64\n—\n—\n\n\nsum\n47\n38.4\n0\n2.7\n0.8\n1.21\n\n\n\n\n\n\n\n\\[\\begin{align*}\n  c_2\n    &= \\frac{1}{n} \\sum\\limits_{t=1}^{n-1}(x_t-\\bar x)(x_{t+2}-\\bar x)\n    = \\frac{1}{10} \\cdot 1.21\n    = 0.121 \\\\\n  r_2\n    &= \\frac{c_2}{c_0}\n    = \\frac{0.121}{0.27}\n    = 0.448\n\\end{align*}\\]\n\nClass Activity: k=4\n\nSolutions to Class Activity: \\(k=4\\)\n\n\n\n\n\nt\nx_t\nx_{t+k}\nx_t-mean(x)\n(x_t-mean(x))^2\nx_{t+k}-mean(x)\n(x-mean(x))(x_{t+k}-mean(x))\n\n\n\n\n1\n4.4\n4.4\n-0.3\n0.09\n-0.3\n0.09\n\n\n2\n4.2\n4.7\n-0.5\n0.25\n0\n0\n\n\n3\n4.2\n4.9\n-0.5\n0.25\n0.2\n-0.1\n\n\n4\n4\n5.3\n-0.7\n0.49\n0.6\n-0.42\n\n\n5\n4.4\n5.4\n-0.3\n0.09\n0.7\n-0.21\n\n\n6\n4.7\n5.5\n0\n0\n0.8\n0\n\n\n7\n4.9\n—\n0.2\n0.04\n—\n—\n\n\n8\n5.3\n—\n0.6\n0.36\n—\n—\n\n\n9\n5.4\n—\n0.7\n0.49\n—\n—\n\n\n10\n5.5\n—\n0.8\n0.64\n—\n—\n\n\nsum\n47\n30.2\n0\n2.7\n2\n-0.64\n\n\n\n\n\n\n\n\\[\\begin{align*}\n  c_4\n    &= \\frac{1}{n} \\sum\\limits_{t=1}^{n-1}(x_t-\\bar x)(x_{t+4}-\\bar x)\n    = \\frac{1}{10} \\cdot -0.64\n    = -0.064 \\\\\n  r_4\n    &= \\frac{c_4}{c_0} = \\frac{-0.064}{0.27}\n    = -0.237\n\\end{align*}\\]",
    "crumbs": [
      "Lesson 2",
      "Autocorrelation Concepts"
    ]
  },
  {
    "objectID": "chapter_2_lesson_1.html",
    "href": "chapter_2_lesson_1.html",
    "title": "Covariance and Correlation",
    "section": "",
    "text": "Compute the key statistics used to describe the linear relationship between two variables\n\n\nCompute the sample mean\nCompute the sample variance\nCompute the sample standard deviation\nCompute the sample covariance\nCompute the sample correlation coefficient\nExplain sample covariance using a scatter plot\n\n\n\n\nInterpret the key statistics used to describe sample data\n\n\nInterpret the sample mean\nInterpret the sample variance\nInterpret the sample standard deviation\nInterpret the sample covariance\nInterpret the sample correlation coefficient",
    "crumbs": [
      "Lesson 1",
      "Covariance and Correlation"
    ]
  },
  {
    "objectID": "chapter_2_lesson_1.html#learning-outcomes",
    "href": "chapter_2_lesson_1.html#learning-outcomes",
    "title": "Covariance and Correlation",
    "section": "",
    "text": "Compute the key statistics used to describe the linear relationship between two variables\n\n\nCompute the sample mean\nCompute the sample variance\nCompute the sample standard deviation\nCompute the sample covariance\nCompute the sample correlation coefficient\nExplain sample covariance using a scatter plot\n\n\n\n\nInterpret the key statistics used to describe sample data\n\n\nInterpret the sample mean\nInterpret the sample variance\nInterpret the sample standard deviation\nInterpret the sample covariance\nInterpret the sample correlation coefficient",
    "crumbs": [
      "Lesson 1",
      "Covariance and Correlation"
    ]
  },
  {
    "objectID": "chapter_2_lesson_1.html#preparation",
    "href": "chapter_2_lesson_1.html#preparation",
    "title": "Covariance and Correlation",
    "section": "Preparation",
    "text": "Preparation\n\nRead Sections 2.1-2.2.2 and 2.2.4",
    "crumbs": [
      "Lesson 1",
      "Covariance and Correlation"
    ]
  },
  {
    "objectID": "chapter_2_lesson_1.html#learning-journal-exchange-10-min",
    "href": "chapter_2_lesson_1.html#learning-journal-exchange-10-min",
    "title": "Covariance and Correlation",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading your partner’s?\nWhat would you recommend your partner add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 1",
      "Covariance and Correlation"
    ]
  },
  {
    "objectID": "chapter_2_lesson_1.html#class-activity-variance-and-standard-deviation-10-min",
    "href": "chapter_2_lesson_1.html#class-activity-variance-and-standard-deviation-10-min",
    "title": "Covariance and Correlation",
    "section": "Class Activity: Variance and Standard Deviation (10 min)",
    "text": "Class Activity: Variance and Standard Deviation (10 min)\nWe will explore the variance and standard deviation in this section.\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do the standard deviation and the variance measure?\n\n\n\nThe following code simulates observations of a random variable. We will use these data to explore the variance and standard deviation.\n\n# Set random seed\nset.seed(2412)\n\n# Specify means and standard deviation\nn &lt;- 5        # number of points\nmu &lt;- 10      # mean\nsigma &lt;- 3    # standard deviation\n\n# Simulate normal data\nsim_data &lt;- data.frame(x = round(rnorm(n, mu, sigma), 1)) |&gt; \n  arrange(x)\n\nThe data simulated by this process are:\n\n6.9, 7.7, 8.1, 10.8, 13.5\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nFind the sample mean of these numbers. \nWhat are some ways to interpret the mean?\n\n\n\nThe variance and standard deviation are individual numbers that summarize how far the data are from the mean. We first compute the deviations from the mean, \\(x - \\bar x\\). This is the directed distance from the mean to each data point.\n\n\n\n\n\n\n\n\n\nWe can summarize this information in a table:\n\nTable 1: Deviations from the mean\n\n\n\n\n\n$$x_t$$\n$$x_t-\\bar x$$\n\n\n\n\n\n\n\n\n6.9\n-2.5\n\n\n\n\n\n\n7.7\n-1.7\n\n\n\n\n\n\n8.1\n-1.3\n\n\n\n\n\n\n10.8\n1.4\n\n\n\n\n\n\n13.5\n4.1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nHow can we obtain one number that summarizes how spread out the data are from the mean? We may try averaging the deviations from the mean.\n\nWhat is the average deviation from the mean?\nWill we get the same value with other data sets, or is this just a coincidence?\nWhat could you do to prevent this from happening?\nApply your idea. Compute the resulting value that summarizes the spread. What do you get?\nWhat is the relationship between the sample variance and the sample standard deviation?\nUse a table like the one above to verify that the sample variance is 7.4.\nShow that the sample standard deviation is 2.7203.",
    "crumbs": [
      "Lesson 1",
      "Covariance and Correlation"
    ]
  },
  {
    "objectID": "chapter_2_lesson_1.html#class-activity-covariance-and-correlation-15-min",
    "href": "chapter_2_lesson_1.html#class-activity-covariance-and-correlation-15-min",
    "title": "Covariance and Correlation",
    "section": "Class Activity: Covariance and Correlation (15 min)",
    "text": "Class Activity: Covariance and Correlation (15 min)\n \n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do you get if you multiply the equations for \\(r\\), \\(s_x\\), and \\(s_y\\) together?\n\n\n\n\\[\n  r \\cdot s_x \\cdot s_y\n    =\n      \\frac{\\sum\\limits_{t=1}^n (x - \\bar x)(y - \\bar y)}{\\sqrt{\\sum\\limits_{t=1}^n (x - \\bar x)^2} \\sqrt{\\sum\\limits_{t=1}^n (y - \\bar y)^2}}\n      \\cdot\n      \\sqrt{ \\frac{\\sum\\limits_{t=1}^n (x - \\bar x)^2}{n-1} }\n      \\cdot\n      \\sqrt{ \\frac{\\sum\\limits_{t=1}^n (y - \\bar y)^2}{n-1} }\n    =\n      ?\n\\]\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nUse the numerical values above to confirm your result. Any discrepancy is due to roundoff error.",
    "crumbs": [
      "Lesson 1",
      "Covariance and Correlation"
    ]
  },
  {
    "objectID": "chapter_2_lesson_1.html#team-activity-computational-practice-15-min",
    "href": "chapter_2_lesson_1.html#team-activity-computational-practice-15-min",
    "title": "Covariance and Correlation",
    "section": "Team Activity: Computational Practice (15 min)",
    "text": "Team Activity: Computational Practice (15 min)\n\nTable 3: Computational Practice\n\n\n\n\n\n\nDownload Excel Handout\n\n\n\n Tables-Handout-Excel \n\n\nThe table below contains values of two time series \\(\\{x_t\\}\\) and \\(\\{y_t\\}\\) observed at times \\(t = 1, 2, \\ldots, 6\\). We will use these values to practice finding the means, standard deviations, correlation coefficient, and covariance without using built-in R functions.\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$y_t$$\n$$x_t-\\bar x$$\n$$(x_t - \\bar x)^2$$\n$$y_t-\\bar y$$\n$$(y_t-\\bar y)^2$$\n$$(x_t - \\bar x)(y_t-\\bar y)$$\n\n\n\n\n1\n-2.1\n2.8\n-1.9\n3.61\n1\n1\n-1.9\n\n\n2\n-0.2\n2.2\n\n\n\n\n\n\n\n3\n0.8\n0.9\n\n\n\n\n\n\n\n4\n0.4\n2\n\n\n\n\n\n\n\n5\n2.3\n-1\n\n\n\n\n\n\n\n6\n-2.4\n3.9\n\n\n\n\n\n\n\nsum\n-1.2\n10.8\n\n\n\n\n\n\n\n$$~$$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the table above to determine these values:\n\n\n\n\\(\\bar x =\\)\n\\(\\bar y =\\)\n\n\n\n\n\n\\(s_x =\\)\n\\(s_y =\\)\n\n\n\n\n\n\\(r =\\)\n\\(\\\\cov(x,y) =\\)\n\n\n\nHere is a scatterplot of the data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nWorking with your partner, prepare to explain the following concepts to the class:\n\nVariance\nStandard deviation\nCorrelation\nCovariance",
    "crumbs": [
      "Lesson 1",
      "Covariance and Correlation"
    ]
  },
  {
    "objectID": "chapter_2_lesson_1.html#computations-in-r-5-min",
    "href": "chapter_2_lesson_1.html#computations-in-r-5-min",
    "title": "Covariance and Correlation",
    "section": "Computations in R (5 min)",
    "text": "Computations in R (5 min)\nUse these commands to load the data from the previous activity into R.\n\n\nx &lt;- c( -2.1, -0.2, 0.8, 0.4, 2.3, -2.4 )\n\n\ny &lt;- c( 2.8, 2.2, 0.9, 2, -1, 3.9 )\n\n\nWe can use R to compute the mean, variance, standard deviation, correlation coefficient, and covariance.\n\nMean, \\(\\bar x\\)\n\nmean(x)\n\n[1] -0.2\n\n\n\n\nVariance, \\(s_x^2\\)\n\nvar(x)\n\n[1] 3.212\n\n\n\n\nStandard Deviation, \\(s_x\\)\n\nsd(x)\n\n[1] 1.792205\n\n\n\n\nCorrelation Coefficient, \\(r\\)\n\ncor(x, y)\n\n[1] -0.9449384\n\n\n\n\nCovariance, \\(\\\\cov(x,y)\\)\n\ncov(x, y)\n\n[1] -2.86",
    "crumbs": [
      "Lesson 1",
      "Covariance and Correlation"
    ]
  },
  {
    "objectID": "chapter_2_lesson_1.html#homework-preview-5-min",
    "href": "chapter_2_lesson_1.html#homework-preview-5-min",
    "title": "Covariance and Correlation",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions",
    "crumbs": [
      "Lesson 1",
      "Covariance and Correlation"
    ]
  },
  {
    "objectID": "chapter_2_lesson_1.html#homework",
    "href": "chapter_2_lesson_1.html#homework",
    "title": "Covariance and Correlation",
    "section": "Homework",
    "text": "Homework\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_2_1.qmd \n\n\nClass Activity: Variance and Standard Deviation\n\n Tables-Handout-Excel-key \nSolutions to Class Activity: Variance and Standard Deviation\n\n\n\n\n\nSolution\n$$x_t$$\n$$x_t-\\bar x$$\n$$(x_t-\\bar x)^2$$\n\n\n\n\n\n6.9\n-2.5\n6.25\n\n\n\n7.7\n-1.7\n2.89\n\n\n\n8.1\n-1.3\n1.69\n\n\n\n10.8\n1.4\n1.96\n\n\n\n13.5\n4.1\n16.81\n\n\nSum\n47.0\n0.0\n29.60\n\n\n\n\n\n\n\nThe variance of these values is \\(s^2 = \\frac{29.6}{5 - 1} = 3.212\\).\nThe standard deviation is \\(s = \\sqrt{s^2} = \\sqrt{3.212} = 1.792\\).\n\nTeam Activity: Computational Practice\n\nSolutions to Team Activity: Computational Practice\n\nTable 3: Computational Practice\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$y_t$$\n$$x_t-\\bar x$$\n$$(x_t - \\bar x)^2$$\n$$y_t-\\bar y$$\n$$(y_t-\\bar y)^2$$\n$$(x_t - \\bar x)(y_t-\\bar y)$$\n\n\n\n\n1\n-2.1\n2.8\n-1.9\n3.61\n1\n1\n-1.9\n\n\n2\n-0.2\n2.2\n0\n0\n0.4\n0.16\n0\n\n\n3\n0.8\n0.9\n1\n1\n-0.9\n0.81\n-0.9\n\n\n4\n0.4\n2\n0.6\n0.36\n0.2\n0.04\n0.12\n\n\n5\n2.3\n-1\n2.5\n6.25\n-2.8\n7.84\n-7\n\n\n6\n-2.4\n3.9\n-2.2\n4.84\n2.1\n4.41\n-4.62\n\n\nsum\n-1.2\n10.8\n0\n16.06\n0\n14.26\n-14.3\n\n\n\n\n\n\n\n\n\n\n\\(\\bar x = -0.2\\)\n\\(\\bar y = 1.8\\)\n\n\n\n\n\n\\(s_x = 1.7922053\\)\n\\(s_y = 1.6887865\\)\n\n\n\n\n\n\\(r = -0.9449384\\)\n\\(\\\\cov(x,y) = -2.86\\)",
    "crumbs": [
      "Lesson 1",
      "Covariance and Correlation"
    ]
  },
  {
    "objectID": "chapter_1_lesson_5_handout.html",
    "href": "chapter_1_lesson_5_handout.html",
    "title": "Chapter 1 Lesson 5: In-Class Worksheet",
    "section": "",
    "text": "Start by finding the values of \\(\\hat s_t\\) in Table 5. Use Table 4 as needed to complete Table 5.\n\n\n\n\nTable 5: Compute \\(\\hat s_t\\); then use Table 4 to find \\(\\bar s_t\\). Use \\(\\bar s_t\\) to find the random component and the seasonally adjusted time series values.\n\n\n\n\n\nQuarter\nRevenue $$x_t$$\n$$ \\hat m_t $$\n$$ \\hat s_t $$\n$$ \\bar s_t $$\nRandom\nSeasonally Adjusted $$x_t$$\n\n\n\n\n2005 Q1\n1.24\nNA\n\n\n\n\n\n\n2005 Q2\n0.48\nNA\n\n\n\n\n\n\n2005 Q3\n1.24\n1.315\n\n\n\n\n\n\n2005 Q4\n1.71\n\n\n\n\n\n\n\n2006 Q1\n2.42\n1.829\n\n\n\n\n\n\n2006 Q2\n1.71\n\n\n\n\n\n\n\n2006 Q3\n1.71\n2.251\n\n\n\n\n\n\n2006 Q4\n2.19\n\n0.808\n0.905\n0.893\n2.42\n\n\n2007 Q1\n4.37\n3.136\n1.393\n1.314\n1.06\n3.325\n\n\n2007 Q2\n3.42\n3.469\n0.986\n0.947\n1.041\n3.612\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n\n\n\n\n\n               \n\n\nTable 4: Table of \\(\\hat s_t\\) values, quarterly means of \\(\\hat s_t\\), and seasonally adjusted factor \\(\\bar s_t\\).\n\n\n\n\n\nYear\nQ1\nQ2\nQ3\nQ4\n\n\n\n\n2005\nNA\nNA\n\n\n\n\n2006\n\n\n\n0.808\n\n\n2007\n1.393\n0.986\n0.864\n0.702\n\n\n2008\n1.525\n0.785\n0.683\n1.325\n\n\n2009\n1.182\n0.781\n0.834\n1.005\n\n\n2010\n1.193\n0.912\n0.849\n0.963\n\n\n2011\n1.106\n0.961\n0.999\n0.734\n\n\n2012\n1.328\n1.141\n0.865\n0.828\n\n\n2013\n1.249\n1.019\n0.819\n0.856\n\n\n2014\n1.301\n1.012\n0.783\n0.818\n\n\n2015\n1.367\n1.013\n0.847\n0.891\n\n\n2016\n1.355\n0.928\n0.781\n0.855\n\n\n2017\n1.412\n0.935\n0.776\n0.864\n\n\n2018\n1.405\n0.939\n0.808\n0.968\n\n\n2019\n1.303\n0.894\n0.816\n0.956\n\n\n2020\n1.356\n0.851\n0.84\n0.835\n\n\n2021\n1.326\n1.005\n0.875\n0.872\n\n\n2022\n1.282\n0.995\n0.849\n0.933\n\n\n2023\n1.219\n0.989\nNA\nNA\n\n\nMean\n\n\n\n\n\n\n$$ \\bar s_t $$"
  },
  {
    "objectID": "chapter_1_lesson_4_handout.html",
    "href": "chapter_1_lesson_4_handout.html",
    "title": "Chapter 1 Lesson 4: In-Class Worksheet",
    "section": "",
    "text": "Table 3: Compute \\(\\hat s_t\\); then use Table 2 to find \\(\\bar s_t\\). Use \\(\\bar s_t\\) to find the random component and the seasonally adjusted time series values.\n\n\n\n\n\nMonth\nDeaths $$x_t$$\n$$ \\hat m $$\n$$ \\hat s $$\n$$ \\bar s $$\nRandom\nSeasonally Adjusted $$x_t$$\n\n\n\n\n2017 Jan\n3034\nNA\n\n\n\n\n\n\n2017 Feb\n2748\nNA\n\n\n\n\n\n\n2017 Mar\n3164\nNA\n\n\n\n\n\n\n2017 Apr\n3238\nNA\n\n\n\n\n\n\n2017 May\n3416\nNA\n\n\n\n\n\n\n2017 Jun\n3492\nNA\n\n\n\n\n\n\n2017 Jul\n3730\n3351.6\n\n\n\n\n\n\n2017 Aug\n3409\n3350\n\n\n\n\n\n\n2017 Sep\n3572\n3343.2\n\n\n\n\n\n\n2017 Oct\n3629\n3326.2\n\n\n\n\n\n\n2017 Nov\n3408\n3316.5\n\n\n\n\n\n\n2017 Dec\n3391\n3318.6\n\n\n\n\n\n\n2018 Jan\n3010\n3312.1\n\n\n\n\n\n\n2018 Feb\n2734\n3308\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 2: Table of \\(\\hat s_t\\) values, monthly means of \\(\\hat s_t\\), and seasonally adjusted mean \\(\\bar s_t\\).\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\n\n\n2017\nNA\nNA\nNA\nNA\nNA\nNA\n\n\n\n\n\n\n\n\n2018\n\n\n-296.7\n-334.2\n135.2\n221.6\n270.9\n219.8\n319.5\n395.8\n-14.2\n-79.5\n\n\n2019\n-308.7\n-727.1\n-311.1\n-180.3\n163\n192\n270.3\n376\n263.9\n253.2\n46\n-20.2\n\n\n2020\n-399.3\n-457.3\n-534.7\n-878.5\n-135.3\n489.2\n528.1\n631.3\n462.3\n496.9\n-1.2\n-243.8\n\n\n2021\n-390.8\n-1024.6\n-339\n14.7\n222.5\n207.2\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nMean\n-350.2\n-695.8\n-370.4\n-344.6\n96.3\n277.5\n361.9\n321.5\n318.6\n362.2\n\n\n\n\n$$ \\bar s_t $$"
  },
  {
    "objectID": "chapter_1_lesson_3.html",
    "href": "chapter_1_lesson_3.html",
    "title": "Averages for Time Series",
    "section": "",
    "text": "Decompose time series into trends, seasonal variation, and residuals\n\n\nDefine smoothing\nCompute the centered moving average for a time series\nEstimate the trend component using moving averages\n\n\n\n\nPlot time series data to visualize trends, seasonal patterns, and potential outliers\n\n\nPlot the estimated trend of a time series using a moving average\nMake box plots to examine seasonality\nInterpret the trend and seasonal pattern observed in a time series",
    "crumbs": [
      "Lesson 3",
      "Averages for Time Series"
    ]
  },
  {
    "objectID": "chapter_1_lesson_3.html#learning-outcomes",
    "href": "chapter_1_lesson_3.html#learning-outcomes",
    "title": "Averages for Time Series",
    "section": "",
    "text": "Decompose time series into trends, seasonal variation, and residuals\n\n\nDefine smoothing\nCompute the centered moving average for a time series\nEstimate the trend component using moving averages\n\n\n\n\nPlot time series data to visualize trends, seasonal patterns, and potential outliers\n\n\nPlot the estimated trend of a time series using a moving average\nMake box plots to examine seasonality\nInterpret the trend and seasonal pattern observed in a time series",
    "crumbs": [
      "Lesson 3",
      "Averages for Time Series"
    ]
  },
  {
    "objectID": "chapter_1_lesson_3.html#preparation",
    "href": "chapter_1_lesson_3.html#preparation",
    "title": "Averages for Time Series",
    "section": "Preparation",
    "text": "Preparation\n\nRead Sections 1.5.1-1.5.3",
    "crumbs": [
      "Lesson 3",
      "Averages for Time Series"
    ]
  },
  {
    "objectID": "chapter_1_lesson_3.html#learning-journal-exchange-10-min",
    "href": "chapter_1_lesson_3.html#learning-journal-exchange-10-min",
    "title": "Averages for Time Series",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading your partner’s?\nWhat would you recommend your partner add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 3",
      "Averages for Time Series"
    ]
  },
  {
    "objectID": "chapter_1_lesson_3.html#vocabulary-and-nomenclature-matching-activity-10-min",
    "href": "chapter_1_lesson_3.html#vocabulary-and-nomenclature-matching-activity-10-min",
    "title": "Averages for Time Series",
    "section": "Vocabulary and Nomenclature Matching Activity (10 min)",
    "text": "Vocabulary and Nomenclature Matching Activity (10 min)\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nWorking with a partner, match the definitions on the left with the terms on the right.\n\n\nNomenclature Matching\n\n\n\nDiscrete observations of a time series, taken at times \\(1, 2, \\ldots, n\\).\nNumber of observations of a time series\nLead time\nThe trend as observed at time \\(t\\)\nThe seasonal effect, as observed at time \\(t\\)\nThe error term (a sequence of correlated random variables with mean zero), as observed at time \\(t\\)\nCentered moving average for obsrvations made monthly\nEstimate of monthly additive effect\nEstimate of monthly multiplicative effect\n\n\n\n\n\\(n\\)\n\\(k\\)\n\\(m_t\\)\n\\(\\hat m_t\\)\n\\(s_t\\)\n\\(\\hat s_t = x_t - \\hat m_t ~~~~~~~~~~~~~~~~~~~~~~~~~\\)\n\\(\\hat s_t = \\dfrac{x_t}{\\hat m_t}\\)\n\\(\\{x_t\\}\\)\n\\(z_t\\)\n\n\n\nwhere \\(\\hat m_t = \\dfrac{\\frac{1}{2}x_{t-6} + x_{t-5} + \\cdots + x_{t-1} + x_t + x_{t+1} + \\cdot + x_{t+5} + \\frac{1}{2} x_{t+6}}{12}\\).\n\n\nAdditional Nomenclature Matching\n\n\n\nForecast made at time \\(t\\) for a future value \\(k\\) time units in the future \\(~~~~~~~~~~~~~~~~~~~~~~\\)\nAdditive decomposition model\nAdditive decomposition model after taking the logarithm\nMultiplicative decomposition model\nSeasonally adjusted mean for the month corresponding to time \\(t\\)\nSeasonal adjusted series (additive seasonal effect)\nSeasonal adjusted series (multiplicative seasonal effect)\n\n\n\n\n\\(\\bar s_t\\)\n\\(x_t = m_t + s_t + z_t\\)\n\\(x_t = m_t \\cdot s_t + z_t\\)\n\\(\\log(x_t) = m_t + s_t + z_t\\)\n\\(x_t - \\bar s_t\\)\n\\(\\frac{x_t}{\\bar s_t}\\)\n\\(\\hat x_{t+k \\mid t}\\)",
    "crumbs": [
      "Lesson 3",
      "Averages for Time Series"
    ]
  },
  {
    "objectID": "chapter_1_lesson_3.html#team-activity-moving-averages-30-min",
    "href": "chapter_1_lesson_3.html#team-activity-moving-averages-30-min",
    "title": "Averages for Time Series",
    "section": "Team Activity: Moving Averages (30 min)",
    "text": "Team Activity: Moving Averages (30 min)\n\nDerivation\nData representing some value have been collected each month for a few years. This plot represents the first 12 observations in this time series.\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nSuppose you wanted to compute the mean of the observations from the first year (\\(t = 1\\) to \\(t=12\\).) What is the formula you would use to compute this mean? Write this expression without a summation symbol.\nTo what value of \\(t\\) should this mean be assigned? (If you were to plot this mean on a time plot, where should it go?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nSuppose you want to compute the mean of one year’s worth of observations, beginning at month \\(t=2\\). Write the formula you would use to compute this mean without using a summation symbol.\nTo what value of \\(t\\) should this mean be assigned? (If you were to plot this mean on a time plot, where should it go?)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nNote that neither of the two means above are appropriately located on an integer value of \\(t\\).\nGive the formula that combines the two means above to give one mean that is centered on an integer value of \\(t\\). Do not include a summation symbol in your formula. (Hint: try averaging the two means.)\nUpon what value of \\(t\\) is your new mean centered?\n\n\n\nWe will now adjust this moving average adjusted so it is centered on any given value of \\(t\\), not just \\(t=7\\).\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nConsider the values \\(x_{t-6}\\), \\(x_{t-5}\\), \\(x_{t-4}\\), \\(x_{t-3}\\), \\(x_{t-2}\\), \\(x_{t-1}\\), \\(x_{t}\\), \\(x_{t+1}\\), \\(x_{t+2}\\), \\(x_{t+3}\\), \\(x_{t+4}\\), and \\(x_{t+5}\\).\n\nGive an expression for the mean of the values.\nWhere will this mean be centered?\n\nConsider the values \\(x_{t-5}\\), \\(x_{t-4}\\), \\(x_{t-3}\\), \\(x_{t-2}\\), \\(x_{t-1}\\), \\(x_{t}\\), \\(x_{t+1}\\), \\(x_{t+2}\\), \\(x_{t+3}\\), \\(x_{t+4}\\), \\(x_{t+5}\\), and \\(x_{t+6}\\).\n\nGive an expression for the mean of the values.\nWhere will this mean be centered?\n\nWe now combine these two means by averaging them.\n\nGive an expression for the mean of these two means.\nWhere will this combined mean be centered?\n\n\n\n\n\n\nApplication: Google Trends Searches for “Chocolate”\nRecall the Google Trends data for the term “chocolate” given in the file chocolate.csv.\n\n# load packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\"tsibble\", \"fable\",\n               \"feasts\", \"tsibbledata\",\n               \"fable.prophet\", \"tidyverse\",\n               \"patchwork\", \"rio\")\n\n# read in the data from a csv and make the tsibble\n# change the line below to include your file path\nchocolate_month &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/chocolate.csv\")\nstart_date &lt;- lubridate::ymd(\"2004-01-01\")\ndate_seq &lt;- seq(start_date,\n                start_date + months(nrow(chocolate_month)-1),\n                by = \"1 months\")\nchocolate_tibble &lt;- tibble(\n  dates = date_seq,\n  year = lubridate::year(date_seq),\n  month = lubridate::month(date_seq),\n  value = pull(chocolate_month, chocolate)\n)\nchocolate_month_ts &lt;- chocolate_tibble |&gt;\n  mutate(index = tsibble::yearmonth(dates)) |&gt;\n  as_tsibble(index = index)\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nUsing any tool (except R functions that automate the process) compute the centered moving average for the chocolate data. To help your check yourself, the value of \\(\\hat m\\) in month 7 should be 35.6667.\nCreate a plot of your centered moving average. Here are some examples of ways you could display your centered moving average.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat does the centered moving average reveal about the chocolate search time series?\nSuppose the chocolate data were reported daily. How would you compute the moving average? (Note: there are 365 days in a year.)",
    "crumbs": [
      "Lesson 3",
      "Averages for Time Series"
    ]
  },
  {
    "objectID": "chapter_1_lesson_3.html#estimating-the-seasonal-effect-side-by-side-box-plots-by-month-10-min",
    "href": "chapter_1_lesson_3.html#estimating-the-seasonal-effect-side-by-side-box-plots-by-month-10-min",
    "title": "Averages for Time Series",
    "section": "Estimating the Seasonal Effect: Side-by-Side Box Plots by Month (10 min)",
    "text": "Estimating the Seasonal Effect: Side-by-Side Box Plots by Month (10 min)\nTo better visualize the effect of seasonal variation, we can make box plots by month.\n\nggplot(chocolate_month_ts, aes(x = factor(month), y = value)) +\n    geom_boxplot() +\n  labs(\n    x = \"Month Number\",\n    y = \"Searches\",\n    title = \"Boxplots of Google Searches for 'Chocolate' by Month\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do you observe?\nWhich months tend to have the most searches? Which months tend to have the fewest seraches?\n\nCan you provide an explanation for this?\n\n\n\n\n\nSummary\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat does the centered moving average tell us?\nWhy is a centered moving average helpful when there are seasonal effects?\nFor the chocolate search data, answer the following questions:\n\nHow many values of \\(t\\) were not assigned a value of the centered moving average?\nInterpret that number in years.\nDoes this number depend on the length of the time series?",
    "crumbs": [
      "Lesson 3",
      "Averages for Time Series"
    ]
  },
  {
    "objectID": "chapter_1_lesson_3.html#homework-preview-5-min",
    "href": "chapter_1_lesson_3.html#homework-preview-5-min",
    "title": "Averages for Time Series",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions",
    "crumbs": [
      "Lesson 3",
      "Averages for Time Series"
    ]
  },
  {
    "objectID": "chapter_1_lesson_3.html#homework",
    "href": "chapter_1_lesson_3.html#homework",
    "title": "Averages for Time Series",
    "section": "Homework",
    "text": "Homework\n\n\n\n\n\n\nDownload Assignment\n\n\n\n\n homework_1_3.qmd \n\n\nMatching\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nMatching Solutions\n\nNomenclature Matching\n\n\n\n\n\n\n\n8. Discrete observations of a time series, taken at times \\(1, 2, \\ldots, n\\).\nO. \\(\\{x_t\\}\\)\n\n\n9. Number of observations of a time series\nH. \\(n\\)\n\n\n10. Lead time\nI. \\(k\\)\n\n\n11. The trend as observed at time \\(t\\)\nJ. \\(m_t\\)\n\n\n12. The seasonal effect, as observed at time \\(t\\)\nL. \\(s_t\\)\n\n\n13. The error term (a sequence of correlated random variables with mean zero), as observed at time \\(t\\)\nP. \\(z_t\\)\n\n\n14. Centered moving average for obsrvations made monthly\nK. \\(\\hat m_t\\)\n\n\n15. Estimate of monthly additive effect\nM. \\(\\hat s_t = x_t - \\hat m_t\\)\n\n\n16. Estimate of monthly multiplicative effect\nN. \\(\\hat s_t = \\dfrac{x_t}{\\hat m_t}\\)\n\n\n\n\n\nAdditional Nomenclature Matching\n\n\n\n\n\n\n\n17. Forecast made at time \\(t\\) for a future value \\(k\\) time units in the future\nW. \\(\\hat x_{t+k \\mid t}\\)\n\n\n18. Additive decomposition model\nR. \\(x_t = m_t + s_t + z_t\\)\n\n\n19. Additive decomposition model after taking the logarithm\nT. \\(\\log(x_t) = m_t + s_t + z_t\\)\n\n\n20. Multiplicative decomposition model\nS. \\(x_t = m_t \\cdot s_t + z_t\\)\n\n\n21. Seasonally adjusted mean for the month corresponding to time \\(t\\)\nQ. \\(\\bar s_t\\)\n\n\n22. Seasonal adjusted series (additive seasonal effect)\nU. \\(x_t - \\bar s_t\\)\n\n\n23. Seasonal adjusted series (multiplicative seasonal effect)\nV. \\(\\frac{x_t}{\\bar s_t}\\)\n\n\n\n\n\n\n\nTeam Activity\n\nSolution to Team Activity:\nIf you stored your centered moving average in a variable called “m_hat” in the “chocolate_month_ts” tsibble, you can generate the superimposed plot with the R command:\n\nchocolate_month_ts &lt;- chocolate_month_ts %&gt;% \n  mutate(\n    m_hat = (\n          (1/2) * lag(value,6)\n          + lag(value,5)\n          + lag(value,4)\n          + lag(value,3)\n          + lag(value,2)\n          + lag(value,1)\n          + value\n          + lead(value,1)\n          + lead(value,2)\n          + lead(value,3)\n          + lead(value,4)\n          + lead(value,5)\n          + (1/2) * lead(value,6)\n        ) / 12\n  )\n\nautoplot(chocolate_month_ts, .vars = value) +\n  labs(\n    x = \"Month\",\n    y = \"Searches\",\n    title = \"Google Searches for 'Chocolate'\"\n  ) +\n  geom_line(aes(x = dates, y = m_hat), color = \"#D55E00\") +\n  theme(plot.title = element_text(hjust = 0.5))",
    "crumbs": [
      "Lesson 3",
      "Averages for Time Series"
    ]
  },
  {
    "objectID": "chapter_1_lesson_1.html",
    "href": "chapter_1_lesson_1.html",
    "title": "Course Introduction",
    "section": "",
    "text": "Introduce the course structure and syllabus\n\n\nGet to know each other\nDescribe key concepts in time series analysis\nExplore an example time series interactively",
    "crumbs": [
      "Lesson 1",
      "Course Introduction"
    ]
  },
  {
    "objectID": "chapter_1_lesson_1.html#learning-outcomes",
    "href": "chapter_1_lesson_1.html#learning-outcomes",
    "title": "Course Introduction",
    "section": "",
    "text": "Introduce the course structure and syllabus\n\n\nGet to know each other\nDescribe key concepts in time series analysis\nExplore an example time series interactively",
    "crumbs": [
      "Lesson 1",
      "Course Introduction"
    ]
  },
  {
    "objectID": "chapter_1_lesson_1.html#introduction-to-the-course-structure-and-canvas-30-min",
    "href": "chapter_1_lesson_1.html#introduction-to-the-course-structure-and-canvas-30-min",
    "title": "Course Introduction",
    "section": "Introduction to the course structure and Canvas (30 min)",
    "text": "Introduction to the course structure and Canvas (30 min)\n\nIntroduction of teacher(s)\nIntroduction of students\nSyllabus\nSoftware: R and RStudio\nTextbook\n\nCowpertwait, P. S. P., & Metcalfe, A. V. (2009). Introductory Time Series with R. Springer. ISBN 978-0-387-88697-8; e-ISBN 978-0-387-88698-5; DOI 10.1007/978-0-387-88698-5.\n\nSupplement to the Textbook\n\nModern R code\nTime Series (TS) Notebook for in-class activities\n\nLesson cadence\n\nRead assigned section(s) from the textbook\n\nAssigned sections listed in the TS notebook\n\nReading Journals\n\nRecord your learning\nInclude all of the following from the assigned reading: vocabulary terms, nomenclature, models, important concepts, and your questions\nReview another student’s learning journal at the beginning of class\n\nIn-class Activities\nHomework\n\nAssessment Structure\n\nDaily Homework, Multi-week Projects, Three Exams\n\nGrading Categories\n\nReading Journal (10%)\nHomework (40%)\nProjects (25%)\nExams (25%)\n\nGrades: 93% = A\nCalendar\nTeam structure for class activities\n\nRandom assignment, frequent changes, partner with each student in the class\nWe are all in this together",
    "crumbs": [
      "Lesson 1",
      "Course Introduction"
    ]
  },
  {
    "objectID": "chapter_1_lesson_1.html#class-activity-google-trends-searches-for-chocolate-10-min",
    "href": "chapter_1_lesson_1.html#class-activity-google-trends-searches-for-chocolate-10-min",
    "title": "Course Introduction",
    "section": "Class Activity: Google Trends (Searches for “Chocolate”) (10 min)",
    "text": "Class Activity: Google Trends (Searches for “Chocolate”) (10 min)\nGoogle Trends allows you to download a time series showing the proportional number of searches for a given term. The month with the highest number of searches has a value of 100. The values for the other months are given as a percentage of the peak month’s value. The following table illustrates the data, as given by Google Trends.\n\n\n                                     \n Category: All categories            \n                                     \n Month     chocolate: (United States)\n 2004-01                           36\n 2004-02                           45\n 2004-03                           29\n 2004-04                           32\n 2004-05                           29\n 2004-06                           26\n 2004-07                           27\n 2004-08                           27\n 2004-09                           29\n 2004-10                           33\n 2004-11                           46\n 2004-12                           67\n 2005-01                           40\n    ⋮                               ⋮\n 2023-10                           57\n 2023-11                           69\n 2023-12                          100\n\n\nThe cleaned version of the data used for this demonstration are available in the file chocolate.csv. We can read this directly into a data frame using the command\nchocolate_month &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/chocolate.csv\")\nIn Lesson 3, we will practice converting data like this into a time series (tsibble) object.\n\n\nShow the code\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\"tsibble\", \"fable\",\n               \"feasts\", \"tsibbledata\",\n               \"fable.prophet\", \"tidyverse\",\n               \"patchwork\", \"rio\")\n\n# read in the data from a csv and make the tsibble\n# change the line below to include your file path\nchocolate_month &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/chocolate.csv\")\nstart_date &lt;- lubridate::ymd(\"2004-01-01\")\ndate_seq &lt;- seq(start_date,\n                start_date + months(nrow(chocolate_month)-1),\n                by = \"1 months\")\nchocolate_tibble &lt;- tibble(\n  dates = date_seq,\n  year = lubridate::year(date_seq),\n  month = lubridate::month(date_seq),\n  value = dplyr::pull(chocolate_month, chocolate)\n)\nchocolate_month_ts &lt;- chocolate_tibble |&gt;\n  mutate(index = tsibble::yearmonth(dates)) |&gt;\n  as_tsibble(index = index)\n\nchocolate_month_ts |&gt; head()\n\n\n# A tsibble: 6 x 5 [1M]\n  dates       year month value    index\n  &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt;    &lt;mth&gt;\n1 2004-01-01  2004     1    36 2004 Jan\n2 2004-02-01  2004     2    45 2004 Feb\n3 2004-03-01  2004     3    29 2004 Mar\n4 2004-04-01  2004     4    32 2004 Apr\n5 2004-05-01  2004     5    29 2004 May\n6 2004-06-01  2004     6    26 2004 Jun\n\n\nFor now, we will use the tsibble object (which in this case is called chocolate_month_ts) to explore the time series. Here is a plot of the time series representing the proportional frequency of searches for the term “chocolate.”\n\n\nShow the code\nautoplot(chocolate_month_ts, .vars = value) +\n  labs(\n    x = \"Month\",\n    y = \"Searches\",\n    title = \"Relative Number of Google Searches for 'Chocolate'\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do you notice about this plot?\n\n\n\n\nCharacteristicsTrendSeasonality / CyclesAutocorrelation\n\n\n\n\n\nThe red line represents the mean for each year. The point for this line was positioned to align with July of the year.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do you observe about the number of searches for “chocolate” each month?\nWhat might be causing this trend?\n\n\n\n\n\nConsider the data for the last few years:\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhich month tends to have the greatest number of Google searches for “chocolate”?\nWhich month has the second greatest number of Google searches for “chocolate”?\nWhen do the fewest number of Google searches for “chocolate” occur?\nHow can you explain these observations?\n\n\n\n\n\nAutocorrelation is a fancy word that means that sequential values in a sequence of data are related in some way.\nConsider searches in successive months. Are they independent?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nThink about what you know about the reported number of searches in December compared to the following February. The reported number of searches for “chocolate” in December 2022 is 93. Does it make sense that the reported number of searches in February 2023 is 71 ? Given the value from December, is the value in the following February independent and completely random?\nThe value reported by Google for June 2023 is 53. Based on what you have observed in the data, do you think the value for July 2023 will be close to or far from this value? Justify your answer.\n\n\n\n\n\n\nDiscuss these vocabulary terms in the context of the Google Trends (“Chocolate”) example: - Time series - Sampling interval - Autocorrelation (or serial dependence) - Trend - Seasonal variation - Cycle",
    "crumbs": [
      "Lesson 1",
      "Course Introduction"
    ]
  },
  {
    "objectID": "chapter_1_lesson_1.html#class-activity-sp-500-10-min",
    "href": "chapter_1_lesson_1.html#class-activity-sp-500-10-min",
    "title": "Course Introduction",
    "section": "Class Activity: S&P 500 (10 min)",
    "text": "Class Activity: S&P 500 (10 min)\nThe time series plot below illustrates the daily closing prices of the standard and Poor’s 500 stock index (S&P 500).\n\n\n\n\n\n\n\n\n\n\nCharacteristicsTrendSeasonality / CyclesAutocorrelation\n\n\n\n\n\nThe red line represents the mean for each year. The point for this line was positioned to align with July of the year.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do you observe about the value of the S&P 500 over time?\nWhat might be causing this trend?\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nAre there regularly-occurring seasonal trends in the data?\nAre there some random (stochastic) business cycles observable in the data?\nHow can you explain these observations?\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nConsider closing prices in successive days. Are they independent?\nWhy would there be autocorrelation in the data?\n\n\n\n\n\n\nDiscuss these vocabulary terms in the context of the S&P 500 example:\n\nTime series\nSampling interval\nAutocorrelation (or serial dependence)\nTrend\nSeasonal variation\nCycle\nDeterministic vs. Stochastic",
    "crumbs": [
      "Lesson 1",
      "Course Introduction"
    ]
  },
  {
    "objectID": "chapter_1_lesson_1.html#recap-5-min",
    "href": "chapter_1_lesson_1.html#recap-5-min",
    "title": "Course Introduction",
    "section": "Recap (5 min)",
    "text": "Recap (5 min)\n\nWhat is time series data?\n\nDefine “time series” (e.g. observations collected sequentially over time)\n\nExamples of time series data\nWhy ordinary regression fails – correlated error terms\nExamples of time series from different domains:\n\nDaily credit card balance\nDaily closing stock prices\nMonthly sales figures\nYearly global temperature measurements\nSecondly wave heights in an ocean buoy\nWeekly unemployment rates\nQuarterly GDP estimates\n\nImportance of context and subject matter knowledge\nRole of models (explanation, prediction, simulation)\nAre there any questions on the course or time series data?",
    "crumbs": [
      "Lesson 1",
      "Course Introduction"
    ]
  },
  {
    "objectID": "chapter_1_lesson_1.html#homework-preview-5-min",
    "href": "chapter_1_lesson_1.html#homework-preview-5-min",
    "title": "Course Introduction",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions",
    "crumbs": [
      "Lesson 1",
      "Course Introduction"
    ]
  },
  {
    "objectID": "chapter_1_lesson_1.html#homework",
    "href": "chapter_1_lesson_1.html#homework",
    "title": "Course Introduction",
    "section": "Homework",
    "text": "Homework\n\n\n\n\n\n\nDownload Assignment\n\n\n\n homework_1_1.qmd \n\nPreparation for the next class meeting\n\nUpdate R and RStudio\nAccess\n\nCanvas course\nTime Series Notebook (Quarto file)\n\nPurchase the textbook\nRead sections 1.1-1.4 in the textbook\nObtain a Learning Journal\nPrepare to share your Learning Journal with another student in the next class meeting",
    "crumbs": [
      "Lesson 1",
      "Course Introduction"
    ]
  },
  {
    "objectID": "chapter_1_lesson_1.html#preparation-for-the-next-class-meeting",
    "href": "chapter_1_lesson_1.html#preparation-for-the-next-class-meeting",
    "title": "Course Introduction",
    "section": "Preparation for the next class meeting",
    "text": "Preparation for the next class meeting\n\nUpdate R and RStudio\nAccess\n\nCanvas course\nTime Series Notebook (Quarto file)\n\nPurchase the textbook\nRead sections 1.1-1.4 in the textbook\nObtain a Learning Journal\nPrepare to share your Learning Journal with another student in the next class meeting",
    "crumbs": [
      "Lesson 1",
      "Course Introduction"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_A1_New.html",
    "href": "NewSequence/Chapter_A1_New.html",
    "title": "Time Series Components",
    "section": "",
    "text": "library(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3"
  },
  {
    "objectID": "NewSequence/Chapter_6_New.html",
    "href": "NewSequence/Chapter_6_New.html",
    "title": "Stationary Stochastic Processes",
    "section": "",
    "text": "library(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3",
    "crumbs": [
      "Chapter 6",
      "Stationary Stochastic Processes"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_6_New.html#understand-the-residual-as-a-stochastic-process",
    "href": "NewSequence/Chapter_6_New.html#understand-the-residual-as-a-stochastic-process",
    "title": "Stationary Stochastic Processes",
    "section": "6.1 Understand the residual as a stochastic process",
    "text": "6.1 Understand the residual as a stochastic process",
    "crumbs": [
      "Chapter 6",
      "Stationary Stochastic Processes"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_6_New.html#identify-a-time-series-as-stationary",
    "href": "NewSequence/Chapter_6_New.html#identify-a-time-series-as-stationary",
    "title": "Stationary Stochastic Processes",
    "section": "6.2 Identify a time series as stationary",
    "text": "6.2 Identify a time series as stationary\n\n6.2.1 Define strict stationarity\n\n\n6.2.2 Define weak or 2nd order stationarity\n\n\n6.2.3 Identify stationary and non-stationary stochastic processes using time series",
    "crumbs": [
      "Chapter 6",
      "Stationary Stochastic Processes"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_6_New.html#white-noise",
    "href": "NewSequence/Chapter_6_New.html#white-noise",
    "title": "Stationary Stochastic Processes",
    "section": "6.3 White Noise",
    "text": "6.3 White Noise",
    "crumbs": [
      "Chapter 6",
      "Stationary Stochastic Processes"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_6_New.html#random-walk",
    "href": "NewSequence/Chapter_6_New.html#random-walk",
    "title": "Stationary Stochastic Processes",
    "section": "6.4 Random Walk",
    "text": "6.4 Random Walk",
    "crumbs": [
      "Chapter 6",
      "Stationary Stochastic Processes"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_6_New.html#ar",
    "href": "NewSequence/Chapter_6_New.html#ar",
    "title": "Stationary Stochastic Processes",
    "section": "6.5 AR",
    "text": "6.5 AR",
    "crumbs": [
      "Chapter 6",
      "Stationary Stochastic Processes"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_6_New.html#ma",
    "href": "NewSequence/Chapter_6_New.html#ma",
    "title": "Stationary Stochastic Processes",
    "section": "6.6 MA",
    "text": "6.6 MA",
    "crumbs": [
      "Chapter 6",
      "Stationary Stochastic Processes"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_6_New.html#arma",
    "href": "NewSequence/Chapter_6_New.html#arma",
    "title": "Stationary Stochastic Processes",
    "section": "6.7 ARMA",
    "text": "6.7 ARMA",
    "crumbs": [
      "Chapter 6",
      "Stationary Stochastic Processes"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_6_New.html#test-for-stationarity",
    "href": "NewSequence/Chapter_6_New.html#test-for-stationarity",
    "title": "Stationary Stochastic Processes",
    "section": "6.8 Test for stationarity",
    "text": "6.8 Test for stationarity\n\n6.8.1 Backward operator and unit roots\n\n\n6.8.2 Dickey Fuller Test\n\n\n6.8.3 KPSS (Kwiatkowski-Phillips-Schmidt-Shin) test\n\n\n6.8.4 Phillips Perron",
    "crumbs": [
      "Chapter 6",
      "Stationary Stochastic Processes"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_4_New.html",
    "href": "NewSequence/Chapter_4_New.html",
    "title": "Trends",
    "section": "",
    "text": "library(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3",
    "crumbs": [
      "Chapter 4",
      "Trends"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_4_New.html#describe-trends-in-time-series",
    "href": "NewSequence/Chapter_4_New.html#describe-trends-in-time-series",
    "title": "Trends",
    "section": "4.1 Describe trends in time series",
    "text": "4.1 Describe trends in time series\n\n4.1.1 Identify examples of trends in time series\n\nLinear\n\n\nExponential\n\n\n\n4.1.2 Use a correlogram to see the effect of deterministic trends in autocorrelation time series",
    "crumbs": [
      "Chapter 4",
      "Trends"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_4_New.html#differentiate-stochastic-and-deterministic-trends",
    "href": "NewSequence/Chapter_4_New.html#differentiate-stochastic-and-deterministic-trends",
    "title": "Trends",
    "section": "4.2 Differentiate stochastic and deterministic trends",
    "text": "4.2 Differentiate stochastic and deterministic trends",
    "crumbs": [
      "Chapter 4",
      "Trends"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_4_New.html#model-deterministic-trends-using-regression",
    "href": "NewSequence/Chapter_4_New.html#model-deterministic-trends-using-regression",
    "title": "Trends",
    "section": "4.3 Model deterministic trends using regression",
    "text": "4.3 Model deterministic trends using regression\n\n4.3.1 Linear\n\n\n4.3.2 Exponential\n\n\n4.3.3 Polynomial\n\n\n4.3.4 Detrend a time series",
    "crumbs": [
      "Chapter 4",
      "Trends"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_4_New.html#model-non-deterministic-trends",
    "href": "NewSequence/Chapter_4_New.html#model-non-deterministic-trends",
    "title": "Trends",
    "section": "4.4 Model non-deterministic trends",
    "text": "4.4 Model non-deterministic trends\n\n4.4.1 Moving average\n\n\n4.4.2 Exponential smoothing trend and slope.",
    "crumbs": [
      "Chapter 4",
      "Trends"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_2_New_old.html",
    "href": "NewSequence/Chapter_2_New_old.html",
    "title": "Sample Statistics and Correlation",
    "section": "",
    "text": "Loading required package: pacman"
  },
  {
    "objectID": "NewSequence/Chapter_2_New_old.html#functions-stuff",
    "href": "NewSequence/Chapter_2_New_old.html#functions-stuff",
    "title": "Sample Statistics and Correlation",
    "section": "",
    "text": "Loading required package: pacman"
  },
  {
    "objectID": "NewSequence/Chapter_2_New_old.html#motivate-the-chapter.-why-does-it-matter",
    "href": "NewSequence/Chapter_2_New_old.html#motivate-the-chapter.-why-does-it-matter",
    "title": "Sample Statistics and Correlation",
    "section": "2.1 Motivate the chapter. Why does it matter?",
    "text": "2.1 Motivate the chapter. Why does it matter?\n\n2.1.1 Connect this with Ch1.1"
  },
  {
    "objectID": "NewSequence/Chapter_2_New_old.html#differentiate-population-and-sample-properties-and-statistics",
    "href": "NewSequence/Chapter_2_New_old.html#differentiate-population-and-sample-properties-and-statistics",
    "title": "Sample Statistics and Correlation",
    "section": "2.2 Differentiate population and sample properties and statistics",
    "text": "2.2 Differentiate population and sample properties and statistics\n\n2.2.1 Define population and samples\n\n\n2.2.2 Illustrate population and samples using pdf and histogram"
  },
  {
    "objectID": "NewSequence/Chapter_2_New_old.html#contrast-parameters-and-statistics-estimators-and-estimates",
    "href": "NewSequence/Chapter_2_New_old.html#contrast-parameters-and-statistics-estimators-and-estimates",
    "title": "Sample Statistics and Correlation",
    "section": "2.3 Contrast parameters and statistics, estimators and estimates",
    "text": "2.3 Contrast parameters and statistics, estimators and estimates"
  },
  {
    "objectID": "NewSequence/Chapter_2_New_old.html#calculate-a-sample-arithmetic-mean",
    "href": "NewSequence/Chapter_2_New_old.html#calculate-a-sample-arithmetic-mean",
    "title": "Sample Statistics and Correlation",
    "section": "2.4 Calculate a sample arithmetic mean",
    "text": "2.4 Calculate a sample arithmetic mean\n\n2.4.1 Define expected value and sample mean\nThe expected value, also known as the population mean, represents the true average value of the entire population you’re interested in. Unfortunately, obtaining the data for the entire population can be impractical or even impossible. This is where sample mean comes in.\n\n# Sample data\ndata &lt;- data.frame(val=rnorm(200, 1.37))  # Generate 20 random numbers from a normal distribution\n\n# Sample mean calculation\nsample_mean &lt;- mean(data$val)\n\n# Print the sample mean\ncat(\"Sample mean:\", sample_mean, \"\\n\")\n\nSample mean: 1.45839 \n\n\n\n\n2.4.2 Explain the intuition of sample mean using a histogram\nA histogram helps visualize the distribution of the data points in your sample. The sample mean, by definition, tends to be concentrated around the center of this distribution. The more data points you have in your sample, the closer the sample mean gets to the true expected value.\n\n# Histogram with sample mean highlighted\nggplot(data, aes(x = val)) +\n  geom_histogram(bins = 15, color = \"lightblue\") +  # Adjust bin count as needed\n  labs(x = \"Data value\", y = \"Frequency\", title = \"Sample mean distribution\") +\n  geom_vline(xintercept = sample_mean, color = \"red\", linetype = \"dashed\", lwd = 2, label = \"Sample Mean\") +\n  theme_classic()  # Optional: adjust plot aesthetics\n\nWarning in geom_vline(xintercept = sample_mean, color = \"red\", linetype =\n\"dashed\", : Ignoring unknown parameters: `label`\n\n\n\n\n\n\n\n\n\nBy looking at the histogram, you can see how the data points tend to cluster around the sample mean, providing an intuition for how it estimates the true expected value of the population."
  },
  {
    "objectID": "NewSequence/Chapter_2_New_old.html#calculate-a-sample-standard-deviation",
    "href": "NewSequence/Chapter_2_New_old.html#calculate-a-sample-standard-deviation",
    "title": "Sample Statistics and Correlation",
    "section": "2.5 Calculate a sample standard deviation",
    "text": "2.5 Calculate a sample standard deviation\n\n2.5.1 Define population and sample variance standard deviation\n\n\n2.5.2 Explain the intuition behind the sample standard deviation using a histogram\n\n\n2.5.3 Explain a box and whiskers plot."
  },
  {
    "objectID": "NewSequence/Chapter_2_New_old.html#calculate-sample-covariance-and-correlation-coefficient.",
    "href": "NewSequence/Chapter_2_New_old.html#calculate-sample-covariance-and-correlation-coefficient.",
    "title": "Sample Statistics and Correlation",
    "section": "2.6 Calculate sample covariance and correlation coefficient.",
    "text": "2.6 Calculate sample covariance and correlation coefficient.\n\n2.6.1 Define covariance and sample covariance\n\n\n2.6.2 Explain the intuition of covariance using a scatter plot\n\n\n2.6.3 Define the correlation and sample correlation coefficient.\n\n\n2.6.4 Interpret an estimate of sample correlation coefficient."
  },
  {
    "objectID": "NewSequence/Chapter_2_New_old.html#calculate-a-sample-autocorrelation",
    "href": "NewSequence/Chapter_2_New_old.html#calculate-a-sample-autocorrelation",
    "title": "Sample Statistics and Correlation",
    "section": "2.7 Calculate a sample autocorrelation",
    "text": "2.7 Calculate a sample autocorrelation\n\n2.7.1 Define population and sample autocorrelation\n\n\n2.7.2 Explain the intuition of sample autocorrelation using a scatter plot\n\n\n2.7.3 Interpret an estimate of sample autocorrelation at different lags\n\n\n2.7.4 Define a correlogram\n\n\n2.7.5 Conduct hypothesis testing using a correlogram"
  },
  {
    "objectID": "NewSequence/Chapter_2_New_old.html#content-dump",
    "href": "NewSequence/Chapter_2_New_old.html#content-dump",
    "title": "Sample Statistics and Correlation",
    "section": "2.8 Content Dump",
    "text": "2.8 Content Dump\n\nLearning Outcomes\n\n\nCompute the key statistics used to describe the linear relationship between two variables\n\n\nCompute the sample mean\nCompute the sample variance\nCompute the sample standard deviation\nCompute the sample covariance\nCompute the sample correlation coefficient\nExplain sample covariance using a scatter plot\n\n\n\n\nInterpret the key statistics used to describe sample data\n\n\nInterpret the sample mean\nInterpret the sample variance\nInterpret the sample standard deviation\nInterpret the sample covariance\nInterpret the sample correlation coefficient\n\n\n\n\nPreparation\n\nRead Sections 2.1-2.2.2 and 2.2.4\n\n\n\nLearning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading your partner’s?\nWhat would you recommend your partner add to their learning journal?\nSign the Learning Journal review sheet for your peer\n\n\n\nClass Activity: Variance and Standard Deviation (10 min)\nWe will explore the variance and standard deviation in this section.\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do the standard deviation and the variance measure?\n\n\n\nThe following code simulates observations of a random variable. We will use these data to explore the variance and standard deviation.\n\n# Set random seed\nset.seed(2412)\n\n# Specify means and standard deviation\nn &lt;- 5        # number of points\nmu &lt;- 10      # mean\nsigma &lt;- 3    # standard deviation\n\n# Simulate normal data\nsim_data &lt;- data.frame(x = round(rnorm(n, mu, sigma), 1)) |&gt; \n  arrange(x)\n\nThe data simulated by this process are:\n\n6.9, 7.7, 8.1, 10.8, 13.5\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nFind the sample mean of these numbers. \nWhat are some ways to interpret the mean?\n\n\n\nThe variance and standard deviation are individual numbers that summarize how far the data are from the mean. We first compute the deviations from the mean, \\(x - \\bar x\\). This is the directed distance from the mean to each data point.\n\n\n\n\n\n\n\n\n\nWe can summarize this information in a table:\n\nTable 1: Deviations from the mean\n\n\n\n\n\n$$x_t$$\n$$x_t-\\bar x$$\n\n\n\n\n\n\n\n\n6.9\n-2.5\n\n\n\n\n\n\n7.7\n-1.7\n\n\n\n\n\n\n8.1\n-1.3\n\n\n\n\n\n\n10.8\n1.4\n\n\n\n\n\n\n13.5\n4.1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nHow can we obtain one number that summarizes how spread out the data are from the mean? We may try averaging the deviations from the mean.\n\nWhat is the average deviation from the mean?\nWill we get the same value with other data sets, or is this just a coincidence?\nWhat could you do to prevent this from happening?\nApply your idea. Compute the resulting value that summarizes the spread. What do you get?\nWhat is the relationship between the sample variance and the sample standard deviation?\nUse a table like the one above to verify that the sample variance is 7.4.\nShow that the sample standard deviation is 2.7203.\n\n\n\n\n\n\nClass Activity: Covariance and Correlation (15 min)\n \n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do you get if you multiply the equations for \\(r\\), \\(s_x\\), and \\(s_y\\) together?\n\n\n\n\\[\n  r \\cdot s_x \\cdot s_y\n    =\n      \\frac{\\sum\\limits_{t=1}^n (x - \\bar x)(y - \\bar y)}{\\sqrt{\\sum\\limits_{t=1}^n (x - \\bar x)^2} \\sqrt{\\sum\\limits_{t=1}^n (y - \\bar y)^2}}\n      \\cdot\n      \\sqrt{ \\frac{\\sum\\limits_{t=1}^n (x - \\bar x)^2}{n-1} }\n      \\cdot\n      \\sqrt{ \\frac{\\sum\\limits_{t=1}^n (y - \\bar y)^2}{n-1} }\n    =\n      ?\n\\]\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nUse the numerical values above to confirm your result. Any discrepancy is due to roundoff error.\n\n\n\n\n\nTeam Activity: Computational Practice (15 min)\n\nTable 3: Computational Practice\nThe table below contains values of two time series \\(\\{x_t\\}\\) and \\(\\{y_t\\}\\) observed at times \\(t = 1, 2, \\ldots, 6\\). We will use these values to practice finding the means, standard deviations, correlation coefficient, and covariance without using built-in R functions.\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$y_t$$\n$$x_t-\\bar x$$\n$$(x_t - \\bar x)^2$$\n$$y_t-\\bar y$$\n$$(y_t-\\bar y)^2$$\n$$(x_t - \\bar x)(y_t-\\bar y)$$\n\n\n\n\n1\n-2.1\n2.8\n-1.9\n3.61\n1\n1\n-1.9\n\n\n2\n-0.2\n2.2\n\n\n\n\n\n\n\n3\n0.8\n0.9\n\n\n\n\n\n\n\n4\n0.4\n2\n\n\n\n\n\n\n\n5\n2.3\n-1\n\n\n\n\n\n\n\n6\n-2.4\n3.9\n\n\n\n\n\n\n\nsum\n-1.2\n10.8\n\n\n\n\n\n\n\n$$~$$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the table above to determine these values:\n\n\n\n\\(\\bar x =\\)\n\\(\\bar y =\\)\n\n\n\n\n\n\\(s_x =\\)\n\\(s_y =\\)\n\n\n\n\n\n\\(r =\\)\n\\(\\\\cov(x,y) =\\)\n\n\n\nHere is a scatterplot of the data.\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nWorking with your partner, prepare to explain the following concepts to the class:\n\nVariance\nStandard deviation\nCorrelation\nCovariance\n\n\n\n\n\n\nComputations in R (5 min)\nUse these commands to load the data from the previous activity into R.\n\n\nx &lt;- c( -2.1, -0.2, 0.8, 0.4, 2.3, -2.4 )\n\n\ny &lt;- c( 2.8, 2.2, 0.9, 2, -1, 3.9 )\n\n\nWe can use R to compute the mean, variance, standard deviation, correlation coefficient, and covariance.\n\nMean, \\(\\bar x\\)\n\nmean(x)\n\n[1] -0.2\n\n\n\n\nVariance, \\(s_x^2\\)\n\nvar(x)\n\n[1] 3.212\n\n\n\n\nStandard Deviation, \\(s_x\\)\n\nsd(x)\n\n[1] 1.792205\n\n\n\n\nCorrelation Coefficient, \\(r\\)\n\ncor(x, y)\n\n[1] -0.9449384\n\n\n\n\nCovariance, \\(\\\\cov(x,y)\\)\n\ncov(x, y)\n\n[1] -2.86\n\n\nClass Activity: Variance and Standard Deviation\n\nSolutions to Class Activity: Variance and Standard Deviation\n\n\n\n\n\nSolution\n$$x_t$$\n$$x_t-\\bar x$$\n$$(x_t-\\bar x)^2$$\n\n\n\n\n\n6.9\n-2.5\n6.25\n\n\n\n7.7\n-1.7\n2.89\n\n\n\n8.1\n-1.3\n1.69\n\n\n\n10.8\n1.4\n1.96\n\n\n\n13.5\n4.1\n16.81\n\n\nSum\n47.0\n0.0\n29.60\n\n\n\n\n\n\n\nThe variance of these values is \\(s^2 = \\frac{29.6}{5 - 1} = 3.212\\).\nThe standard deviation is \\(s = \\sqrt{s^2} = \\sqrt{3.212} = 1.792\\).\n\nTeam Activity: Computational Practice\n\nSolutions to Team Activity: Computational Practice\n\nTable 3: Computational Practice\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$y_t$$\n$$x_t-\\bar x$$\n$$(x_t - \\bar x)^2$$\n$$y_t-\\bar y$$\n$$(y_t-\\bar y)^2$$\n$$(x_t - \\bar x)(y_t-\\bar y)$$\n\n\n\n\n1\n-2.1\n2.8\n-1.9\n3.61\n1\n1\n-1.9\n\n\n2\n-0.2\n2.2\n0\n0\n0.4\n0.16\n0\n\n\n3\n0.8\n0.9\n1\n1\n-0.9\n0.81\n-0.9\n\n\n4\n0.4\n2\n0.6\n0.36\n0.2\n0.04\n0.12\n\n\n5\n2.3\n-1\n2.5\n6.25\n-2.8\n7.84\n-7\n\n\n6\n-2.4\n3.9\n-2.2\n4.84\n2.1\n4.41\n-4.62\n\n\nsum\n-1.2\n10.8\n0\n16.06\n0\n14.26\n-14.3\n\n\n\n\n\n\n\n\n\n\n\\(\\bar x = -0.2\\)\n\\(\\bar y = 1.8\\)\n\n\n\n\n\n\\(s_x = 1.7922053\\)\n\\(s_y = 1.6887865\\)\n\n\n\n\n\n\\(r = -0.9449384\\)\n\\(\\\\cov(x,y) = -2.86\\)\n\n\n\n\n\n\n\n\n2.8.1 New section 1\n\nLearning Outcomes\n\n\nDefine key terms in time series analysis\n\n\nDefine the ensemble of a time series\nDefine the expected value (or mean function) of a time series model\nDefine the sample estimate of the population mean of a time series\nDefine the variance function of a time series model\nState the constant variance estimator for a time series model\nExplain the stationarity assumption\nExplain the stationary variance assumption\nDefine lag\nDefine autocorrelation\nDefine the second-order stationary time series\nExplain the autocovariance function in Equation (2.11)\nExplain the lag k autocorrelation function in Equation (2.12)\nDefine the autocovariance function, acvf\nDefine the sample autocorrelation function, acf\n\n\n\n\nCalculate sample estimates of autocovariance and autocorrelation functions from time series data\n\n\nDefine the sample autocovariance function, c_k\nDefine the sample autocorrelation function, r_k\n\n\n\n\nPreparation\n\nRead Sections 2.2.5 and 2.3-2.5\n\n\n\nLearning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading your partner’s?\nWhat would you recommend your partner add to their learning journal?\nSign the Learning Journal review sheet for your peer\n\n\n\nHands-on Exercise – Exploring Sample Autocorrelation (40 min)\n\n\nComparison of Independent and Autocorrelated Error Terms\nIn the previous lesson, we computed the sample covariance and sample correlation coefficient between two independent variables. When working with time series, the observations are not independent. There is often a relationship between sequential observations. We will compute the autocovariance function and autocorrelation function for a time series. Note: the prefix “auto” comes from a Greek root meaning “self.”\nThe figure below illustrates the difference between a series of data, where the residuals are independent compared to a series with autocorrelated data.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nThe variances of the residuals for these two series are approximately equal. What characteristics distinguish the two series in the figure above?\n\n\n\n\n\nAutocovariance and Autocorrelation\nWe will use the following data to explore the concepts of autovariance and autocorrelation.\n\n\n\n\n\nt\n$$ x_t $$\n\n\n\n\n1\n4.4\n\n\n2\n4.2\n\n\n3\n4.2\n\n\n4\n4.0\n\n\n5\n4.4\n\n\n6\n4.7\n\n\n7\n4.9\n\n\n8\n5.3\n\n\n9\n5.4\n\n\n10\n5.5\n\n\n\n\n\n\n\nYou can use this R command to read in the observations.\n\n\nx &lt;- c( 4.4, 4.2, 4.2, 4, 4.4, 4.7, 4.9, 5.3, 5.4, 5.5 )\n\n\n\n\n\n\n\n\n\n\n\nWe will use the sample mean of these data repeatedly. The value of \\(\\bar x\\) is:\n\\[\n  \\bar x\n    = \\frac{1}{n} \\sum\\limits_{t=1}^{n} x_t\n    = \\frac{1}{10} \\cdot 47\n    = 4.7\n\\]\nWe will be finding the autocovariance and correlation of a time series with itself. First, we start with a lag of 1. With a lag of 1 the corresponding values of the time series that are being compared are shifted by one time unit. Then, we will consider any integer lag: lag \\(k\\).\n\n\nLag \\(k\\) Sample Autocovariance Function (acvf), \\(c_k\\)\nThe lag \\(k\\) sample autocovariance function, acvf, denoted \\(c_k\\), is defined as\n\\[\n  c_k = \\frac{1}{n} \\sum\\limits_{t=1}^{n-k}(x_t-\\bar x)(x_{t+k}-\\bar x)\n\\]\nWe denote the lag by the letter \\(k\\), where \\(k \\ge 0\\). This is the number of values the data set is shifted to compute the autocovariance.\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nExplain the equation for \\(c_k\\) to your partner.\nWhat is the equation for \\(c_0\\), the value of the autocovariance function with lag \\(k=0\\)?\n\nThis expression is very similar to a definition we have encountered previously. What is it?\n\n\n\n\n\nLag \\(k=1\\) Sample Autocovariance Function, \\(c_1\\)\nWe will now find the autocovariance between the values in a time series (\\(x = x_t\\)) and the same values, shifted by one unit of time (\\(y = x_{t+1}\\)).\n\n\n\n\n\nt\n$$ x_t $$\n$$ x_{t+k} $$\n$$ x_t-\\bar x $$\n$$ (x_t-\\bar x)^2 $$\n$$ x_{t+k}-\\bar x$$\n$$ (x-\\bar x)(x_{t+k}-\\bar x) $$\n\n\n\n\n1\n4.4\n4.2\n-0.3\n0.09\n-0.5\n0.15\n\n\n2\n4.2\n4.2\n-0.5\n0.25\n-0.5\n0.25\n\n\n3\n4.2\n4\n-0.5\n0.25\n-0.7\n0.35\n\n\n4\n4\n4.4\n-0.7\n0.49\n-0.3\n0.21\n\n\n5\n4.4\n4.7\n-0.3\n0.09\n0\n0\n\n\n6\n4.7\n4.9\n0\n0\n0.2\n0\n\n\n7\n4.9\n5.3\n0.2\n0.04\n0.6\n0.12\n\n\n8\n5.3\n5.4\n0.6\n0.36\n0.7\n0.42\n\n\n9\n5.4\n5.5\n0.7\n0.49\n0.8\n0.56\n\n\n10\n5.5\n—\n0.8\n0.64\n—\n—\n\n\nsum\n47\n42.6\n0\n2.7\n0.3\n2.06\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWorking with your assigned partner, compute each of the values in row 1 by hand. Recall that \\(\\bar x = 4.7\\).\nWith your partner, add up the values in the last column to verify that the sum is 2.06.\n\n\n\nThe scatterplot below illustrates the relationship between the observed data (\\(x_t\\)) and the next observation (\\(x_{t+1}\\)).\n\n\n\n\n\n\n\n\n\nIn this example, the second variable is \\(x_{t+1}\\), where \\(t&gt;1\\). the autocovariance of \\(x_t\\) and \\(x_{t+1}\\) is:\n\\[\n  c_1\n    = \\frac{1}{n} \\sum\\limits_{t=1}^{n-1}(x_t-\\bar x)(x_{t+1}-\\bar x)\n    = \\frac{1}{10} \\sum\\limits_{t=1}^{9}(x_t-\\bar x)(x_{t+1}-\\bar x)\n  = \\frac{1}{10} \\cdot 2.06\n  = 0.206\n\\]\nThis is the (auto)covariance of \\(x\\) with itself, but with a lag of 1 time unit. This is the value of the lag \\(k=1\\) autocovariance function, acvf_1.\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat does the lag 1 autocovariance measure?\n\n\n\n\n\n\nLag \\(k\\) Sample Autocorrelation Function (acf), \\(r_k\\)\nThe sample autocorrelation function, acf, denoted \\(r_k\\), where \\(k\\) is the lag, is defined as\n\\[\n  r_k\n    = \\frac{c_k}{c_0}\n    = \\frac{ \\frac{1}{n} \\sum\\limits_{t=1}^{n-k}(x_t-\\bar x)(x_{t+k}-\\bar x) }{ \\frac{1}{n} \\sum\\limits_{t=1}^{n}(x_t-\\bar x)^2 }\n    = \\frac{ \\sum\\limits_{t=1}^{n-k}(x_t-\\bar x)(x_{t+k}-\\bar x) }{ \\sum\\limits_{t=1}^{n}(x_t-\\bar x)^2 }\n\\]\nNote that \\(c_0\\) is the variance of \\(x\\), but computed by dividing by \\(n\\), instead of \\(n-1\\).\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nInterpret the components of the numerator and the denominator of the expression for \\(r_k\\) to your partner.\n\n\n\n\nLag \\(k=1\\) Sample Autocorrelation Function, \\(r_1\\)\nWe can compute the lag 1 autocorrelation or the autocorrelation of \\(x\\) with lag 1 as the quotient \\(r_1 = \\frac{c_1}{c_0}\\). We have already determined that \\(c_1 = 0.206\\). We now compute \\(c_0\\):\n\\[\n  c_0 = \\frac{1}{n} \\sum\\limits_{t=1}^{n-0} (x_t-\\bar x)(x_{t+0}-\\bar x)\n    = \\frac{1}{n} \\sum\\limits_{t=1}^{n} (x_t-\\bar x)^2\n    = \\frac{1}{10} \\cdot 2.7\n    = 0.27\n\\]\nWe use \\(c_0\\) and \\(c_1\\) to compute \\(r_1\\). Here are two ways we can compute this value:\n\\[\\begin{align*}\n  r_1\n    &= \\frac{c_1}{c_0}\n    =\n    \\frac{ \\frac{1}{n} \\sum\\limits_{t=1}^{9}(x_t-\\bar x)(x_{t+1}-\\bar x)\n        }{\n            \\frac{1}{n} \\sum\\limits_{t=1}^{10}(x_t-\\bar x)^2\n        }  \n    =\n    \\frac{\n        \\frac{1}{10} \\cdot 2.06\n      }{\n        \\frac{1}{10} \\cdot 2.7\n      }\n    = \\frac{0.206}{0.27}\n    = 0.763\n    \\\\\n    &=\n    \\frac{ \\sum\\limits_{t=1}^{9}(x_t-\\bar x)(x_{t+1}-\\bar x)\n        }{\n           \\sum\\limits_{t=1}^{10}(x_t-\\bar x)^2\n        }  \n    = \\frac{2.06}{2.7}\n    = 0.763\n    \n\\end{align*}\\]\n\nWhat does the lag 1 autocorrelation, \\(c_1\\), measure?\n\n\n\nLag \\(k = 2\\)\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWorking with your assigned partner, fill in the blanks in the following table. Use the results to compute \\(c_2\\) and \\(r_2\\).\n\n\n\n\n\n\n\n\nt\n$$ x_t $$\n$$ x_{t+k} $$\n$$ x_t-\\bar x $$\n$$ (x_t-\\bar x)^2 $$\n$$ x_{t+k}-\\bar x$$\n$$ (x-\\bar x)(x_{t+k}-\\bar x) $$\n\n\n\n\n1\n4.4\n4.2\n-0.3\n0.09\n-0.5\n0.15\n\n\n2\n4.2\n4\n-0.5\n0.25\n-0.7\n0.35\n\n\n3\n4.2\n4.4\n-0.5\n0.25\n-0.3\n0.15\n\n\n4\n4\n\n\n\n\n\n\n\n5\n4.4\n\n\n\n\n\n\n\n6\n4.7\n\n\n\n\n\n\n\n7\n4.9\n5.4\n0.2\n0.04\n0.7\n0.14\n\n\n8\n5.3\n5.5\n0.6\n0.36\n0.8\n0.48\n\n\n9\n5.4\n—\n0.7\n0.49\n—\n—\n\n\n10\n5.5\n—\n0.8\n0.64\n—\n—\n\n\nsum\n47\n\n\n\n\n\n\n\n\n\n\n\n\nThe figure below illustrates the relationship between \\(x_t\\) and \\(x_{t+2}\\).\n\n\n\n\n\n\n\n\n\n\n\nLag \\(k = 3\\)\n\n\n\n\n\nt\n$$ x_t $$\n$$ x_{t+k} $$\n$$ x_t-\\bar x $$\n$$ (x_t-\\bar x)^2 $$\n$$ x_{t+k}-\\bar x$$\n$$ (x-\\bar x)(x_{t+k}-\\bar x) $$\n\n\n\n\n1\n4.4\n4\n-0.3\n0.09\n-0.7\n0.21\n\n\n2\n4.2\n4.4\n-0.5\n0.25\n-0.3\n0.15\n\n\n3\n4.2\n4.7\n-0.5\n0.25\n0\n0\n\n\n4\n4\n4.9\n-0.7\n0.49\n0.2\n-0.14\n\n\n5\n4.4\n5.3\n-0.3\n0.09\n0.6\n-0.18\n\n\n6\n4.7\n5.4\n0\n0\n0.7\n0\n\n\n7\n4.9\n5.5\n0.2\n0.04\n0.8\n0.16\n\n\n8\n5.3\n—\n0.6\n0.36\n—\n—\n\n\n9\n5.4\n—\n0.7\n0.49\n—\n—\n\n\n10\n5.5\n—\n0.8\n0.64\n—\n—\n\n\nsum\n47\n34.2\n0\n2.7\n1.3\n0.2\n\n\n\n\n\n\n\nThe figure below illustrates the correlations between \\(x_t\\) and \\(x_{t+3}\\). Note that \\(c_3 = \\dfrac{0.2}{10} = 0.02\\) and \\(r_3 = \\dfrac{0.02}{0.27} = 0.0741\\).\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nDoes the value of \\(r_3 = 0.0741\\) seem reasonable, given the pattern in this plot?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLag \\(k = 4\\)\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nCompute \\(c_4\\) and \\(r_4\\) using R (but not automated functions), Excel, or hand calculations.\n\n\n\n\n\n\n\n\nt\n$$ x_t $$\n$$ x_{t+k} $$\n$$ x_t-\\bar x $$\n$$ (x_t-\\bar x)^2 $$\n$$ x_{t+k}-\\bar x$$\n$$ (x-\\bar x)(x_{t+k}-\\bar x) $$\n\n\n\n\n1\n4.4\n\n\n\n\n\n\n\n2\n4.2\n\n\n\n\n\n\n\n3\n4.2\n\n\n\n\n\n\n\n4\n4\n\n\n\n\n\n\n\n5\n4.4\n\n\n\n\n\n\n\n6\n4.7\n\n\n\n\n\n\n\n7\n4.9\n\n\n\n\n\n\n\n8\n5.3\n\n\n\n\n\n\n\n9\n5.4\n\n\n\n\n\n\n\n10\n5.5\n\n\n\n\n\n\n\nsum\n47\n\n\n\n\n\n\n\n\n\n\n\n\nThe figure below illustrates the correlations between \\(x_t\\) and \\(x_{t+4}\\).\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nDoes the value of \\(r_4\\) you computed seem reasonable, given the pattern in this plot?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nClass Activity: Using R to compute the acvf and acf (5 min)\nWe will continue to use the following sample data.\n\n\n x &lt;- c( 4.4, 4.2, 4.2, 4, 4.4, 4.7, 4.9, 5.3, 5.4, 5.5 ) \n df &lt;- data.frame(x = x)\n\n\n\nacvf\nThis code gives the values of the acvf.\n\nacf(df$x, plot=FALSE, type = \"covariance\")\n\n\nAutocovariances of series 'df$x', by lag\n\n     0      1      2      3      4      5      6      7      8      9 \n 0.270  0.206  0.121  0.020 -0.064 -0.113 -0.127 -0.093 -0.061 -0.024 \n\n\n\n\nacf\nWe can obtain the acf by changing the argument for the paramter type to \"correlation\".\n\nacf(df$x, plot=FALSE, type = \"correlation\")\n\n\nAutocorrelations of series 'df$x', by lag\n\n     0      1      2      3      4      5      6      7      8      9 \n 1.000  0.763  0.448  0.074 -0.237 -0.419 -0.470 -0.344 -0.226 -0.089 \n\n\n\n\n\nHomework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\nHomework\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_2_2.qmd \n\n\nClass Activity: k=2\n\nSolutions to Class Activity: \\(k=2\\)\n\n\n\n\n\nt\nx_t\nx_{t+k}\nx_t-mean(x)\n(x_t-mean(x))^2\nx_{t+k}-mean(x)\n(x-mean(x))(x_{t+k}-mean(x))\n\n\n\n\n1\n4.4\n4.2\n-0.3\n0.09\n-0.5\n0.15\n\n\n2\n4.2\n4\n-0.5\n0.25\n-0.7\n0.35\n\n\n3\n4.2\n4.4\n-0.5\n0.25\n-0.3\n0.15\n\n\n4\n4\n4.7\n-0.7\n0.49\n0\n0\n\n\n5\n4.4\n4.9\n-0.3\n0.09\n0.2\n-0.06\n\n\n6\n4.7\n5.3\n0\n0\n0.6\n0\n\n\n7\n4.9\n5.4\n0.2\n0.04\n0.7\n0.14\n\n\n8\n5.3\n5.5\n0.6\n0.36\n0.8\n0.48\n\n\n9\n5.4\n—\n0.7\n0.49\n—\n—\n\n\n10\n5.5\n—\n0.8\n0.64\n—\n—\n\n\nsum\n47\n38.4\n0\n2.7\n0.8\n1.21\n\n\n\n\n\n\n\n\\[\\begin{align*}\n  c_2\n    &= \\frac{1}{n} \\sum\\limits_{t=1}^{n-1}(x_t-\\bar x)(x_{t+2}-\\bar x)\n    = \\frac{1}{10} \\cdot 1.21\n    = 0.121 \\\\\n  r_2\n    &= \\frac{c_2}{c_0}\n    = \\frac{0.121}{0.27}\n    = 0.448\n\\end{align*}\\]\n\nClass Activity: k=4\n\nSolutions to Class Activity: \\(k=4\\)\n\n\n\n\n\nt\nx_t\nx_{t+k}\nx_t-mean(x)\n(x_t-mean(x))^2\nx_{t+k}-mean(x)\n(x-mean(x))(x_{t+k}-mean(x))\n\n\n\n\n1\n4.4\n4.4\n-0.3\n0.09\n-0.3\n0.09\n\n\n2\n4.2\n4.7\n-0.5\n0.25\n0\n0\n\n\n3\n4.2\n4.9\n-0.5\n0.25\n0.2\n-0.1\n\n\n4\n4\n5.3\n-0.7\n0.49\n0.6\n-0.42\n\n\n5\n4.4\n5.4\n-0.3\n0.09\n0.7\n-0.21\n\n\n6\n4.7\n5.5\n0\n0\n0.8\n0\n\n\n7\n4.9\n—\n0.2\n0.04\n—\n—\n\n\n8\n5.3\n—\n0.6\n0.36\n—\n—\n\n\n9\n5.4\n—\n0.7\n0.49\n—\n—\n\n\n10\n5.5\n—\n0.8\n0.64\n—\n—\n\n\nsum\n47\n30.2\n0\n2.7\n2\n-0.64\n\n\n\n\n\n\n\n\\[\\begin{align*}\n  c_4\n    &= \\frac{1}{n} \\sum\\limits_{t=1}^{n-1}(x_t-\\bar x)(x_{t+4}-\\bar x)\n    = \\frac{1}{10} \\cdot -0.64\n    = -0.064 \\\\\n  r_4\n    &= \\frac{c_4}{c_0} = \\frac{-0.064}{0.27}\n    = -0.237\n\\end{align*}\\]\n\n\n\n\n2.8.2 New section 2\n\nLearning Outcomes\n\n\nExplain the theoretical implications of autocorrelation for the estimation of time series statistics\n\n\nExplain how positive autocorrelation leads to underestimation of variance in short time series\nExplain how negative autocorrelation can improve efficiency of sample mean estimate\n\n\n\n\nInterpret correlograms to identify significant lags, correlations, trends, and seasonality\n\n\nCreate a correlogram\nInterpret a correlogram\nDefine a sampling distribution\nState the sampling distribution of rk\nExplain the concept of a confidence interval\nConduct a single hypothesis test using a correlogram\nDescribe the problems associated with multiple hypothesis testing in a correlogram\nDifferentiate statistical and practical significance\nDiagnose non-stationarity using a correlogram\n\n\n\n\nPreparation\n\nRead Sections 2.2.5 and 2.3-2.5 (No new reading assignment)\n\n\n\nLearning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading your partner’s?\nWhat would you recommend your partner add to their learning journal?\nSign the Learning Journal review sheet for your peer\n\n\n\nCorrelograms (10 min)\nIn the previous lesson, we used the following time series as an example. Here are the values in that time series:\n\n\nx &lt;- c( 4.4, 4.2, 4.2, 4, 4.4, 4.7, 4.9, 5.3, 5.4, 5.5 )\n\n\n\nThe table below gives the sample autocorrelation function, acf, for this data set. You may recognize some of these values from the previous lesson.\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n1\n0.763\n0.448\n0.074\n-0.237\n-0.419\n-0.47\n-0.344\n-0.226\n-0.089\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nUse the acf values to sketch the correlogram for these data in your Learning Journal. The figure below can help you begin.\n\n\n\nWarning in geom_segment(aes(x = 0, y = 0, xend = 0, yend = 1)): All aesthetics have length 1, but the data has 10 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = 0, y = 0, xend = 9, yend = 0)): All aesthetics have length 1, but the data has 10 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\n\n\n\n\n\n\n\n\n\nAre any of the autocorrelations statistically significant? If so, which one(s)?\n\n\n\n\n\nApplication: Chocolate Search Trends (10 min)\nRecall the Google Trends data for the term “chocolate” from the last lesson. The cleaned data are available in the file chocolate.csv.\n\nImport the chocolate search data and convert to tsibble format\nUse the code below to import the data and convert it into a time series (tsibble) object.\n\n# load packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\"tsibble\", \"fable\",\n               \"feasts\", \"tsibbledata\",\n               \"fable.prophet\", \"tidyverse\",\n               \"patchwork\", \"rio\")\n\n# read in the data from a csv and make the tsibble\n# change the line below to include your file path\nchocolate_month_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/chocolate.csv\") |&gt;\n  mutate(\n    dates = yearmonth(ym(Month)),\n    month = month(dates),\n    year = year(dates),\n    value = chocolate\n  ) |&gt; \n  dplyr::select(dates, month, year, value) |&gt;\n  as_tsibble(index = dates)\n\nchoc_decompose &lt;- chocolate_month_ts |&gt;\n    model(feasts::classical_decomposition(value,\n        type = \"add\"))  |&gt;\n    components()\n\nautoplot(choc_decompose)\n\n\n\n\n\n\n\n\nHere are the values of the acf for the chocolate search data:\n\nacf(chocolate_month_ts$value, plot=FALSE, type = \"correlation\", lag.max = 25)\n\n\nAutocorrelations of series 'chocolate_month_ts$value', by lag\n\n     0      1      2      3      4      5      6      7      8      9     10 \n 1.000  0.522  0.440  0.159  0.041 -0.018 -0.081 -0.024  0.020  0.121  0.386 \n    11     12     13     14     15     16     17     18     19     20     21 \n 0.425  0.814  0.426  0.357  0.103 -0.001 -0.051 -0.114 -0.057 -0.003  0.104 \n    22     23     24     25 \n 0.358  0.398  0.768  0.389 \n\n\nHere is the associated correlogram:\n\nacf(chocolate_month_ts$value, plot=TRUE, type = \"correlation\", lag.max = 25)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat does the information displayed in this correlogram suggest?\n\n\n\nIf we consider only the random component of this time series, the correlogram is:\n\nacf(choc_decompose$random |&gt; na.omit(), plot=TRUE, type = \"correlation\", lag.max = 25)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do the spikes in the correlogram tell us about this time series?\nIs there evidence of autocorrelation in the data after removing the trend and seasonal variation?\n\n\n\n\n\n\nSmall Group Activity: BYU-Idaho On-Campus Enrollment (25 min)\nThe official number of on-campus BYU-Idaho students each semester is given in the file byui_enrollment.csv.\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nDo the following:\n\nCreate a tsibble with the BYU-Idaho enrollment data. (Hint: There are three semesters in a year, so treat the enrollments as observations taken every four months in January, May, and September.)\nPlot the decomposition of this time series.\nDescribe the trend.\nDescribe the seasonal component.\nIs there evidence of seasonal variation? If so, propose an explanation for the seasonal variation.\nCreate the correlogram for these data.\n\nWhat do you observe?\nDoes the correlogram support the statement you made about the seasonal component?\n\nIs there evidence of autocorrelation in the data after removing the trend and seasonal variation?\n\n\n\n\n\nHomework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\nHomework\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_2_3.qmd \n\n\nCorrelograms\n\nSolutions to correlogram activity\n\nx &lt;- c( 4.4, 4.2, 4.2, 4, 4.4, 4.7, 4.9, 5.3, 5.4, 5.5 )\nacf(x, plot=FALSE, type = \"correlation\")\n\n\nAutocorrelations of series 'x', by lag\n\n     0      1      2      3      4      5      6      7      8      9 \n 1.000  0.763  0.448  0.074 -0.237 -0.419 -0.470 -0.344 -0.226 -0.089 \n\nacf(x, plot=TRUE, type = \"correlation\")\n\n\n\n\n\n\n\n\n\nBYU-Idaho Enrollment\n\nSolutions to BYU-Idaho Enrollment Activity\n\n# read in the data from a csv and make the tsibble\n\n# Method 1:\nenrollment_df &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/byui_enrollment.csv\")\nstart_date &lt;- lubridate::ymd(\"2019-05-01\")\ndate_seq &lt;- seq(start_date,\n                start_date + months(nrow(enrollment_df)-1) * 4,\n                by = \"4 months\")\nenrollment_ts &lt;- tibble(\n    dates = tsibble::yearmonth(date_seq),\n    semester = pull(enrollment_df, semester),\n    enrollment = pull(enrollment_df, enrollment)\n  ) |&gt;\n  dplyr::select(semester, dates, enrollment) |&gt;\n  as_tsibble(index = dates)\n\n# Method 2:\nenrollment_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/byui_enrollment.csv\") |&gt;\n  mutate(\n    dates = yearmonth(ym(paste(year, term * 4 - 3)))\n  ) |&gt;\n  dplyr::select(semester, dates, enrollment) |&gt;\n  as_tsibble(index = dates) \n\n# Compute and plot the decomposition\nenrollment_decompose &lt;- enrollment_ts |&gt;\n    model(feasts::classical_decomposition(enrollment,\n        type = \"add\"))  |&gt;\n    components()\nautoplot(enrollment_decompose)\n\n\n\n\n\n\n\n\n\nacf(enrollment_decompose$enrollment, type = \"correlation\")\n\n\n\n\n\n\n\n\n\nacf(enrollment_decompose$enrollment, plot=FALSE, type = \"correlation\")\n\n\nAutocorrelations of series 'enrollment_decompose$enrollment', by lag\n\n     0      1      2      3      4      5      6      7      8      9     10 \n 1.000 -0.333 -0.346  0.707 -0.298 -0.312  0.436 -0.278 -0.265  0.349 -0.153 \n    11 \n-0.134 \n\n\n\nacf(enrollment_decompose$enrollment, plot=TRUE, type = \"correlation\")\n\n\n\n\n\n\n\n\n\nacf(enrollment_decompose$random |&gt; na.omit(), plot=TRUE, type = \"correlation\")"
  },
  {
    "objectID": "NewSequence/Chapter_1_New.html",
    "href": "NewSequence/Chapter_1_New.html",
    "title": "Time Series Components",
    "section": "",
    "text": "library(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3",
    "crumbs": [
      "Chapter 1",
      "Time Series Components"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_2_New.html",
    "href": "NewSequence/Chapter_2_New.html",
    "title": "Sample Statistics and Correlation",
    "section": "",
    "text": "Loading required package: pacman",
    "crumbs": [
      "Chapter 2",
      "Sample Statistics and Correlation"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_2_New.html#functions-stuff",
    "href": "NewSequence/Chapter_2_New.html#functions-stuff",
    "title": "Sample Statistics and Correlation",
    "section": "",
    "text": "Loading required package: pacman",
    "crumbs": [
      "Chapter 2",
      "Sample Statistics and Correlation"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_2_New.html#motivate-the-chapter.-why-does-it-matter",
    "href": "NewSequence/Chapter_2_New.html#motivate-the-chapter.-why-does-it-matter",
    "title": "Sample Statistics and Correlation",
    "section": "2.1 Motivate the chapter. Why does it matter?",
    "text": "2.1 Motivate the chapter. Why does it matter?\n\n2.1.1 Connect this with Ch1.1",
    "crumbs": [
      "Chapter 2",
      "Sample Statistics and Correlation"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_2_New.html#differentiate-population-and-sample-properties-and-statistics",
    "href": "NewSequence/Chapter_2_New.html#differentiate-population-and-sample-properties-and-statistics",
    "title": "Sample Statistics and Correlation",
    "section": "2.2 Differentiate population and sample properties and statistics",
    "text": "2.2 Differentiate population and sample properties and statistics\nTime series are the realizations of uncertain events and processes. Uncertainty and randomness are inherent to human experience and natural phenomena. Scholars have developed a framework to study uncertainty, so before working with time series, we must build the vocabulary and tools necessary to understand and apply the science of uncertainty and risk.\n\nRandom Variables and Stochastic Processes\nA random variable is a numerical quantity whose value is determined by the outcome of a random process. A stochastic or random process is a collection of random variables that represent the evolution of a stochastic system over time. In time series analysis, a stochastic process can describe how a variable, such as daily temperature, changes over time. Each temperature measurement is a value of the random variable daily temperature. A sequence of temperature readings from the same location represents a time series. Each time series is a realization of the stochastic process, one of a potentially infinite number of sequences that could have been measured. In the case of daily temperatures, the stochastic process generating the data is governed by the natural processes of the earth’s weather: sun radiation, earth’s rotation, ocean temperatures, and wind patterns, among others.\n[Insert here a simulated temperature series and it’s ensemble]\nPopulation and Sample\nThe definitions of population and sample differentiate between all the possibilities of a stochastic process and our measurements. In statistics, a population refers to the entire set of individuals, observations, or values possible in a group of interest. It includes every possible member of the group we are studying. For example, if we are studying the heights of adult men in a country, the population would include the heights of all adult men in that country. Or if you are studying the roll of a die, it represents all the possible numerical values a die can take.\nIn the context of time series analysis, the concept of a population is called the ensemble. An ensemble represents the collection of all possible time series that a given stochastic process could generate. Each time series in the ensemble is a realization of the underlying process. For instance, consider the study of the daily temperatures in a city over ten years. The ensemble, or population, includes all possible sequences of daily temperatures that could be recorded over ten years.\nA sample is a subset of the population selected for analysis. In the context of time series, a time series is a sample from its ensemble. In modeling daily temperature readings, a single sequence of daily temperatures observed over ten years in a specific city is a sample from the population (ensemble) of all possible temperature sequences that could have occurred under similar conditions. Unlike sampling in cross-sectional analysis, the researcher does not direct or influence the sampling process in time series analysis; the time series measured is the only sample obtained, and there is no way to repeat the process or influence it in any way.\n\n\nParameters and Statistics\nParameters are features that describe an entire population, and the associated numerical values provide a summary measure of some aspect of the population. For example, the mean height of all adult men in a country is a parameter. In time series analysis, parameters include the mean, variance, and autocorrelation function of the entire ensemble of time series. Because parameters encompass the entire population, they are often denoted by Greek letters, such as \\(\\mu\\) for the population mean and \\(\\sigma^2\\) for the population variance.\nStatistics, on the other hand, are features that describe a sample drawn from the population. Unlike parameters, statistics can vary depending on which individuals are included in the sample. For example, the mean height of a sample of 1,000 men selected from the population is a statistic. The sample mean from a second sample of 1,000 men will differ from the first sample. We expect the two sample means to be numerically similar because they are drawn from the same population but are unlikely to be the same. In time series analysis, statistics include the sample mean, sample variance, and sample autocorrelation function of a single time series realization. Statistics are typically denoted by Latin letters, such as \\(\\bar{x}\\) for the sample mean and \\(s^2\\) for the sample variance.\nStatistics, on the other hand, are features that describe a sample drawn from the population. Unlike parameters, statistics can vary depending on which individuals are included in the sample. For example, the mean height of a sample of 1,000 men selected from the population is a statistic. The sample mean from a second sample of 1,000 men will differ from the first sample. We expect the two sample means to be numerically similar because they are drawn from the same population but are unlikely to be the same. In time series analysis, statistics include the sample mean, sample variance, and sample autocorrelation function of a single time series realization. Statistics are typically denoted by Latin letters, such as \\(\\bar{x}\\) for the sample mean and \\(s^2\\) for the sample variance.",
    "crumbs": [
      "Chapter 2",
      "Sample Statistics and Correlation"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_2_New.html#contrast-parameters-and-statistics-estimators-and-estimates",
    "href": "NewSequence/Chapter_2_New.html#contrast-parameters-and-statistics-estimators-and-estimates",
    "title": "Sample Statistics and Correlation",
    "section": "2.3 Contrast parameters and statistics, estimators and estimates",
    "text": "2.3 Contrast parameters and statistics, estimators and estimates\n\n2.3.1 Estimators and Estimates\nAn estimator is a rule or formula used to estimate a population parameter based on sample data. Estimators are functions of the observed data and are used to infer the values of unknown parameters. For example, the sample mean is an estimator used to estimate the population mean. There are preferred estimators for certain parameters; they are popular and usually spoken of as the only ones available, but note that there are many estimators for each parameter, and which one we use might depend on the context.\nFor example, consider the mean of a population as the parameter we aim to estimate. Two common estimators for this parameter are the sample and trimmed mean. The sample mean is calculated by summing all the values in a sample and then dividing by the number of values in that sample. It is a simple and widely used estimator for the population mean. On the other hand, the trimmed mean is calculated by removing a certain percentage of the smallest and largest values from the sample and then computing the mean of the remaining values. This estimator is particularly useful when the data contains outliers that might skew the sample mean. For instance, a 10% trimmed mean involves removing the lowest 10% and the highest 10% of the data points and then taking the mean of the remaining 80%. Both the sample mean and the trimmed mean aim to estimate the population mean but might yield different results depending on the presence of outliers or the distribution of the data.\nAn estimate is the numerical value obtained from an estimator when applied to a specific sample. For instance, if we calculate the mean of a sample of data points, this calculated value is the estimate of the population mean.\n\n\n2.3.2 Probabililty distributions of a Population and the Sampling Distribution of Estimators.\nTo illustrate and differentiate estimators and estimates, consider the outcomes of rolling two dice and summing the results.\nThe population in this case is the set of all possible outcomes of the sum of the two dice rolls, which ranges from 2 to 12. Each possible outcome has a theoretical probability associated with it, forming the probability distribution of the population. For example, there is only one way to roll a sum of 2 (both dice showing 1), but there are six ways to roll a sum of 7 (such as 1 and 6, 2 and 5, etc.). The probabilities for each possible sum are as follows:\n\nSum of 2: Probability = 1/36\nSum of 3: Probability = 2/36\nSum of 4: Probability = 3/36\nSum of 5: Probability = 4/36\nSum of 6: Probability = 5/36\nSum of 7: Probability = 6/36\nSum of 8: Probability = 5/36\nSum of 9: Probability = 4/36\nSum of 10: Probability = 3/36\nSum of 11: Probability = 2/36\nSum of 12: Probability = 1/36\n\nUsing these probabilities, the expected value, or mean, of the population can be calculated as\n\\[\\mu = \\sum_{i} X_i \\cdot P(X_i)\\]\nwhere \\(X_i\\) is each value that the the random variable \\(X\\) can take, and \\(P(X_i)\\) is the probability that \\(X\\) takes the value \\(X_i\\). The expected value is a weighted average of all the possible values of \\(X\\) weighted by how often they occur in the population.\nIn the case of the sum of two dice rolls, the expected value is\n\\[\\mu = \\sum_{X_i=2}^{12} X_i \\cdot P(X_i)\\]\n\\[\\mu = 2 \\cdot \\frac{1}{36} + 3 \\cdot \\frac{2}{36} + 4 \\cdot \\frac{3}{36} + 5 \\cdot \\frac{4}{36} + 6 \\cdot \\frac{5}{36} + 7 \\cdot \\frac{6}{36} + 8 \\cdot \\frac{5}{36} + 9 \\cdot \\frac{4}{36} + 10 \\cdot \\frac{3}{36} + 11 \\cdot \\frac{2}{36} + 12 \\cdot \\frac{1}{36}\\]\n\\[\\mu = \\frac{252}{36} = 7\\]\nWhich interpreted means that when we draw from the population at random, we expect to roll a seven when we add the value of two dice. Seven is not the only outcome, but its the most likely outcome among all the possibilities\nIn practice, the population parameter \\(\\mu\\) is often unobserved or unknown. An estimator of \\(\\mu\\) is the sample mean \\(\\bar x\\)\n\\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i\n\\]\nwhere \\(n\\) is the total number of observations in the sample, and \\(x_i\\) represents each individual observation in the sample.\nSuppose we record the results of rolling the two dice 50 times. This set of 50 outcomes is a sample from the population distribution. We repeat this process multiple times, each time calculating the sample mean,\n\\[\n\\bar{x} = \\frac{1}{n} \\sum_{i=1}^{n} x_i = \\frac{1}{50} \\sum_{i=1}^{50} x_i\n\\]\nthe code below simulates 1000 samples of 50 observations each and creates a histogram of the sample means.The red line indicates the theoretical population mean (7).\nA histogram is a graphical representation of the distribution of numerical data. The entire range of data is divided into a series of intervals or bins. Each bin represents a specific range of data values. The width of each bin is often the same, but this is not a strict requirement. Each bin is represented by a bar, and the height of the bar corresponds to the frequency (count) or relative frequency (probability) of data points that fall within the bin.\nThe x-axis (horizontal axis) represents the variable being measured. It is divided into intervals that correspond to the bins. The y-axis (vertical axis) represents the frequency or relative frequency of data points within each bin. For a probability histogram, the y-axis represents the probability density.\n\n\nCode\n# Simulate rolling two dice 50 times and calculate sample means\nset.seed(123)  # For reproducibility\nnum_samples &lt;- 1000\nsample_size &lt;- 50\nsample_means &lt;- numeric(num_samples)\n\nfor (i in 1:num_samples) {\n  sample_rolls &lt;- sample(2:12, size = sample_size, replace = TRUE, prob = c(1, 2, 3, 4, 5, 6, 5, 4, 3, 2, 1) / 36)\n  sample_means[i] &lt;- mean(sample_rolls)\n}\n\n# Plot histogram of sample means and theoretical population mean\nhist(sample_means, breaks = 30, probability = TRUE, col = \"lightblue\",\n     main = \"Sample Means of 50 Dice Rolls\", xlab = \"Sample Mean\", ylab = \"Frequency\")\nabline(v = 7, col = \"red\", lwd = 2)  # Theoretical population mean\n\n\n\n\n\n\n\n\n\nThe estimator we used to calculate the estimates is the same, but each sample has a different samples mean (estimate). Because each sample is a subset of the population, each sample mean contains information about the population mean. However, most of the 1000 estimates are not equal to the population mean.\nIn most situations, the population parameters are unknown, but we can use the distribution of sample estimates to infer their location. The histogram above represents the sampling distribution of sample means. In general, all estimators have a sampling distribution; statistical inference relies on these distributions to estimate population parameters or conduct statistical tests.\nThe following section will introduce four parameters critical to studying time series and their respective estimators. Understanding what the parameters communicate will help us characterize the various stochastic processes that generate time series. The respective estimators will provide estimates used as evidence to match the time series we observe with the underlying stochastic processes likely to have generated it. Successful forecasting and inference rely on correctly using estimates to narrow down which stochastic process we are modeling.",
    "crumbs": [
      "Chapter 2",
      "Sample Statistics and Correlation"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_2_New.html#calculate-a-sample-standard-deviation",
    "href": "NewSequence/Chapter_2_New.html#calculate-a-sample-standard-deviation",
    "title": "Sample Statistics and Correlation",
    "section": "2.4 Calculate a sample standard deviation",
    "text": "2.4 Calculate a sample standard deviation\n\n2.4.1 Understanding and Calculating the Sample Standard Deviation\nAn important feature of population is the variation of the random outcomes. One of the key measures of variability is the standard deviation.\n\n1.1 Define Population and Sample Variance and Standard Deviation\nPopulation Variance and Standard Deviation\nPopulation variance \\(\\sigma^2\\) is a measure of how data points in a population are spread out around the mean. It is calculated as the average of the squared differences from the mean. Mathematically, it is expressed as\n\\(\\sigma^2 = \\frac{1}{N} \\sum_{i=1}^{N} (X_i - \\mu)^2\\)\nwhere N is the size of the population, \\(X_i\\) represents each data point in the population, and \\(\\mu\\) is the population mean\nThe population standard deviation \\(\\sigma\\) is the square root of the population variance:\n\\(\\sigma = \\sqrt{\\sigma^2}\\)\nSample Variance and Standard Deviation\nSample variance \\(s^2\\) is used to estimate the population variance from a sample. It is calculated similarly to the population variance but with a slight adjustment (Bessel’s correction) to account for the fact that the sample is only a subset of the population:\n\\(s^2 = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})^2\\)\nwhere \\(n\\) is the size of the sample, \\(x_i\\) represents the value of each data point in the sample, \\(\\bar{x}\\) is the sample mean\nThe sample standard deviation \\(s\\) is the square root of the sample variance:\n\\(s = \\sqrt{s^2}\\)\n\n\n1.2 Explain the Intuition Behind the Sample Standard Deviation Using a Histogram\nTo grasp the intuition behind the sample standard deviation, consider a histogram of two samples with different variances.\n\n\nCode\n# Simulate daily temperatures for one year (365 days) with two different standard deviations\nset.seed(123)  # For reproducibility\ntemperature_data1 &lt;- rnorm(365, mean = 75, sd = 5)  # Sample 1: Mean temperature = 75, standard deviation = 5\ntemperature_data2 &lt;- rnorm(365, mean = 75, sd = 10) # Sample 2: Mean temperature = 75, standard deviation = 10\n\n# Calculate the sample means and standard deviations\nsample_mean1 &lt;- mean(temperature_data1)\nsample_sd1 &lt;- sd(temperature_data1)\nsample_mean2 &lt;- mean(temperature_data2)\nsample_sd2 &lt;- sd(temperature_data2)\n\n# Set x-axis limits to fit 3.5 standard deviations away from the mean on both sides\nxlim_range &lt;- range(sample_mean2 - 3.5 * sample_sd2, sample_mean2 + 3.5 * sample_sd2)\n\n# Create histograms\nhist(temperature_data1, breaks = 30, probability = TRUE, col = rgb(0, 0, 1, 0.5),\n     main = \"Sample Distribution of Daily Temperature Series\",\n     xlab = \"Temperature (°C)\", xlim = xlim_range, ylim = c(0, 0.08))\nhist(temperature_data2, breaks = 30, probability = TRUE, col = rgb(1, 0, 0, 0.5), add = TRUE)\n\n# Add vertical lines at the sample means\nabline(v = sample_mean1, col = \"blue\", lwd = 2, lty = 2)\nabline(v = sample_mean2, col = \"red\", lwd = 2, lty = 2)\n\n# Display sample means and standard deviations on the plot\ngraphics::legend(\"topright\", legend = c(paste(\"Sample 1: Mean =\", round(sample_mean1, 2), \", SD =\", round(sample_sd1, 2)),\n                              paste(\"Sample 2: Mean =\", round(sample_mean2, 2), \", SD =\", round(sample_sd2, 2))),\n       fill = c(rgb(0, 0, 1, 0.5), rgb(1, 0, 0, 0.5)), border = NA)\n\n\n\n\n\n\n\n\n\nThis code generates superimposed histograms of two simulated daily temperature series, one with a standard deviation of 5 and the other with a standard deviation of 10. The blue histogram represents the sample with a standard deviation of 5, and the red histogram represents the sample with a standard deviation of 10. The dashed lines indicate the sample means of the samples by color.\nThe spread of the data points around the mean gives us an idea of the variability. The red histogram shows a broader spread, indicating higher variability. The blue histogram shows a narrower spread, indicating lower variability.\nIn the time series context, the population variance quantifies the variability or dispersion of data points around the mean for the entire ensemble. By calculating the sample variance of a time series, we can assess the extent to which the observed values deviate from the average value over the observed time frame. This measure helps identify the volatility and stability of the time series, revealing patterns and trends. Understanding the sample variance allows us to make informed inferences about the underlying stochastic process and accurately model and forecast future values.\n\n\n1.3 Explain a Box-and-Whisker Plot\nA box-and-whisker plot (or box plot) is another visualization tool that illustrates the distribution and variability of a sample. It provides a summary of the data based on five key statistics: minimum, first quartile (Q1), median (Q2), third quartile (Q3), and maximum.\n\n\nCode\n# Create box-and-whisker plots\nboxplot(temperature_data1, temperature_data2, \n        names = c(\"Sample 1\", \"Sample 2\"), \n        main = \"Box-and-Whisker Plots of Two Simulated Samples\",\n        ylab = \"Temperature (°C)\", col = c(\"lightblue\", \"lightpink\"))\n\n\n# Display sample means on the plot\ngraphics::legend(\"bottomleft\", legend = c(paste(\"Sample 1: Mean =\", round(sample_mean1, 2), \", SD =\", round(sample_sd1, 2)),\n                              paste(\"Sample 2: Mean =\", round(sample_mean2, 2), \", SD =\", round(sample_sd2, 2))),\n       fill = c(rgb(0, 0, 1, 0.5), rgb(1, 0, 0, 0.5)), border = NA)\n\n\n\n\n\n\n\n\n\nThis code generates two box-and-whisker plots of the simulated daily temperature data. The box represents the interquartile range (IQR), showing the spread of the middle 50% of the data.The median line inside the box indicates the median temperature. The whiskers extend to the smallest and largest values within 1.5 times the IQR from Q1 and Q3. Any data points that fall outside these whiskers, known as outliers, are considered to be significantly different from the rest of the data and are plotted individually. The red dashed line represents the sample mean temperature, highlighting its position relative to the median and the overall data distribution.",
    "crumbs": [
      "Chapter 2",
      "Sample Statistics and Correlation"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_2_New.html#calculate-sample-covariance-and-correlation-coefficient.",
    "href": "NewSequence/Chapter_2_New.html#calculate-sample-covariance-and-correlation-coefficient.",
    "title": "Sample Statistics and Correlation",
    "section": "2.5 Calculate sample covariance and correlation coefficient.",
    "text": "2.5 Calculate sample covariance and correlation coefficient.\n\n2.5.1 Understanding Covariance and Correlation\nVariance measures the variability of one random variable. In time series, we are often interested in how two random variables “move together.” Covariance and correlation measure the linear co-movements between two random variables, and sample covariance and correlation are key statistics for inferring relationships between variables. Because complex systems generate most of the data collected today, statistics that illuminate the connections between variables are the foundation of quantitative science and our understanding of complex phenomena.\n\nCovariance and Sample Covariance\nCovariance\nCovariance is a measure of the degree to which two variables change together. If the variables tend to increase and decrease simultaneously, the covariance will be positive. If one variable tends to increase while the other decreases, the covariance will be negative. Mathematically, the population covariance between two variables \\(X\\) and \\(Y\\) is defined as:\n\\[\\text{Cov}(X, Y) = \\frac{1}{N} \\sum_{i=1}^{N} (X_i - \\mu_X)(Y_i - \\mu_Y)\\]\nwhere \\(N\\) is the size of the population, \\(X_i\\) and \\(Y_i\\) are the individual data points of \\(X\\) and \\(Y\\), and \\(\\mu_X\\) and \\(\\mu_Y\\) are the means of \\(X\\) and \\(Y\\).\nSample Covariance\nSample covariance is used to estimate the population covariance from a sample. It is calculated similarly to population covariance but uses sample data and includes Bessel’s correction to account for the sample size. The sample covariance between two variables \\(X\\) and \\(Y\\) is defined as:\n\\[s_{XY} = \\frac{1}{n-1} \\sum_{i=1}^{n} (x_i - \\bar{x})(y_i - \\bar{y})\\]\nwhere \\(n\\) is the size of the sample, \\(x_i\\) and \\(y_i\\) are the individual sample data points of \\(X\\) and \\(Y\\) , and \\(\\bar{x}\\) and \\(\\bar{y}\\) are the sample means of \\(X\\) and \\(Y\\).\n\n\nCorrelation and Sample Correlation Coefficient\nCovariance provides a numerical expression of the magnitude of the joint variability of two variables using their units measure. For example, in a study of the relationship between height in centimeters and weight in kilograms, the covariance will have units of centimeters-kilograms. This unit dependency can make it difficult to interpret the strength of the relationship, especially when comparing covariance statistics across data sets with different units. Additionally, the scale of covariance can vary widely depending on the variability of the data, complicating its interpretation. Correlation standardizes the covariance measure, providing a dimensionless number that quantifies the strength and direction of the linear relationship between two variables. The population correlation coefficient \\(\\rho\\) between two variables \\(X\\) and \\(Y\\) is defined as:\n\\[\\rho_{XY} = \\frac{\\text{Cov}(X, Y)}{\\sigma_X \\sigma_Y}\\]\nwhere \\(\\text{Cov}(X, Y)\\) is the covariance between \\(X\\) and \\(Y\\), and \\(\\sigma_X\\) and \\(\\sigma_Y\\) are the standard deviations of \\(X\\) and \\(Y\\). The correlation coefficient ranges from -1 to 1. A correlation of 1 indicates a perfect positive linear relationship, while a correlation of -1 indicates a perfect negative linear relationship. A correlation of 0 indicates no linear relationship.\nSample Correlation Coefficient\nThe sample correlation coefficient \\(r_{X,Y}\\) is the estimator \\(\\rho_{X,Y}\\) and is defined as:\n\\[r_{XY} = \\frac{s_{XY}}{s_X s_Y}\\]\nwhere \\(s_{XY}\\) is the sample covariance between \\(X\\) and \\(Y\\), and \\(s_X\\) and \\(s_Y\\) are the sample standard deviations of \\(X\\) and \\(Y\\).\n\n\nInterpreting an Estimate of the Sample Correlation Coefficient\nFor example, consider two time series: daily temperatures and daily ice cream sales. If the sample correlation coefficient, \\(r\\), between these variables is 0.8, it indicates a strong positive relationship, suggesting that higher temperatures are associated with increased ice cream sales. Conversely, if \\(r=-0.8\\), it would suggest that higher temperatures are associated with decreased ice cream sales. A correlation value around 0 would indicate that temperature changes have little to no effect on ice cream sales.\n\n\n\n2.5.2 Explain the intuition of covariance using a scatter plot\nThe following simulation explains the intuition behind the estimates for sample covariance and correlation. Please choose the size of the sample and the parameters in the population. The simulation will generate a sample and calculate, step by step, the estimates using the estimators.",
    "crumbs": [
      "Chapter 2",
      "Sample Statistics and Correlation"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_2_New.html#calculate-a-sample-autocorrelation",
    "href": "NewSequence/Chapter_2_New.html#calculate-a-sample-autocorrelation",
    "title": "Sample Statistics and Correlation",
    "section": "2.6 Calculate a sample autocorrelation",
    "text": "2.6 Calculate a sample autocorrelation\n\n2.6.1 Autocovariance and Autocorrelation\nAs explained in Chapter 1, in cross-sectional analysis, data is collected at one point in time across multiple subjects, such as individuals, countries, company stocks, or bacteria. Sample covariance and correlation are essential statistics used to understand the relationships between variables common to all the subjects. A critical assumption in cross-sectional analysis is the independent sampling of each subject in the population. Intuitively, that means that the selection of each subject in the sample is random or not connected in any systematic way. The violation of independent sampling jeopardizes the validity of the analysis. Suppose there were connections between subjects created by the sampling process; sample covariance and correlation estimates would pick up that connection alongside the population relationship the analysis was supposed to estimate. The sampling interconnections between subjects will muddle the estimates for the population relationships, either increasing or attenuating the magnitude of the relationship, eliminating our confidence in the estimates.\nRandom sampling is impossible in time series data, which tracks a single subject over multiple periods. Because time series are collected from a single subject, each observation is inherently related to another in the series and not chosen at random. Suppose a time series tracks the height of an individual over time at a monthly frequency. Most of us would find it curious if the height of our subject increased by 10 inches in a month and then decreased by 20 inches the next month. Humans do not grow so quickly, and unless for the loss of limbs, they will not shorten that quickly either. The connection between observations in a sample implies that many of the tools of cross-sectional inference are invalid. Instead of relying on information from multiple subjects, as in cross-sectional analysis, time-series statistics use multiple sequential observations to learn about the subject.\nThe autocovariance and autocorrelation functions help us characterize the relationships between sequential observations of the same subject. The prefix “auto” comes from a Greek root meaning “self.” Like in cross-sectional analysis, where we exploit variation across subjects to find something common to all the subjects, understanding the temporal relationships in a time series will illuminate the characteristics of the stochastic process that generated the data. The measurement and analysis of autocovariance and the tools that rely on this statistic will be the foundation of most of the analysis taught in the rest of the textbook. Like cross-sectional analysis, meeting the assumptions about the autocovariance structure of the data will be the primary driver in selecting the estimators we choose for our analysis.\n\n\n2.6.2 Define population and sample autocorrelation\nAutocovariance and autocorrelation quantify how values in a time series relate to their past and future values, providing insights into the internal structure and patterns of the data. Because there is a temporal component in these statistics, we specify them as a function of time or lag; so there is not a single estimator of autocorrelation, like covariance, but rather an autocovariance and autocorrelation function, with one estimator for each possible time lag.\n\nAutocovariance\nPopulation autocovariance measures the covariance of a time series with a lagged version of itself for the entire ensemble, or population. It quantifies the degree to which current values of the series are related to its past values. The population autocovariance function at lag \\(k\\) for a time series \\(X_t\\) is defined as:\n\\[\\gamma_k =\\frac{1}{N} \\sum^{N-k}_{t=1}{(X_t - \\mu)(X_{t+k} - \\mu)}\\]\nwhere: \\(N\\) is the number of observations in the time series, \\(X_t\\) and \\(X_{t+k}\\) are the values of the series at time \\(t\\) and \\(t+k\\), \\(\\mu\\) is the mean of the time series, and \\(k\\) is the lag.\nThe collection of \\(\\gamma_k\\) for all possible lags \\(k\\) forms the population autocovariance function, or ACVF.\nSample autocovariance is used to estimate the population autocovariance from a time series, not an ensemble. It is calculated similarly to the population autocovariance but uses sample data. The sample autocovariance at lag \\(k\\) is defined as:\n\\[\\hat{\\gamma}_k = c_k =\\frac{1}{n} \\sum_{t=1}^{n-k} (x_t - \\bar{x})(x_{t+k} - \\bar{x})\\]\nwhere: \\(n\\) is the number of observations in the sample, \\(x_t\\) and \\(x_{t+k}\\) are the sample values at time \\(t\\) and \\(t+k\\), \\(\\bar{x}\\) is the sample mean, and \\(k\\) is the lag\nThe collection of \\(c_k\\) for all possible lags \\(k\\) forms the sample autocovariance function.\n\n\nAutocorrelation\nPopulation autocorrelation standardizes the measure of autocovariance, providing a dimensionless value that quantifies the strength and direction of the relationship between values in a time series at different lags. The population autocorrelation function at lag \\(k\\), denoted as \\(\\rho_k\\) is defined as:\n\\[\\rho_k = \\frac{\\gamma_k}{\\gamma_0}\\]\nwhere: \\(\\gamma_k\\) is the population autocovariance at lag \\(k\\), and \\(\\gamma_0\\) is the variance of the time series, or autocovariance at lag zero.\nThe collection of \\(\\rho_k\\) for all possible lags \\(k\\) forms the population autocorrelation function, or ACF.\nSample Autocorrelation\nSample autocorrelation is used to estimate the population autocorrelation from a time series. It is defined as:\n\\[\n  \\hat{\\rho}_k= r_k\n    = \\frac{c_k}{c_0}\n    = \\frac{ \\frac{1}{n} \\sum\\limits_{t=1}^{n-k}(x_t-\\bar{x})(x_{t+k}-\\bar{x}) }{ \\frac{1}{n} \\sum\\limits_{t=1}^{n}(x_t-\\bar{x})^2 }\n    = \\frac{ \\sum\\limits_{t=1}^{n-k}(x_t-\\bar{x})(x_{t+k}-\\bar{x}) }{ \\sum\\limits_{t=1}^{n}(x_t-\\bar{x})^2 }\n\\]\nwhere: \\(c_k\\) is the sample autocovariance at lag \\(k\\), \\(c_0\\) is the sample variance of the time series, or autocovariance at lag zero.\nThe collection of \\(r_k\\) for all possible lags \\(k\\) forms the sample autocorrelation function.\n\n\n\n2.6.3 Explain the intuition of sample autocorrelation using a scatter plot\n\n\n2.6.4 Interpret an estimate of sample autocorrelation at different lags\nThe autocovariance and autocorrelation functions are a function of time. Each example that follows will show a scatter plot of a single time series against itself, lagged at different periods. The scatter plot shows the linear relationships between lagged variables and the code below each scatter plot shows the value of the autocovariance and autocorrelation functions for each lag.\nThis table and chart show ten observations from a simulation.\n\n\nCode\noffset_value &lt;- 1\ncov_df &lt;- get_data_for_cov_table(offset = offset_value)\n\n# Obtain the number of data values.\nn &lt;- nrow(cov_df)\n\n\ncov_df %&gt;%\n  dplyr::select(t, x) %&gt;% \n  rename(\"$$ x_t $$\"= x) %&gt;% \n  display_table()\n\n\n\n\n\nt\n$$ x_t $$\n\n\n\n\n1\n4.4\n\n\n2\n4.2\n\n\n3\n4.2\n\n\n4\n4.0\n\n\n5\n4.4\n\n\n6\n4.7\n\n\n7\n4.9\n\n\n8\n5.3\n\n\n9\n5.4\n\n\n10\n5.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe will use the sample mean of these data repeatedly. The value of \\(\\bar{x}\\) is:\n\\[\n  \\bar x\n    = \\frac{1}{n} \\sum\\limits_{t=1}^{n} x_t\n    = \\frac{1}{10} \\cdot 47\n    = 4.7\n\\]\n\nLag \\(k=1\\) Sample Autocovariance Function (ACVF), \\(c_1\\)\nThe lag \\(k=1\\) sample ACVF denoted \\(c_1\\), is defined as\n\\[\n  c_1 = \\frac{1}{n} \\sum\\limits_{t=1}^{n-1}(x_t-\\bar x)(x_{t+1}-\\bar x)\n\\] There are some important details to point out. Starting with the upper limit of the summation operator, \\(n-1\\). When we shift the series by one unit of time, we reduce the number of observations available for the calculation by one unit; for an illustration, see the table below. The inside of the summation operator there are \\(n-1\\) values for the expression \\((x_t-\\bar x)(x_{t+1}-\\bar x)\\). The summation operator then sums all the products and divides it by the sample size, or the length of the time series.\nIntuitively, the autocorrelation coefficient is the average of the product \\((x_t-\\bar x)(x_{t+1}-\\bar x)\\). The following table illustrates each step in the calculation.\n\n\n\n\n\nt\n$$ x_t $$\n$$ x_{t+1} $$\n$$ (x_t-\\bar x) $$\n$$ (x_t-\\bar x)^2 $$\n$$ (x_{t+1}-\\bar x)$$\n$$ (x-\\bar x)(x_{t+1}-\\bar x) $$\n\n\n\n\n1\n4.4\n4.2\n-0.3\n0.09\n-0.5\n0.15\n\n\n2\n4.2\n4.2\n-0.5\n0.25\n-0.5\n0.25\n\n\n3\n4.2\n4\n-0.5\n0.25\n-0.7\n0.35\n\n\n4\n4\n4.4\n-0.7\n0.49\n-0.3\n0.21\n\n\n5\n4.4\n4.7\n-0.3\n0.09\n0\n0\n\n\n6\n4.7\n4.9\n0\n0\n0.2\n0\n\n\n7\n4.9\n5.3\n0.2\n0.04\n0.6\n0.12\n\n\n8\n5.3\n5.4\n0.6\n0.36\n0.7\n0.42\n\n\n9\n5.4\n5.5\n0.7\n0.49\n0.8\n0.56\n\n\n10\n5.5\n—\n0.8\n0.64\n—\n—\n\n\nsum\n47\n42.6\n0\n2.7\n0.3\n2.06\n\n\n\n\n\n\n\nIn this example, the second variable is \\(x_{t+1}\\), where \\(t&gt;1\\). the autocovariance of \\(x_t\\) and \\(x_{t+1}\\) is:\n\\[\n  c_1\n    = \\frac{1}{n} \\sum\\limits_{t=1}^{n-1}(x_t-\\bar x)(x_{t+1}-\\bar x)\n    = \\frac{1}{10} \\sum\\limits_{t=1}^{ 9 }(x_t-\\bar x)(x_{t+1}-\\bar x)\n  = \\frac{1}{10} \\cdot 2.06\n  = 0.206\n\\]\nVisually, the scatter plot below illustrates the relationship between the observed data \\(x_t\\) and the next observation \\(x_{t+1}\\). Note that the product \\((x_t-\\bar x)(x_{t+1}-\\bar x)\\) is positive for all observations, suggesting a positive linear relationship. However, the magnitude of the relationship is difficult to evaluate give that the statistic is measured in the same units as the observations. The autocorrelation calculated in the next section will highlight the usefulness of a normalized measurement that doesn’t have the unit measurement problem of autocovariance.\n\n\n\n\n\n\n\n\n\n\n\nLag \\(k=1\\) Sample Autocorrelation Function, \\(r_1\\)\nWe can compute the autocorrelation of \\(x\\) with lag 1 as the quotient \\(r_1 = \\frac{c_1}{c_0}\\). We have already determined that \\(c_1 = 0.206\\). We now compute \\(c_0\\):\n\\[\n  c_0 = \\frac{1}{n} \\sum\\limits_{t=1}^{n-0} (x_t-\\bar x)(x_{t+0}-\\bar x)\n    = \\frac{1}{n} \\sum\\limits_{t=1}^{n} (x_t-\\bar x)^2\n    = \\frac{1}{10} \\cdot 2.7\n    = 0.27\n\\]\nWe use \\(c_0\\) and \\(c_1\\) to compute \\(r_1\\). Here are two ways we can compute this value:\n\\[\\begin{align*}\n  r_1\n    &= \\frac{c_1}{c_0}\n    =\n    \\frac{ \\frac{1}{n} \\sum\\limits_{t=1}^{9}(x_t-\\bar x)(x_{t+1}-\\bar x)\n        }{\n            \\frac{1}{n} \\sum\\limits_{t=1}^{10}(x_t-\\bar x)^2\n        }  \n    =\n    \\frac{\n        \\frac{1}{10} \\cdot 2.06\n      }{\n        \\frac{1}{10} \\cdot 2.7\n      }\n    = \\frac{0.206}{0.27}\n    = 0.763\n    \\\\\n    &=\n    \\frac{ \\sum\\limits_{t=1}^{9}(x_t-\\bar x)(x_{t+1}-\\bar x)\n        }{\n           \\sum\\limits_{t=1}^{10}(x_t-\\bar x)^2\n        }  \n    = \\frac{2.06}{2.7}\n    = 0.763\n    \n\\end{align*}\\]\nBecause we are using variance of \\(x_t\\) as the denominator in the ratio, the autocorrelation \\(r_1\\) is bounded in the range \\([-1,1]\\), with zero suggesting no linear relationship. We can evaluate the strength and magnitude of the relationship because all autocorrelation estimates have the same range. The value of 0.763 suggests a strong positive correlation between between \\(x_t\\) and \\(x_{t+1}\\).\n\n\nLag \\(k=2\\) Sample Autocorrelation Function, \\(r_2\\)\n\n\n\n\n\nt\n$$ x_t $$\n$$ x_{t+2} $$\n$$ x_t-\\bar x $$\n$$ (x_t-\\bar x)^2 $$\n$$ x_{t+2}-\\bar x$$\n$$ (x-\\bar x)(x_{t+2}-\\bar x) $$\n\n\n\n\n1\n4.4\n4.2\n-0.3\n0.09\n-0.5\n0.15\n\n\n2\n4.2\n4\n-0.5\n0.25\n-0.7\n0.35\n\n\n3\n4.2\n4.4\n-0.5\n0.25\n-0.3\n0.15\n\n\n4\n4\n4.7\n-0.7\n0.49\n0\n0\n\n\n5\n4.4\n4.9\n-0.3\n0.09\n0.2\n-0.06\n\n\n6\n4.7\n5.3\n0\n0\n0.6\n0\n\n\n7\n4.9\n5.4\n0.2\n0.04\n0.7\n0.14\n\n\n8\n5.3\n5.5\n0.6\n0.36\n0.8\n0.48\n\n\n9\n5.4\n—\n0.7\n0.49\n—\n—\n\n\n10\n5.5\n—\n0.8\n0.64\n—\n—\n\n\nsum\n47\n38.4\n0\n2.7\n0.8\n1.21\n\n\n\n\n\n\n\nThe autocovariance of \\(x_t\\) and \\(x_{t+2}\\) is:\n\\[\n  c_2\n    = \\frac{1}{n} \\sum\\limits_{t=1}^{n-2}(x_t-\\bar x)(x_{t+2}-\\bar x)\n    = \\frac{1}{10} \\sum\\limits_{t=1}^{8}(x_t-\\bar x)(x_{t+2}-\\bar x)\n  = \\frac{1}{10} \\cdot 1.21\n  = 0.121\n\\]\nWe use \\(c_0\\) and \\(c_2\\) to compute \\(r_2\\). Here are two ways we can compute this value:\n\\[\\begin{align*}\n  r_2\n    &= \\frac{c_2}{c_0}\n    =\n    \\frac{ \\frac{1}{n} \\sum\\limits_{t=1}^{8}(x_t-\\bar x)(x_{t+2}-\\bar x)\n        }{\n            \\frac{1}{n} \\sum\\limits_{t=1}^{10}(x_t-\\bar x)^2\n        }  \n    =\n    \\frac{\n        \\frac{1}{10} \\cdot 1.21\n      }{\n        \\frac{1}{10} \\cdot 2.7\n      }\n    = \\frac{0.121}{0.27}\n    = 0.4481\n    \\\\\n    &=\n    \\frac{ \\sum\\limits_{t=1}^{8}(x_t-\\bar x)(x_{t+2}-\\bar x)\n        }{\n           \\sum\\limits_{t=1}^{10}(x_t-\\bar x)^2\n        }  \n    = \\frac{1.21}{2.7}\n    = 0.4481\n    \n\\end{align*}\\]\nNotice that the number of elements inside the summation operator is \\(n-2=8\\). As we increase the lag by one, the number of products \\((x_t - \\bar{x})(x_{t+2} - \\bar{x})\\) is reduced by \\(k=2\\). For an illustration, please see the table above. The value of 0.4481 suggests a positive correlation between between \\(x_t\\) and \\(x_{t+2}\\), but one that is much weaker than the autocorrelation between \\(x_t\\) and \\(x_{t+1}\\).\nThe figure below illustrates the relationship between \\(x_t\\) and \\(x_{t+2}\\). Please compare this scatter plot with the one for lag \\(k=1\\) and understand the graphical differences that illustrate the difference in the magnitude of the sample autocorrelation estimates.\n\n\n\n\n\n\n\n\n\n\n\nLag \\(k=3\\) Sample Autocorrelation Function, \\(r_3\\)\n\n\n\n\n\nt\n$$ x_t $$\n$$ x_{t+3} $$\n$$ x_t-\\bar x $$\n$$ (x_t-\\bar x)^2 $$\n$$ x_{t+3}-\\bar x$$\n$$ (x-\\bar x)(x_{t+3}-\\bar x) $$\n\n\n\n\n1\n4.4\n4\n-0.3\n0.09\n-0.7\n0.21\n\n\n2\n4.2\n4.4\n-0.5\n0.25\n-0.3\n0.15\n\n\n3\n4.2\n4.7\n-0.5\n0.25\n0\n0\n\n\n4\n4\n4.9\n-0.7\n0.49\n0.2\n-0.14\n\n\n5\n4.4\n5.3\n-0.3\n0.09\n0.6\n-0.18\n\n\n6\n4.7\n5.4\n0\n0\n0.7\n0\n\n\n7\n4.9\n5.5\n0.2\n0.04\n0.8\n0.16\n\n\n8\n5.3\n—\n0.6\n0.36\n—\n—\n\n\n9\n5.4\n—\n0.7\n0.49\n—\n—\n\n\n10\n5.5\n—\n0.8\n0.64\n—\n—\n\n\nsum\n47\n34.2\n0\n2.7\n1.3\n0.2\n\n\n\n\n\n\n\nNote that the column with the product \\((x_t - \\bar{x})(x_{t+3} - \\bar{x})\\) has positive, negative, and zero values. Their average \\(r_3\\) is close to zero, suggesting a very weak or non-existent linear relationship at lag \\(k=3\\) The estimates for the autocovariance and autocorrelation are \\(c_3 = \\dfrac{0.2}{10} = 0.02\\) and \\(r_3 = \\dfrac{0.02}{0.27} =0.0741\\).\nThe figure below illustrates the correlations between \\(x_t\\) and \\(x_{t+3}\\). Please compare it to the prior scatter plots to understand the intuition behind the magnitude of the estimate \\(r_3\\)\n\n\n\n\n\n\n\n\n\n\n\nLag \\(k=4\\) Sample Autocorrelation Function, \\(r_4\\)\n\n\n\n\n\nt\n$$ x_t $$\n$$ x_{t+4} $$\n$$ x_t-\\bar x $$\n$$ (x_t-\\bar x)^2 $$\n$$ x_{t+4}-\\bar x$$\n$$ (x-\\bar x)(x_{t+4}-\\bar x) $$\n\n\n\n\n1\n4.4\n4.4\n-0.3\n0.09\n-0.3\n0.09\n\n\n2\n4.2\n4.7\n-0.5\n0.25\n0\n0\n\n\n3\n4.2\n4.9\n-0.5\n0.25\n0.2\n-0.1\n\n\n4\n4\n5.3\n-0.7\n0.49\n0.6\n-0.42\n\n\n5\n4.4\n5.4\n-0.3\n0.09\n0.7\n-0.21\n\n\n6\n4.7\n5.5\n0\n0\n0.8\n0\n\n\n7\n4.9\n—\n0.2\n0.04\n—\n—\n\n\n8\n5.3\n—\n0.6\n0.36\n—\n—\n\n\n9\n5.4\n—\n0.7\n0.49\n—\n—\n\n\n10\n5.5\n—\n0.8\n0.64\n—\n—\n\n\nsum\n47\n30.2\n0\n2.7\n2\n-0.64\n\n\n\n\n\n\n\nAt lag \\(k=4\\) the number of products \\((x_t - \\bar{x})(x_{t+4} - \\bar{x})\\) is only six. Note that a majority are negative. The estimates for the autocovariance and autocorrelation are \\(c_4 = \\dfrac{-0.64}{10} = -0.064\\) and \\(r_4 = \\dfrac{-0.064}{0.27} =-0.237\\). For most purposes, this autocorrelation estimate is not practically significant, there is no linear relationship at lag \\(k=4\\). The figure below illustrates the correlations between \\(x_t\\) and \\(x_{t+4}\\).\n\n\n\n\n\n\n\n\n\nThe autocovariance and autocorrelation functions are crucial tools in time series analysis, as they measure how a time series relates to its past values at different lags. Autocovariance quantifies the covariance between a time series and its lagged version, while autocorrelation standardizes this measure, providing a dimensionless value that indicates the strength and direction of the relationship. Both functions are dependent on the lag and illustrating the temporal relationship between the variables involves the analysis of multiple lags. The next section will introduce the correlogram, a single chart that shows the temporal relationships of a time series at many lags simultaneously.\n\n\nDefine a Correlogram\nA correlogram, also known as an autocorrelation plot, visually represents the autocorrelation coefficients of a time series for different lags. This graphical tool helps identify patterns, trends, and the extent of correlations within the data over time. The table below summarizes the ACF values calculated in the previous section and five additional lags.\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\n\n\n1\n0.763\n0.448\n0.074\n-0.237\n-0.419\n-0.47\n-0.344\n-0.226\n-0.089\n\n\n\n\n\n\n\n\n\nCreate a correlogram\nThe correlogram below consists of several key components. The primary feature is the display of autocorrelation coefficients for different lags; compare them to the table above. Each bar in the correlogram represents the correlation coefficient at a specific lag, with values ranging from -1 to 1. Values close to 1 indicate a strong positive correlation, values close to -1 indicate a strong negative correlation, and values around 0 suggest no significant correlation. The x-axis of the correlogram represents the lag values, typically starting from lag 0 (the correlation of the series with itself) and extending to higher lag values. Each lag value indicates the time shift applied to the series for computing the autocorrelation.\n\n\nCode\nacf(df$x, plot=TRUE, \n    type = \"correlation\", \n    main = \"ACF Plot of Simulated Data\",\n    ylim = c(-1, 1)\n    )\n\n\n\n\n\n\n\n\n\nCorrelograms include confidence intervals, shown as dashed horizontal lines. These intervals help assess the statistical significance of the autocorrelation coefficients. If a bar extends beyond the confidence interval, the autocorrelation at that lag is considered statistically significant. The zero line, or baseline, indicates zero correlation. Bars extending above or below this line show positive or negative correlations, respectively.\nThe statistical significance decision rule involves evaluating a hypothesis tests with each autocorrelation coefficient, one at a time. The hypothesis test uses the null hypothesis \\(H_0: \\rho_k = 0\\) which states that there is no autocorrelation in the time series at a given lag. The alternative hypothesis \\(H_1: \\rho_k \\neq 0\\) states that there is significant autocorrelation at a given lag. We assume that the data follows a normal distribution centered around the value of the null hypothesis. If a autocorrelation coefficient lies outside the interval, we reject the null hypothesis for that lag, concluding that there is significant autocorrelation at that lag. If the coefficient lies within the interval, we fail to reject the null hypothesis, indicating no significant autocorrelation.\nTo create the confidence interval, first choose a significance level \\(\\alpha\\), the default in R is 0.05. For large samples, the confidence intervals for \\(\\alpha=0.05\\) are \\(0 \\pm \\frac{\\alpha_c}{\\sqrt{n}}\\), where \\(n\\) is the number of observations in the time series and \\(\\alpha_c\\) is the test’s critical value. For large samples, the critical value \\(\\alpha_c=1.96\\) corresponds to a 95% confidence level using the standard normal distribution.\nBased on the simulated data, the autocorrelation \\(r_1\\) is the only statistically significant temporal relationship. The value of \\(r_0\\) is always 1 because it represents the correlation of the time series with itself and should be ignored. The estimates \\(r_5\\) and \\(r_6\\) are numerically large, suggesting a relationship at lags \\(k=5\\) and \\(k=6\\), but neither satisfy the single hypothesis test for each lag and are not statistically different from zero.\nThere are two key points related to hypothesis testing that need to be highlighted. First, the significance level, \\(\\alpha\\), represents the probability of making a Type I error, or false positive. A Type I error occurs when the null hypothesis is true, but we incorrectly reject it. If \\(\\alpha=0.05\\), approximately five percent of hypothesis tests will be false positives. In correlograms with many lags, around five percent of autocorrelation coefficients will extend beyond the confidence interval thresholds. Additionally, it’s likely that the estimates \\(r_k\\) are correlated, so adjacent estimates might fall in patterns, making interpretation of the underlying phenomenon more difficult. We don’t know which estimates are false positives, so the investigator needs to be careful when making statements about the significance of each estimate. Remember, in statistics we are never completely certain.\nSecond, statistically significant estimates might not be practically significant. As the number of observations increase the confidence interval shrinks as the standard error \\(\\pm \\frac{1.96}{\\sqrt{n}}\\) tends to zero. Estimates of autocorrelation coefficients that are close to zero in magnitude might be statistically significant in this case. As previously stated, five percent of the estimates will be false positives, so a practical significance rule is to report statistically significant estimates that are large in magnitude. Practically significance can be justified based on a feature of autocorrelation coefficients. The expression \\(r_k^2\\) represents how much of the variation in \\(x_t\\) is explained in the linear dependency between \\(x_t\\) on \\(x_k\\). For estimates that are small in magnitude the explanatory power of the relationship will be diminutive rendering the estimate of no practical consequence.\n\n\nCode\nwww &lt;- \"https://raw.githubusercontent.com/AtefOuni/ts/master/Data/wave.dat\"\nwave &lt;- read.table(www, header = T)\n\nplot (as.ts(wave$waveht), ylab = 'Wave height')\n\n\n\n\n\n\n\n\n\nCode\nacf (wave$waveht)\n\n\n\n\n\n\n\n\n\n[insert here the waves time series from the textbook]\nIn this example many estimates are statistically significant. In a chart of 25 lags approximately one estimate will be a false positive. Take \\(r_17 \\approx0.1\\) for example, \\(r_{17}^2 \\approx0.01\\) so only about 1% of the variation in \\(x_t\\) is explained by the linear relationship between \\(r_t\\) and \\(r_{t+17}\\). While \\(r_{17}\\) is statistically significant it fails the practical significance test. We can’t know whether \\(r_{17}\\) is a false positive, but it is not worth reporting regardless.\nThe pattern of estimates at adjacent lags suggest a correlation between them. This correlation make the interpretation of the individual estimates difficult since it’s clear the magnitude of \\(r_4\\) is linked to the magnitude of \\(r_3\\). The next chapter will introduce the idea of trends and seasonality in time series. Trends and seasonality are relationships in sequential observations that we can estimate and account for, reducing the correlation between the time periods and allowing us to interpret autocorrelation estimates easier.",
    "crumbs": [
      "Chapter 2",
      "Sample Statistics and Correlation"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_3_New.html",
    "href": "NewSequence/Chapter_3_New.html",
    "title": "Time Series Components",
    "section": "",
    "text": "library(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3",
    "crumbs": [
      "Chapter 3",
      "Time Series Components"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_3_New.html#differentiate-deterministic-vs-stochastic-processes",
    "href": "NewSequence/Chapter_3_New.html#differentiate-deterministic-vs-stochastic-processes",
    "title": "Time Series Components",
    "section": "3.1 Differentiate deterministic vs stochastic processes",
    "text": "3.1 Differentiate deterministic vs stochastic processes\n\n3.1.1 Define deterministic process\n\n\n3.1.2 Define stochastic process\n\n\n3.1.3 Plot deterministic process\n\n\n3.1.4 Simulate and illustrate a stochastic process",
    "crumbs": [
      "Chapter 3",
      "Time Series Components"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_3_New.html#simulate-seasonal-variation",
    "href": "NewSequence/Chapter_3_New.html#simulate-seasonal-variation",
    "title": "Time Series Components",
    "section": "3.2 Simulate seasonal variation",
    "text": "3.2 Simulate seasonal variation\n\n3.2.1 Define seasonal variation\n\n\n3.2.2 Define the sine and cosine functions\n\n\n3.2.3 Simulate seasonal variation using sine and cosine curves",
    "crumbs": [
      "Chapter 3",
      "Time Series Components"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_3_New.html#simulate-a-time-series-with-trend-seasonal-and-random-component",
    "href": "NewSequence/Chapter_3_New.html#simulate-a-time-series-with-trend-seasonal-and-random-component",
    "title": "Time Series Components",
    "section": "3.3 Simulate a time series with trend, seasonal, and random component",
    "text": "3.3 Simulate a time series with trend, seasonal, and random component\n\n3.3.1 Define trend\n\n\n3.3.2 Define additive time series\n\n\n3.3.3 Define multiplicative time series\n\n\n3.3.4 Simulate an additive time series with linear trend\n\n\n3.3.5 Simulate a multiplicative time series with linear trend\n\n\n3.3.6 Plot time series",
    "crumbs": [
      "Chapter 3",
      "Time Series Components"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_5_New.html",
    "href": "NewSequence/Chapter_5_New.html",
    "title": "Seasonality",
    "section": "",
    "text": "library(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3",
    "crumbs": [
      "Chapter 5",
      "Seasonality"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_5_New.html#identify-seasonality",
    "href": "NewSequence/Chapter_5_New.html#identify-seasonality",
    "title": "Seasonality",
    "section": "5.1 Identify Seasonality",
    "text": "5.1 Identify Seasonality\n\n5.1.1 Identify seasonality in a time series plot\n\n\n5.1.2 Identify a seasonality in a correlogram",
    "crumbs": [
      "Chapter 5",
      "Seasonality"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_5_New.html#model-seasonality-as-indicator-variables-using-regression.",
    "href": "NewSequence/Chapter_5_New.html#model-seasonality-as-indicator-variables-using-regression.",
    "title": "Seasonality",
    "section": "5.2 Model seasonality as indicator variables using regression.",
    "text": "5.2 Model seasonality as indicator variables using regression.",
    "crumbs": [
      "Chapter 5",
      "Seasonality"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_5_New.html#model-seasonality-as-harmonic-terms-using-regression-appendix-in-reserve",
    "href": "NewSequence/Chapter_5_New.html#model-seasonality-as-harmonic-terms-using-regression-appendix-in-reserve",
    "title": "Seasonality",
    "section": "5.3 Model seasonality as harmonic terms using regression (appendix in reserve)",
    "text": "5.3 Model seasonality as harmonic terms using regression (appendix in reserve)",
    "crumbs": [
      "Chapter 5",
      "Seasonality"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_5_New.html#model-trend-and-seasonality-using-the-constant-term-decomposition-additive-vs-multiplicative",
    "href": "NewSequence/Chapter_5_New.html#model-trend-and-seasonality-using-the-constant-term-decomposition-additive-vs-multiplicative",
    "title": "Seasonality",
    "section": "5.4 Model trend and seasonality using the Constant term decomposition (Additive vs Multiplicative)",
    "text": "5.4 Model trend and seasonality using the Constant term decomposition (Additive vs Multiplicative)",
    "crumbs": [
      "Chapter 5",
      "Seasonality"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_5_New.html#decompose-time-series-using-holt-winters-additive-vs-multiplicative",
    "href": "NewSequence/Chapter_5_New.html#decompose-time-series-using-holt-winters-additive-vs-multiplicative",
    "title": "Seasonality",
    "section": "5.5 Decompose time series using Holt winters (Additive vs Multiplicative)",
    "text": "5.5 Decompose time series using Holt winters (Additive vs Multiplicative)",
    "crumbs": [
      "Chapter 5",
      "Seasonality"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_5_New.html#seasonal-adjustment-for-time-series-using-all-the-methods-described-above",
    "href": "NewSequence/Chapter_5_New.html#seasonal-adjustment-for-time-series-using-all-the-methods-described-above",
    "title": "Seasonality",
    "section": "5.6 Seasonal adjustment for time series using all the methods described above",
    "text": "5.6 Seasonal adjustment for time series using all the methods described above",
    "crumbs": [
      "Chapter 5",
      "Seasonality"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_7_New.html",
    "href": "NewSequence/Chapter_7_New.html",
    "title": "Non Stationary Stochastic Processes",
    "section": "",
    "text": "library(ggplot2)\n\nWarning: package 'ggplot2' was built under R version 4.3.3",
    "crumbs": [
      "Chapter 7",
      "Non Stationary Stochastic Processes"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_7_New.html#arima",
    "href": "NewSequence/Chapter_7_New.html#arima",
    "title": "Non Stationary Stochastic Processes",
    "section": "7.1 ARIMA",
    "text": "7.1 ARIMA",
    "crumbs": [
      "Chapter 7",
      "Non Stationary Stochastic Processes"
    ]
  },
  {
    "objectID": "NewSequence/Chapter_7_New.html#garch",
    "href": "NewSequence/Chapter_7_New.html#garch",
    "title": "Non Stationary Stochastic Processes",
    "section": "7.2 GARCH",
    "text": "7.2 GARCH",
    "crumbs": [
      "Chapter 7",
      "Non Stationary Stochastic Processes"
    ]
  },
  {
    "objectID": "chapter_1.html",
    "href": "chapter_1.html",
    "title": "Lessons & Homework",
    "section": "",
    "text": "Chapter 1\n\n\n\n\n\n\nLessons\n\n\n\n\nLesson 1 - Course Introduction\nLesson 2 - Plots Trends, and Seasonal Variation\nLesson 3 - Averages for Time Series\nLesson 4 - Additive Models\nLesson 5 - Multiplicative Models\n\n\n\n\n\n\n\n\n\nDownload Homework Assignments\n\n\n\n\n homework_1_1.qmd \n homework_1_2.qmd \n homework_1_3.qmd \n homework_1_4.qmd \n homework_1_5.qmd",
    "crumbs": [
      "Overview",
      "Lessons & Homework"
    ]
  },
  {
    "objectID": "chapter_1_lesson_2.html",
    "href": "chapter_1_lesson_2.html",
    "title": "Plots Trends, and Seasonal Variation",
    "section": "",
    "text": "Use technical language to describe the main features of time series data\n\n\nDefine time series analysis\nDefine time series\nDefine sampling interval\nDefine serial dependence or autocorrelation\nDefine a time series trend\nDefine seasonal variation\nDefine cycle\nDifferentiate between deterministic and stochastic trends\n\n\n\n\nPlot time series data to visualize trends, seasonal patterns, and potential outliers\n\n\nPlot a “ts” object\nPlot the estimated trend of a time series by computing the mean across one full period",
    "crumbs": [
      "Lesson 2",
      "Plots Trends, and Seasonal Variation"
    ]
  },
  {
    "objectID": "chapter_1_lesson_2.html#learning-outcomes",
    "href": "chapter_1_lesson_2.html#learning-outcomes",
    "title": "Plots Trends, and Seasonal Variation",
    "section": "",
    "text": "Use technical language to describe the main features of time series data\n\n\nDefine time series analysis\nDefine time series\nDefine sampling interval\nDefine serial dependence or autocorrelation\nDefine a time series trend\nDefine seasonal variation\nDefine cycle\nDifferentiate between deterministic and stochastic trends\n\n\n\n\nPlot time series data to visualize trends, seasonal patterns, and potential outliers\n\n\nPlot a “ts” object\nPlot the estimated trend of a time series by computing the mean across one full period",
    "crumbs": [
      "Lesson 2",
      "Plots Trends, and Seasonal Variation"
    ]
  },
  {
    "objectID": "chapter_1_lesson_2.html#preparation",
    "href": "chapter_1_lesson_2.html#preparation",
    "title": "Plots Trends, and Seasonal Variation",
    "section": "Preparation",
    "text": "Preparation\n\nRead Sections 1.1-1.4",
    "crumbs": [
      "Lesson 2",
      "Plots Trends, and Seasonal Variation"
    ]
  },
  {
    "objectID": "chapter_1_lesson_2.html#learning-journal-exchange-15-min",
    "href": "chapter_1_lesson_2.html#learning-journal-exchange-15-min",
    "title": "Plots Trends, and Seasonal Variation",
    "section": "Learning Journal Exchange (15 min)",
    "text": "Learning Journal Exchange (15 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading your partner’s?\nWhat would you recommend your partner add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 2",
      "Plots Trends, and Seasonal Variation"
    ]
  },
  {
    "objectID": "chapter_1_lesson_2.html#vocabulary-and-nomenclature-matching-activity-5-min",
    "href": "chapter_1_lesson_2.html#vocabulary-and-nomenclature-matching-activity-5-min",
    "title": "Plots Trends, and Seasonal Variation",
    "section": "Vocabulary and Nomenclature Matching Activity (5 min)",
    "text": "Vocabulary and Nomenclature Matching Activity (5 min)\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nWorking with a partner, match the definitions on the left with the terms on the right.\n\n\nVocabulary Matching\n\n\n\nA figure with time on the horizontal axis and the value of a random variable on the vertical axis\nA systematic change in a time series that does not appear to be periodic\nRepeated pattern within each year (or any other fixed time period)\nRepeated pattern that does not correspond to some fixed natural period\nObservations in which values are related to lagged observations of the same variable\nRandom trend that does not follow a discernible or predictable pattern\nCan be modeled with mathematical functions, facilitating the long-term prediction of the behavior\n\n\n\n\nCycle\nCorrelated (Serially Dependent) Data\nDeterministic Trend\nSeasonal Variation\nStochastic Trend\nTime Plot\nTrend",
    "crumbs": [
      "Lesson 2",
      "Plots Trends, and Seasonal Variation"
    ]
  },
  {
    "objectID": "chapter_1_lesson_2.html#comparison-of-deterministic-and-stochastic-time-series-10-min",
    "href": "chapter_1_lesson_2.html#comparison-of-deterministic-and-stochastic-time-series-10-min",
    "title": "Plots Trends, and Seasonal Variation",
    "section": "Comparison of Deterministic and Stochastic Time Series (10 min)",
    "text": "Comparison of Deterministic and Stochastic Time Series (10 min)\n\nStochastic Time Series\nThe following app illustrates a few realizations of a stochastic time series.\n \n\nIf a stochastic time series displays an upward trend, can we conclude that trend will continue in the same direction? Why or why not?\n\n\n\nDeterministic Time Series\nThe figure below illustrates realizations of a deterministic time series. The data fluctuate around a sine curve.",
    "crumbs": [
      "Lesson 2",
      "Plots Trends, and Seasonal Variation"
    ]
  },
  {
    "objectID": "chapter_1_lesson_2.html#class-activity-importing-data-and-creating-a-tsibble-object-5-min",
    "href": "chapter_1_lesson_2.html#class-activity-importing-data-and-creating-a-tsibble-object-5-min",
    "title": "Plots Trends, and Seasonal Variation",
    "section": "Class Activity: Importing Data and Creating a tsibble Object (5 min)",
    "text": "Class Activity: Importing Data and Creating a tsibble Object (5 min)\nRecall the Google Trends data for the term “chocolate” from the last lesson. The cleaned data are available in the file chocolate.csv. Here are the first few rows of the csv:\n\n\n\nhttps://byuistats.github.io/timeseries/data/chocolate.csv\n\n\n\nImport the Data\nUse the code below to import the chocolate data and convert it into a time series (tsibble) object. You can click on the clipboard icon in the upper right-hand corner of the box below to copy the code.\n\n# load packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\npacman::p_load(\"tsibble\", \"fable\",\n               \"feasts\", \"tsibbledata\",\n               \"fable.prophet\", \"tidyverse\",\n               \"patchwork\", \"rio\")\n\n# read in the data from a csv\nchocolate_month &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/chocolate.csv\")\n\n# define the first date in the time series\nstart_date &lt;- lubridate::ymd(\"2004-01-01\")  \n\n# create a sequence of dates, one month apart, starting with start_date\ndate_seq &lt;- seq(start_date,\n                start_date + months(nrow(chocolate_month)-1),\n                by = \"1 months\")\n\n# create a tibble including variables dates, year, month, value\nchocolate_tibble &lt;- tibble(\n  dates = date_seq,\n  year = lubridate::year(date_seq),        # gets the year part of the date\n  month = lubridate::month(date_seq),      # gets the month\n  value = pull(chocolate_month, chocolate) # gets the value of the ts \n)\n\n# create a tsibble where the index variable is the year/month\nchocolate_month_ts &lt;- chocolate_tibble |&gt;\n  mutate(index = tsibble::yearmonth(dates)) |&gt;\n  as_tsibble(index = index)\n\n# generate the ts plot\nchoc_plot &lt;- autoplot(chocolate_month_ts, .vars = value) +\n  labs(\n    x = \"Month\",\n    y = \"Searches\",\n    title = \"Relative Number of Google Searches for 'Chocolate'\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\nchoc_plot\n\n\n\n\n\n\n\n\n\n\nExplore R commands summarizing time series data\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat does each of the following R commands give us? \n\nhead(chocolate_month_ts, 1)\ntail(chocolate_month_ts, 1)\nguess_frequency(chocolate_month_ts$index)",
    "crumbs": [
      "Lesson 2",
      "Plots Trends, and Seasonal Variation"
    ]
  },
  {
    "objectID": "chapter_1_lesson_2.html#estimating-the-trend-annual-aggregation-10-min",
    "href": "chapter_1_lesson_2.html#estimating-the-trend-annual-aggregation-10-min",
    "title": "Plots Trends, and Seasonal Variation",
    "section": "Estimating the Trend: Annual Aggregation (10 min)",
    "text": "Estimating the Trend: Annual Aggregation (10 min)\nTo help visualize what is happening with a time series, we can simply aggregate the data in the time series to the annual level by computing the mean of the observations in a given year. This can make it easier to spot a trend.\nFor the chocolate data, when we average the data for each year, we get:\n\n\nAggregation\nchocolate_annual_ts &lt;- summarise(\n    index_by(chocolate_month_ts, year), \n    value = mean(value)\n  ) \n\n#chocolate_annual_ts\n\n\n\n\nTable\nchocolate_annual_ts |&gt;\n  as.data.frame() |&gt;\n  concat_partial_table(nrow_head = 6, nrow_tail= 3, decimals = 4) |&gt;\n  display_table()\n\n\n\n\n\nyear\nvalue\n\n\n\n\n2004\n35.5\n\n\n2005\n41.75\n\n\n2006\n41.5833\n\n\n2007\n43.1667\n\n\n2008\n41.6667\n\n\n2009\n43.5\n\n\n⋮\n⋮\n\n\n2021\n55.0833\n\n\n2022\n59.5\n\n\n2023\n60.1667\n\n\n\n\n\n\n\nThe first plot is the time series plot of the raw data, and the second plot is a time series plot of the annual means.\n\n\nShow the code\n# monthly plot\nmp &lt;- autoplot(chocolate_month_ts, .vars = value) +\n  labs(\n    x = \"Month\",\n    y = \"Searches\",\n    title = \"Relative Number of Google Searches for 'Chocolate'\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# yearly plot\nyp &lt;- autoplot(chocolate_annual_ts, .vars = value) +\n  labs(\n    x = \"Year\",\n    y = \"Searches\",\n    title = \"Mean Annual Google Searches for 'Chocolate'\"\n  ) +\n  scale_x_continuous(breaks = seq(2004, max(chocolate_month_ts$year), by = 2)) +\n  theme(plot.title = element_text(hjust = 0.5))\n\nmp / yp\n\n\n\n\n\n\n\n\n\nIf you want to superimpose these plots, it would make sense to align the mean value for the year with the middle of the year. Here is a plot superimposing the annual mean aligned with July 1 (in blue) on the values of the time series (in black).\n\n\nShow the code\nchocolate_annual_ts &lt;- summarise(\n    index_by(chocolate_month_ts, year), \n    value = mean(value)\n  ) |&gt;\n  mutate(index = tsibble::yearmonth( mdy(paste0(\"7/1/\",year)) )) |&gt;\n  as_tsibble(index = index)\n\n# combined plot\nautoplot(chocolate_month_ts, .vars = value) +\n  geom_line(data = chocolate_annual_ts, \n            aes(x = index, y = value), \n            color = \"#56B4E9\") +\n  labs(\n    x = \"Month\",\n    y = \"Searches\",\n    title = \"Relative Number of Google Searches for 'Chocolate'\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do the annually-aggregated data tell us about the trend?\nWhat do you observe in the trend?\nWhat do you suspect is causing this trend?",
    "crumbs": [
      "Lesson 2",
      "Plots Trends, and Seasonal Variation"
    ]
  },
  {
    "objectID": "chapter_1_lesson_2.html#homework-preview-5-min",
    "href": "chapter_1_lesson_2.html#homework-preview-5-min",
    "title": "Plots Trends, and Seasonal Variation",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions",
    "crumbs": [
      "Lesson 2",
      "Plots Trends, and Seasonal Variation"
    ]
  },
  {
    "objectID": "chapter_1_lesson_2.html#homework",
    "href": "chapter_1_lesson_2.html#homework",
    "title": "Plots Trends, and Seasonal Variation",
    "section": "Homework",
    "text": "Homework\n\n\n\n\n\n\nDownload Assignment\n\n\n\n\n homework_1_2.qmd \n\n\nMatching\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nMatching Solutions\n\nVocabulary Matching\n\n\n\n\n\n\n\n1. A figure with time on the horizontal axis and the value of a random variable on the vertical axis\nF. Time Plot\n\n\n2. A systematic change in a time series that does not appear to be periodic\nG. Trend\n\n\n3. Repeated pattern within each year (or any other fixed time period)\nD. Seasonal Variation\n\n\n4. Repeated pattern that does not correspond to some fixed natural period\nA. Cycle\n\n\n5. Observations in which values are related to lagged observations of the same variable\nB. Correlated (Serially Dependent) Data\n\n\n6. Random trend that does not follow a discernible or predictable pattern\nE. Stochastic Trend\n\n\n7. Can be modeled with mathematical functions, facilitating the long-term prediction of the behavior\nC. Deterministic Trend",
    "crumbs": [
      "Lesson 2",
      "Plots Trends, and Seasonal Variation"
    ]
  },
  {
    "objectID": "chapter_1_lesson_4.html",
    "href": "chapter_1_lesson_4.html",
    "title": "Additive Models",
    "section": "",
    "text": "Use R to describe key features of time series data\n\n\nImport CSV data and convert to tsibble format\n\n\n\n\nDecompose time series into trends, seasonal variation, and residuals\n\n\nImplement additive decomposition\nExplain how to remove seasonal variation using an estimate for seasonal component of a time series\nCompute the estimators of seasonal variation for an additive model\nCalculate the random component for an additive model\nCompute a seasonally-adjusted time series based on an additive model",
    "crumbs": [
      "Lesson 4",
      "Additive Models"
    ]
  },
  {
    "objectID": "chapter_1_lesson_4.html#learning-outcomes",
    "href": "chapter_1_lesson_4.html#learning-outcomes",
    "title": "Additive Models",
    "section": "",
    "text": "Use R to describe key features of time series data\n\n\nImport CSV data and convert to tsibble format\n\n\n\n\nDecompose time series into trends, seasonal variation, and residuals\n\n\nImplement additive decomposition\nExplain how to remove seasonal variation using an estimate for seasonal component of a time series\nCompute the estimators of seasonal variation for an additive model\nCalculate the random component for an additive model\nCompute a seasonally-adjusted time series based on an additive model",
    "crumbs": [
      "Lesson 4",
      "Additive Models"
    ]
  },
  {
    "objectID": "chapter_1_lesson_4.html#preparation",
    "href": "chapter_1_lesson_4.html#preparation",
    "title": "Additive Models",
    "section": "Preparation",
    "text": "Preparation\n\nRead Sections 1.5.4-1.5.5 and 1.6",
    "crumbs": [
      "Lesson 4",
      "Additive Models"
    ]
  },
  {
    "objectID": "chapter_1_lesson_4.html#learning-journal-exchange-10-min",
    "href": "chapter_1_lesson_4.html#learning-journal-exchange-10-min",
    "title": "Additive Models",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading your partner’s?\nWhat would you recommend your partner add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 4",
      "Additive Models"
    ]
  },
  {
    "objectID": "chapter_1_lesson_4.html#converting-from-a-data-file-to-a-tsibble-5-min",
    "href": "chapter_1_lesson_4.html#converting-from-a-data-file-to-a-tsibble-5-min",
    "title": "Additive Models",
    "section": "Converting from a Data File to a Tsibble (5 min)",
    "text": "Converting from a Data File to a Tsibble (5 min)\nThis is a demonstration of two ways to convert an Excel or csv data file to a tsibble.\n\ndeaths_df &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/traffic_deaths.xlsx\")\n\n# Method 1: Create date from scratch based on pattern of rows\n# This only works if the data are in ascending order with no missing values\n# Note: This file is not in the right order, so this code gives the wrong tsibble\n# unless you sort the Excel file before proceeding.\nstart_date &lt;- lubridate::ymd(\"2017-01-01\")\ndate_seq &lt;- seq(start_date,\n                start_date + months(nrow(deaths_df)-1),\n                by = \"1 months\")\ndeaths_tibble &lt;- tibble(\n  dates = date_seq,\n  year = lubridate::year(date_seq),\n  month = lubridate::month(date_seq),\n  value = pull(deaths_df, Deaths)\n)\n\n# Method 2: Build using the date information in the Excel file\ndeaths_tibble &lt;- deaths_df |&gt;\n  mutate(\n    date_str = paste(\"1\", Month, Year),\n    dates = dmy(date_str),\n    year = lubridate::year(dates),\n    month = lubridate::month(dates),\n    value = Deaths\n  ) |&gt;\n  dplyr::select(dates, year, month, value)  |&gt; \n  tibble()\n\n# Create the index variable and convert to a tsibble\ndeaths_ts &lt;- deaths_tibble |&gt;\n  mutate(index = tsibble::yearmonth(dates)) |&gt;\n  as_tsibble(index = index) |&gt;\n  dplyr::select(index, dates, year, month, value) |&gt;\n  rename(deaths = value) # rename value to emphasize data context\n\nThis results in a tsibble. The first few rows are given here:\n\n\n# A tsibble: 6 x 5 [1M]\n     index dates       year month deaths\n     &lt;mth&gt; &lt;date&gt;     &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt;\n1 2017 Jan 2017-01-01  2017     1   3034\n2 2017 Feb 2017-02-01  2017     2   2748\n3 2017 Mar 2017-03-01  2017     3   3164\n4 2017 Apr 2017-04-01  2017     4   3238\n5 2017 May 2017-05-01  2017     5   3416\n6 2017 Jun 2017-06-01  2017     6   3492",
    "crumbs": [
      "Lesson 4",
      "Additive Models"
    ]
  },
  {
    "objectID": "chapter_1_lesson_4.html#data-visualizations-5-min",
    "href": "chapter_1_lesson_4.html#data-visualizations-5-min",
    "title": "Additive Models",
    "section": "Data Visualizations (5 min)",
    "text": "Data Visualizations (5 min)\nThe following time plot illustrates the data in this time series.\n\nTime Series Plot\n\nDefault y-axisAdjusted y-axis\n\n\nThis is the plot R creates by default\n\n\nShow the code\nautoplot(deaths_ts, .vars = deaths) +  \n  labs(\n    x = \"Month\",\n    y = \"Traffic Fatalities\",\n    title = \"Traffic Fatalities in the United States (by Month)\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\nThe vertical axis was adjusted in this plot, so it would begin at 0.\n\n\nShow the code\nautoplot(deaths_ts, .vars = deaths) +\n  labs(\n    x = \"Month\",\n    y = \"Traffic Fatalities\",\n    title = \"Traffic Fatalities in the United States (by Month)\"\n  ) +\n  coord_cartesian(ylim = c(0, 4500)) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do you notice?\n\nDoes it seem like there is a trend in the time series?\nIs there evidence of a seasonal effect? If so, when is the number of fatalities particularly high? particularly low?\n\nWhich of the two time plots above is superior? Why?\n\n\n\n\n\nVisualization of Seasonal Effects: Side-by-Side Box Plots\nThese side-by-side box plots illustrate the seasonal effect present in the data.\n\n\nShow the code\nggplot(deaths_ts, aes(x = factor(month), y = deaths)) +\n    geom_boxplot() +\n  labs(\n    x = \"Month Number\",\n    y = \"Deaths\",\n    title = \"Boxplots of Traffic Deaths by Month\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nHow do the positions of the box plots correspond to the times of the year where you noted more (or less) deaths occurring?",
    "crumbs": [
      "Lesson 4",
      "Additive Models"
    ]
  },
  {
    "objectID": "chapter_1_lesson_4.html#computing-the-seasonally-adjusted-series-25-min",
    "href": "chapter_1_lesson_4.html#computing-the-seasonally-adjusted-series-25-min",
    "title": "Additive Models",
    "section": "Computing the Seasonally Adjusted Series (25 min)",
    "text": "Computing the Seasonally Adjusted Series (25 min)\nOur objective is to find an estimate for the time series that does not fluctuate with the seasons. This is called the seasonally adjusted series.\n\nCentered Moving Average\nFirst, we compute the centered moving average, \\(\\hat m_t\\). This computation was explored in detail in the previous lesson. This code can be used to compute the 12-month centered moving average.\n\n# computes the 12-month centered moving average (m_hat)\ndeaths_ts &lt;- deaths_ts |&gt; \n  mutate(\n    m_hat = (\n          (1/2) * lag(deaths, 6)\n          + lag(deaths, 5)\n          + lag(deaths, 4)\n          + lag(deaths, 3)\n          + lag(deaths, 2)\n          + lag(deaths, 1)\n          + deaths\n          + lead(deaths, 1)\n          + lead(deaths, 2)\n          + lead(deaths, 3)\n          + lead(deaths, 4)\n          + lead(deaths, 5)\n          + (1/2) * lead(deaths, 6)\n        ) / 12\n  )\n\nTo emphasize the computation of the centered moving average, the observed data values that were used to find \\(\\hat m_t\\) for December 2017 are shown in green in the table below.\n\n\nEstimated Monthly Additive Effect\nThe centered moving average, \\(\\hat m_t\\), is then used to compute the monthly additive effect, \\(\\hat s_t\\):\n\\[\n  \\hat s_t = x_t - \\hat m_t\n\\]\n\nTable 1: Computation of the Centered Moving Average, \\(\\hat m_t\\), and the Estimated Monthly Additive Effect, \\(\\hat s_t\\)\n\n\n\n\n\nMonth\nDeaths $$x_t$$\n$$ \\hat m $$\n$$ \\hat s $$\n\n\n\n\n2017 Jan\n3034\nNA\n______\n\n\n2017 Feb\n2748\nNA\n______\n\n\n2017 Mar\n3164\nNA\n______\n\n\n2017 Apr\n3238\nNA\n______\n\n\n2017 May\n3416\nNA\n______\n\n\n2017 Jun\n3492\nNA\n______\n\n\n2017 Jul\n3730\n3351.6\n______\n\n\n2017 Aug\n3409\n3350\n______\n\n\n2017 Sep\n3572\n3343.2\n______\n\n\n2017 Oct\n3629\n3326.2\n______\n\n\n2017 Nov\n3408\n3316.5\n______\n\n\n2017 Dec\n3391\n3318.6\n______\n\n\n2018 Jan\n3010\n3312.1\n______\n\n\n2018 Feb\n2734\n3308\n______\n\n\n2018 Mar\n3015\n3311.7\n-296.7\n\n\n2018 Apr\n2979\n3313.2\n-334.2\n\n\n2018 May\n3443\n3307.8\n135.2\n\n\n2018 Jun\n3514\n3292.4\n221.6\n\n\n2018 Jul\n3552\n3281.1\n270.9\n\n\n2018 Aug\n3490\n3270.2\n219.8\n\n\n2018 Sep\n3579\n3259.5\n319.5\n\n\n2018 Oct\n3657\n3261.2\n395.8\n\n\n2018 Nov\n3250\n3264.2\n-14.2\n\n\n2018 Dec\n3181\n3260.5\n-79.5\n\n\n2019 Jan\n2948\n3256.7\n-308.7\n\n\n2019 Feb\n2535\n3262.1\n-727.1\n\n\n2019 Mar\n2956\n3267.1\n-311.1\n\n\n2019 Apr\n3079\n3259.3\n-180.3\n\n\n2019 May\n3417\n3254\n163\n\n\n2019 Jun\n3449\n3257\n192\n\n\n2019 Jul\n3527\n3256.7\n270.3\n\n\n2019 Aug\n3645\n3269\n376\n\n\n2019 Sep\n3543\n3279.1\n263.9\n\n\n2019 Oct\n3506\n3252.8\n253.2\n\n\n2019 Nov\n3274\n3228\n46\n\n\n2019 Dec\n3228\n3248.2\n-20.2\n\n\n2020 Jan\n2895\n3294.3\n-399.3\n\n\n2020 Feb\n2883\n3340.3\n-457.3\n\n\n2020 Mar\n2850\n3384.7\n-534.7\n\n\n2020 Apr\n2555\n3433.5\n-878.5\n\n\n2020 May\n3346\n3481.3\n-135.3\n\n\n2020 Jun\n4004\n3514.8\n489.2\n\n\n2020 Jul\n4078\n3549.9\n528.1\n\n\n2020 Aug\n4199\n3567.7\n631.3\n\n\n2020 Sep\n4053\n3590.7\n462.3\n\n\n2020 Oct\n4169\n3672.1\n496.9\n\n\n2020 Nov\n3757\n3758.2\n-1.2\n\n\n2020 Dec\n3550\n3793.8\n-243.8\n\n\n2021 Jan\n3414\n3804.8\n-390.8\n\n\n2021 Feb\n2792\n3816.6\n-1024.6\n\n\n2021 Mar\n3492\n3831\n-339\n\n\n2021 Apr\n3868\n3853.3\n14.7\n\n\n2021 May\n4098\n3875.5\n222.5\n\n\n2021 Jun\n4107\n3899.8\n207.2\n\n\n2021 Jul\n4240\nNA\nNA\n\n\n2021 Aug\n4320\nNA\nNA\n\n\n2021 Sep\n4276\nNA\nNA\n\n\n2021 Oct\n4482\nNA\nNA\n\n\n2021 Nov\n3977\nNA\nNA\n\n\n2021 Dec\n3914\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWorking with your assigned partner, fill in the missing values of \\(\\hat s_t\\) in the table above.\n\n\n\n\n\n\nSeasonally Adjusted Means\nNext, we need to compute the mean (across years) of \\(\\hat s_t\\) by month. To compute this, it can be convenient to organize the values of \\(\\hat s_t\\) in a table, where the rows give the year and the columns give the month. We will calculate the mean of the \\(\\hat s_t\\) values for each month. We will call this \\(\\bar {\\hat s_t}\\), the unadjusted monthly additive components.\nThe overall mean of these unadjusted monthly additive components \\(\\left( \\bar {\\bar {\\hat s_t}} \\right)\\) will be reasonably close to, but not exactly zero. We adjust these values by subtracting their overall mean, \\(\\bar{\\bar {\\hat s_t}}\\), from from each of them:\n\\[\n  \\bar s_t = \\bar {\\hat s_t} - \\bar{\\bar {\\hat s_t}}\n\\]\nwhere \\(\\bar {\\hat s_t}\\) is the mean of the \\(\\hat s_t\\) values corresponding to month \\(t\\), and \\(\\bar{\\bar {\\hat s_t}}\\) is the mean of the \\(\\bar {\\hat s_t}\\) values.\nThis yields \\(\\bar s_t\\), the seasonally adjusted mean for each month.\n\nTable 2: Computation of \\(\\bar s_t\\)\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\n\n\n2017\nNA\nNA\nNA\nNA\nNA\nNA\n______\n______\n______\n______\n______\n______\n\n\n2018\n______\n______\n-296.7\n-334.2\n135.2\n221.6\n270.9\n219.8\n319.5\n395.8\n-14.2\n-79.5\n\n\n2019\n-308.7\n-727.1\n-311.1\n-180.3\n163\n192\n270.3\n376\n263.9\n253.2\n46\n-20.2\n\n\n2020\n-399.3\n-457.3\n-534.7\n-878.5\n-135.3\n489.2\n528.1\n631.3\n462.3\n496.9\n-1.2\n-243.8\n\n\n2021\n-390.8\n-1024.6\n-339\n14.7\n222.5\n207.2\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nMean\n-350.2\n-695.8\n-370.4\n-344.6\n96.3\n277.5\n361.9\n321.5\n318.6\n362.2\n______\n______\n\n\n$$ \\bar s_t $$\n$$~$$ ______\n$$~$$ ______\n$$~$$ ______\n$$~$$ ______\n$$~$$ ______\n$$~$$ ______\n$$~$$ ______\n$$~$$ ______\n$$~$$ ______\n$$~$$ ______\n$$~$$ ______\n$$~$$ ______\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nThe table above gives the values of \\(\\hat s_t\\). Fill in the missing values in the first two rows of the table above. (Note you already computed these.)\nThe second-to-last row (labeled “Mean”) gives the values of \\(\\bar s_t\\). Fill in the two missing numbers. We will call these 12 means \\(\\bar {\\hat s_t}\\).\nCompute the mean of the \\(\\bar {\\hat s_t}\\) values. Call this value \\(\\bar {\\bar {\\hat s_t}}\\) (This number should be relatively close to 0.)\nSubtract the grand mean \\(\\bar {\\bar {\\hat s_t}}\\) from the column means \\(\\bar {\\hat s_t}\\) to get \\(\\bar s_t\\), the seasonally adjusted mean for month \\(t\\). (Note that the mean of the \\(\\bar s_t\\) values is 0.)\n\n\\[ \\bar s_t = \\bar {\\hat s_t} - \\bar {\\bar {\\hat s_t}} \\]\n\\(\\bar s_t\\) is the seasonally adjusted mean for month \\(t\\).\n\n\n\n\n\nComputing the Random Component and the Seasonally Adjusted Series\nWe calculate the random component by subtracting the trend and seasonally adjusted mean from the time series:\n\\[\n  \\text{random component} = x_t - \\hat m_t - \\bar s_t\n\\]\nThe seasonally adjusted series is computed by subtracting \\(\\bar s_t\\) from each of the observed values:\n\\[\n  \\text{seasonally adjusted series} = x_t - \\bar s_t\n\\]\nCompute the values missing from the table below.\n\nTable 3: Computation of \\(\\bar s\\), the Random Component, and the Seasonally Adjusted Time Series\n\n\n\n\n\nMonth\nDeaths $$x_t$$\n$$ \\hat m $$\n$$ \\hat s $$\n$$ \\bar s $$\nRandom\nSeasonally Adjusted $$x_t$$\n\n\n\n\n2017 Jan\n3034\nNA\nNA\n______\n______\n______\n\n\n2017 Feb\n2748\nNA\nNA\n______\n______\n______\n\n\n2017 Mar\n3164\nNA\nNA\n______\n______\n______\n\n\n2017 Apr\n3238\nNA\nNA\n______\n______\n______\n\n\n2017 May\n3416\nNA\nNA\n______\n______\n______\n\n\n2017 Jun\n3492\nNA\nNA\n______\n______\n______\n\n\n2017 Jul\n3730\n3351.6\n378.4\n______\n______\n______\n\n\n2017 Aug\n3409\n3350\n59\n______\n______\n______\n\n\n2017 Sep\n3572\n3343.2\n228.8\n______\n______\n______\n\n\n2017 Oct\n3629\n3326.2\n302.8\n______\n______\n______\n\n\n2017 Nov\n3408\n3316.5\n91.5\n______\n______\n______\n\n\n2017 Dec\n3391\n3318.6\n72.4\n______\n______\n______\n\n\n2018 Jan\n3010\n3312.1\n-302.1\n______\n______\n______\n\n\n2018 Feb\n2734\n3308\n-574\n______\n______\n______\n\n\n2018 Mar\n3015\n3311.7\n-296.7\n-365.3\n68.6\n3380.3\n\n\n2018 Apr\n2979\n3313.2\n-334.2\n-339.6\n5.4\n3318.6\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮",
    "crumbs": [
      "Lesson 4",
      "Additive Models"
    ]
  },
  {
    "objectID": "chapter_1_lesson_4.html#class-activity-computing-the-additive-decomposition-in-r-10-min",
    "href": "chapter_1_lesson_4.html#class-activity-computing-the-additive-decomposition-in-r-10-min",
    "title": "Additive Models",
    "section": "Class Activity: Computing the Additive Decomposition in R (10 min)",
    "text": "Class Activity: Computing the Additive Decomposition in R (10 min)\n\nThis code calculates the decomposition, including the seasonally adjusted time series, beginning with the tsibble deaths_ts.\n\nTable 4: First Few Rows of the Decomposition of the Traffic Deaths Time Series\n\n# Compute the additive decomposition for deaths_ts\ndeaths_decompose &lt;- deaths_ts |&gt;\n  model(feasts::classical_decomposition(deaths,\n          type = \"add\"))  |&gt;\n  components()\n\nFirst few rows of the deaths_decompose tsibble:\n\n\n\n\n\n.model\nindex\ndeaths\ntrend\nseasonal\nrandom\nseason_adjust\n\n\n\n\nfeasts::classical_decomposition(deaths, type = \"add\")\n2017 Jan\n3034\nNA\n-345.223\nNA\n3379.223\n\n\nfeasts::classical_decomposition(deaths, type = \"add\")\n2017 Feb\n2748\nNA\n-690.775\nNA\n3438.775\n\n\nfeasts::classical_decomposition(deaths, type = \"add\")\n2017 Mar\n3164\nNA\n-365.348\nNA\n3529.348\n\n\nfeasts::classical_decomposition(deaths, type = \"add\")\n2017 Apr\n3238\nNA\n-339.567\nNA\n3577.567\n\n\nfeasts::classical_decomposition(deaths, type = \"add\")\n2017 May\n3416\nNA\n101.371\nNA\n3314.629\n\n\nfeasts::classical_decomposition(deaths, type = \"add\")\n2017 Jun\n3492\nNA\n282.496\nNA\n3209.504\n\n\nfeasts::classical_decomposition(deaths, type = \"add\")\n2017 Jul\n3730\n3351.583\n366.944\n11.473\n3363.056\n\n\nfeasts::classical_decomposition(deaths, type = \"add\")\n2017 Aug\n3409\n3350\n326.527\n-267.527\n3082.473\n\n\nfeasts::classical_decomposition(deaths, type = \"add\")\n2017 Sep\n3572\n3343.208\n323.652\n-94.86\n3248.348\n\n\nfeasts::classical_decomposition(deaths, type = \"add\")\n2017 Oct\n3629\n3326.208\n367.173\n-64.381\n3261.827\n\n\nfeasts::classical_decomposition(deaths, type = \"add\")\n2017 Nov\n3408\n3316.542\n35.506\n55.952\n3372.494\n\n\nfeasts::classical_decomposition(deaths, type = \"add\")\n2017 Dec\n3391\n3318.583\n-62.754\n135.171\n3453.754\n\n\nfeasts::classical_decomposition(deaths, type = \"add\")\n2018 Jan\n3010\n3312.083\n-345.223\n43.14\n3355.223\n\n\nfeasts::classical_decomposition(deaths, type = \"add\")\n2018 Feb\n2734\n3308.042\n-690.775\n116.734\n3424.775\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nHow do these values from R compare to the ones you computed above?\n\n\n\n\n\nShow the code\nautoplot(deaths_decompose)\n\n\n\n\n\n\n\n\n\nThe figure below illustrates the original time series (in black), the centered moving average \\(\\hat m_t\\) (in blue), and the seasonally adjusted series (in red).\n\n\nShow the code\ndeaths_decompose |&gt;\n  ggplot() +\n  geom_line(data = deaths_decompose, aes(x = index, y = deaths), color = \"black\") +\n  geom_line(data = deaths_decompose, aes(x = index, y = season_adjust), color = \"#D55E00\") +\n  geom_line(data = deaths_decompose, aes(x = index, y = trend), color = \"#0072B2\") +\n  labs(\n    x = \"Month\",\n    y = \"Traffic Fatalities\",\n    title = \"Traffic Fatalities in the United States (by Month)\"\n  ) +\n  coord_cartesian(ylim = c(0,4500)) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nDo you observe a trend in the time series?\n\nWhat does this trend suggest?\n\nIn February 2020, there is a spike in the seasonally adjusted time series. Refer to the values in the original time series to explain why this occurred.\nIn April 2020, there is a dip in the seasonally adjusted time series. Refer to the values in the original time series to explain why this occurred.\n\nWhat would explain this phenomenon?",
    "crumbs": [
      "Lesson 4",
      "Additive Models"
    ]
  },
  {
    "objectID": "chapter_1_lesson_4.html#homework-preview-5-min",
    "href": "chapter_1_lesson_4.html#homework-preview-5-min",
    "title": "Additive Models",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions",
    "crumbs": [
      "Lesson 4",
      "Additive Models"
    ]
  },
  {
    "objectID": "chapter_1_lesson_4.html#homework",
    "href": "chapter_1_lesson_4.html#homework",
    "title": "Additive Models",
    "section": "Homework",
    "text": "Homework\n\n\n\n\n\n\nDownload Assignment\n\n\n\n\n homework_1_4.qmd \n Tables-Handout-Excel \n\n\n\nSolutions\n\n Tables-Handout-Excel-key \n\nTeam Activity: Computational Practice\nSolutions for the computations from this lesson\n\nTable 3: (Solutions)\n\n\n\n\n\nMonth\nDeaths $$x_t$$\n$$ \\hat m_t $$\n$$ \\hat s_t $$\n$$ \\bar s_t $$\nRandom\nSeasonally Adjusted $$x_t$$\n\n\n\n\n2017 Jan\n3034\nNA\nNA\n-345.2\nNA\n3379.2\n\n\n2017 Feb\n2748\nNA\nNA\n-690.8\nNA\n3438.8\n\n\n2017 Mar\n3164\nNA\nNA\n-365.3\nNA\n3529.3\n\n\n2017 Apr\n3238\nNA\nNA\n-339.6\nNA\n3577.6\n\n\n2017 May\n3416\nNA\nNA\n101.4\nNA\n3314.6\n\n\n2017 Jun\n3492\nNA\nNA\n282.5\nNA\n3209.5\n\n\n2017 Jul\n3730\n3351.6\n378.4\n366.9\n11.5\n3363.1\n\n\n2017 Aug\n3409\n3350\n59\n326.5\n-267.5\n3082.5\n\n\n2017 Sep\n3572\n3343.2\n228.8\n323.7\n-94.9\n3248.3\n\n\n2017 Oct\n3629\n3326.2\n302.8\n367.2\n-64.4\n3261.8\n\n\n2017 Nov\n3408\n3316.5\n91.5\n35.5\n56\n3372.5\n\n\n2017 Dec\n3391\n3318.6\n72.4\n-62.8\n135.2\n3453.8\n\n\n2018 Jan\n3010\n3312.1\n-302.1\n-345.2\n43.1\n3355.2\n\n\n2018 Feb\n2734\n3308\n-574\n-690.8\n116.7\n3424.8\n\n\n2018 Mar\n3015\n3311.7\n-296.7\n-365.3\n68.6\n3380.3\n\n\n2018 Apr\n2979\n3313.2\n-334.2\n-339.6\n5.4\n3318.6\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n\n\n\n\n\n\n\nTable 2: (Solutions)\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\n\n\n2017\nNA\nNA\nNA\nNA\nNA\nNA\n378.4\n59\n228.8\n302.8\n91.5\n72.4\n\n\n2018\n-302.1\n-574\n-296.7\n-334.2\n135.2\n221.6\n270.9\n219.8\n319.5\n395.8\n-14.2\n-79.5\n\n\n2019\n-308.7\n-727.1\n-311.1\n-180.3\n163\n192\n270.3\n376\n263.9\n253.2\n46\n-20.2\n\n\n2020\n-399.3\n-457.3\n-534.7\n-878.5\n-135.3\n489.2\n528.1\n631.3\n462.3\n496.9\n-1.2\n-243.8\n\n\n2021\n-390.8\n-1024.6\n-339\n14.7\n222.5\n207.2\nNA\nNA\nNA\nNA\nNA\nNA\n\n\nMean\n-350.2\n-695.8\n-370.4\n-344.6\n96.3\n277.5\n361.9\n321.5\n318.6\n362.2\n30.5\n-67.8\n\n\n$$ \\bar s_t $$\n-345.2\n-690.8\n-365.4\n-339.6\n101.3\n282.5\n366.9\n326.5\n323.6\n367.2\n35.5\n-62.8",
    "crumbs": [
      "Lesson 4",
      "Additive Models"
    ]
  },
  {
    "objectID": "chapter_1_lesson_5.html",
    "href": "chapter_1_lesson_5.html",
    "title": "Multiplicative Models",
    "section": "",
    "text": "Decompose time series into trends, seasonal variation, and residuals\n\n\nExplain the differences between additive and multiplicative models\nImplement multiplicative decomposition\nCompute the estimators of seasonal variation for a multiplicative model\nCalculate the random component for a multiplicative model\nCompute a seasonally-adjusted time series based on a multiplicative model",
    "crumbs": [
      "Lesson 5",
      "Multiplicative Models"
    ]
  },
  {
    "objectID": "chapter_1_lesson_5.html#learning-outcomes",
    "href": "chapter_1_lesson_5.html#learning-outcomes",
    "title": "Multiplicative Models",
    "section": "",
    "text": "Decompose time series into trends, seasonal variation, and residuals\n\n\nExplain the differences between additive and multiplicative models\nImplement multiplicative decomposition\nCompute the estimators of seasonal variation for a multiplicative model\nCalculate the random component for a multiplicative model\nCompute a seasonally-adjusted time series based on a multiplicative model",
    "crumbs": [
      "Lesson 5",
      "Multiplicative Models"
    ]
  },
  {
    "objectID": "chapter_1_lesson_5.html#preparation",
    "href": "chapter_1_lesson_5.html#preparation",
    "title": "Multiplicative Models",
    "section": "Preparation",
    "text": "Preparation\n\nReview Sections 1.5.1-1.5.3",
    "crumbs": [
      "Lesson 5",
      "Multiplicative Models"
    ]
  },
  {
    "objectID": "chapter_1_lesson_5.html#learning-journal-exchange-10-min",
    "href": "chapter_1_lesson_5.html#learning-journal-exchange-10-min",
    "title": "Multiplicative Models",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading your partner’s?\nWhat would you recommend your partner add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 5",
      "Multiplicative Models"
    ]
  },
  {
    "objectID": "chapter_1_lesson_5.html#class-activity-comparing-models-in-the-textbook-versus-r-2-min",
    "href": "chapter_1_lesson_5.html#class-activity-comparing-models-in-the-textbook-versus-r-2-min",
    "title": "Multiplicative Models",
    "section": "Class Activity: Comparing Models in the Textbook Versus R (2 min)",
    "text": "Class Activity: Comparing Models in the Textbook Versus R (2 min)\nBoth the textbook and R use the same model in the additive case:\n\\[\n  x_t = m_t + s_t + z_t\n\\]\nHowever, there is a discrepancy in the definitions for the mulitplicative models. The textbook defines the multiplicative model as\n\\[\n  x_t = m_t \\cdot s_t + z_t\n\\] but R defines the multiplicative model as\n\\[\n  x_t = m_t \\cdot s_t \\cdot z_t\n\\] You can investigate R’s definition by executing this command in RStudio.\n\n?classical_decomposition",
    "crumbs": [
      "Lesson 5",
      "Multiplicative Models"
    ]
  },
  {
    "objectID": "chapter_1_lesson_5.html#class-activity-exploring-simulated-time-series-data-10-min",
    "href": "chapter_1_lesson_5.html#class-activity-exploring-simulated-time-series-data-10-min",
    "title": "Multiplicative Models",
    "section": "Class Activity: Exploring Simulated Time Series Data (10 min)",
    "text": "Class Activity: Exploring Simulated Time Series Data (10 min)\nSo far, you have learned how to estimate a trend using aggregated data (i.e., an annual average) or a moving average. We will compute the seasonal effect and use this to get the random component.\n\nAdditive Model\nThe code hidden below simulates 10 years of a monthly time series with a linear trend and seasonal variation based on an additive model. Because the data are simulated, we know exactly which functions were used to create it, and we can observe what happens when we decompose this function.\n\nTable 1: Simulated Data (Additive Model)\n\n\nShow the code\n# Set random seed for reproducibility\nset.seed(20) \n\n# Set parameters & initialize vectors\nnum_years &lt;- 10\nn &lt;- 12 * num_years\nsigma &lt;- .75\na &lt;- 0.05\nb &lt;- 1\nc &lt;- 0.5\ntrend &lt;- seasonal &lt;- x_t &lt;- rep(0,n)\ntime_seq &lt;- seq(1,n)\n\n# Generate correlated error terms\nw &lt;- rnorm(n + 4, 0, 1)\nz = w + lead(w,1) + lead(w,2) + lead(w,3) + lead(w,4)\nz  = head(z, n) / 2\n\n# Get date\nyear_seq &lt;- lubridate::year(today()) - num_years  + (time_seq - 1) %/% 12\nmonth_seq &lt;- (time_seq - 1) %% 12 + 1\ndate_seq &lt;- ymd(paste0(year_seq,\"-\",month_seq,\"-01\"))\n\n# Get data\nfor (t in 1:n) {\n  trend[t] &lt;- a * t + 10\n  seasonal[t] &lt;- b * sin(t / 12 * 2 * pi * 1)  + c * cos(t / 12 * 2 * pi * 3)\n  x_t[t] &lt;- trend[t] + seasonal[t] + z[t]\n}\n\nx_df &lt;- data.frame(x_t = x_t, trend = trend, seasonal = seasonal)\n\nstart_year &lt;- lubridate::year(today()) - num_years\nstart_date &lt;- lubridate::ymd(paste0(start_year,\"-01-01\"))\n\n# start_date &lt;- lubridate::ymd(\"1958-01-01\")\ndate_seq &lt;- seq(start_date,\n    start_date + months(nrow(x_df)-1),\n    by = \"1 months\")\n\nx_df_ts &lt;- x_df |&gt;\n  mutate(\n    date = date_seq,\n    month = tsibble::yearmonth(date)\n  ) |&gt;\n  select(date, month, trend, seasonal, x_t) |&gt; \n  as_tsibble(index = month)\n\n\n\n\n\n\n\nDate\nMonth\nTrend, $$m_t$$\nSeasonal, $$s_t$$\nData, $$x_t$$\n\n\n\n\n2015-01-01\n2015 Jan\n10.05\n0.5\n10.842\n\n\n2015-02-01\n2015 Feb\n10.1\n0.366\n10.461\n\n\n2015-03-01\n2015 Mar\n10.15\n1\n9.993\n\n\n2015-04-01\n2015 Apr\n10.2\n1.366\n9.082\n\n\n2015-05-01\n2015 May\n10.25\n0.5\n8.701\n\n\n2015-06-01\n2015 Jun\n10.3\n-0.5\n7.697\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n2024-11-01\n2024 Nov\n15.95\n-0.5\n16.264\n\n\n2024-12-01\n2024 Dec\n16\n0.5\n18.165\n\n\n\n\n\n\n\nThe code above has generated simulated data, where the trend is linear with equation\n\\[\n  m_t = \\frac{t}{20}\n\\]\nand the seasonal effect follows the function\n\\[\n  s_t = \\sin \\left( \\frac{t\\pi}{6} \\right) + \\frac{1}{2}\\cos\\left(\\frac{t \\pi}{18} \\right)\n\\]\nLetting \\(t\\) represent the month number across 10 years, we simulate a time series. Click on the tabs below to compare the actual construction of the time series (using the components generated in the code above) to the decomposition in R.\n\nActual Construction (Additive)Decomposition (Additive)\n\n\nHere is a plot of the components of the simulated data.\n\n\nShow the code\ntrend_plot &lt;- ggplot(x_df_ts, aes(x=month, y=trend)) + \n  geom_line() +\n  labs(\n    title=\"Plot of Trend\", \n    x=\"Month\", \n    y=\"Trend\"\n    ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\nseasonal_plot &lt;- ggplot(x_df_ts, aes(x=month, y=seasonal)) + \n  geom_line() +\n  labs(\n    title=\"Plot of Seasonal Effect\", \n    x=\"Month\", \n    y=\"Seasonal\"\n    ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\nerror_plot &lt;- ggplot(x_df_ts, aes(x = month, y = x_t - trend - seasonal)) + \n  geom_line() +\n  labs(\n    title=\"Plot of Random Error Term\", \n    x=\"Month\", \n    y=\"Random\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\nx_plot &lt;- ggplot(x_df_ts, aes(x=month, y=x_t)) + \n  geom_line() +\n  labs(\n    title=\"Plot of Simulated Time Series\", \n    x=\"Month\", \n    y=\"$$x_t$$\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\nx_plot &lt;- x_plot  + labs(title = \"True (Simulated) Values\", x = NULL)\ntrend_plot &lt;- trend_plot + labs(title = NULL, x = NULL)\nseasonal_plot &lt;- seasonal_plot + labs(title = NULL, x = NULL)\nerror_plot &lt;- error_plot + labs(title = NULL)\n\nx_plot / trend_plot / seasonal_plot / error_plot \n\n\n\n\n\n\n\n\n\n\n\nNow, we use R to decompose the time series \\(\\{x_t\\}\\).\n\n\nShow the code\nx_decompose &lt;- x_df_ts |&gt;\n    model(feasts::classical_decomposition(x_t,\n        type = \"add\"))  |&gt;\n    components()\n\nautoplot(x_decompose)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nHow does the (estimated) decomposition compare to the theoretical values above?\n\nHow well is the trend estimated?\nHow well is the seasonal effect estimated?\nHow well is the random effect estimated?\n\nMake changes to the simulated data and observe the effect on the plots\n\n\n\n\n\n\nMultiplicative Model\nWe now simulate data and apply R’s multiplicative model. This implies that the error term, \\(z_t\\), has a mean of 1, rather than 0.\n\nTable 2: Simulated Data (Multiplicative Model)\n\n\nShow the code\n# Set random seed for reproducibility\nset.seed(123) \n\n# Set parameters & initialize vectors\nnum_years &lt;- 10\nn &lt;- 12 * num_years\nsigma &lt;- .75\na &lt;- 0.03\nb &lt;- 1\nc &lt;- 0.5 \ntrend &lt;- seasonal &lt;- x_t &lt;- rep(0,n)\ntime_seq &lt;- seq(1,n)\n\n# Generate correlated error terms\nw &lt;- rnorm(n + 4, 0.2, 0.1) # Changed to a mean of 1 and sd of 0.03\nz = w + lead(w,1) + lead(w,2) + lead(w,3) + lead(w,4)\nz  = head(z, n)\n\n# Get date\nyear_seq &lt;- lubridate::year(today()) - num_years  + (time_seq - 1) %/% 12\nmonth_seq &lt;- (time_seq - 1) %% 12 + 1\ndate_seq &lt;- ymd(paste0(year_seq,\"-\",month_seq,\"-01\"))\n\n# Get data\nfor (t in 1:n) {\n  trend[t] &lt;- exp(a * t)\n  seasonal[t] &lt;- exp( b * sin(t / 12 * 2 * pi * 1)  + c * cos(t / 12 * 2 * pi * 3) + 1 )\n  x_t[t] &lt;- trend[t] * seasonal[t] * z[t] # Note R's definition of the mult. model\n}\n\nx_df &lt;- data.frame(x_t = x_t, trend = trend, seasonal = seasonal)\n\nstart_year &lt;- lubridate::year(today()) - num_years\nstart_date &lt;- lubridate::ymd(paste0(start_year,\"-01-01\"))\n\n# start_date &lt;- lubridate::ymd(\"1958-01-01\")\ndate_seq &lt;- seq(start_date,\n    start_date + months(nrow(x_df)-1),\n    by = \"1 months\")\n\nx_df_ts &lt;- x_df |&gt;\n  mutate(\n    date = date_seq,\n    month = tsibble::yearmonth(date)\n  ) |&gt;\n  select(date, month, trend, seasonal, x_t) |&gt;\n  as_tsibble(index = month)\n\n\n\n\n\n\n\nDate\nMonth\nTrend, $$m_t$$\nSeasonal, $$s_t$$\nData, $$x_t$$\n\n\n\n\n2015-01-01\n2015 Jan\n1.03\n4.482\n5.065\n\n\n2015-02-01\n2015 Feb\n1.062\n3.92\n5.512\n\n\n2015-03-01\n2015 Mar\n1.094\n7.389\n11.266\n\n\n2015-04-01\n2015 Apr\n1.127\n10.655\n13.348\n\n\n2015-05-01\n2015 May\n1.162\n4.482\n5.391\n\n\n2015-06-01\n2015 Jun\n1.197\n1.649\n1.93\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n2024-11-01\n2024 Nov\n35.517\n1.649\n39.853\n\n\n2024-12-01\n2024 Dec\n36.598\n4.482\n121.366\n\n\n\n\n\n\n\nThe code above simulated data, where the trend is exponential with equation\n\\[\n  m_t = e^{0.03 t}\n\\]\nand the seasonal effect follows the function\n\\[\n  s_t = \\sin \\left( \\frac{t\\pi}{6} \\right) + \\frac{1}{2}\\cos\\left(\\frac{t \\pi}{18} \\right) + 1\n\\]\nLetting \\(t\\) represent the month number across 10 years, we simulate a time series with multiplicative effects. Click on the tabs below to compare the actual construction of the time series (using the components generated in the code above) to the decomposition in R.\n\nActual Construction (Multiplicative)Decomposition (Multiplicative)\n\n\nHere is a plot of the components of the simulated data.\n\n\nShow the code\ntrend_plot &lt;- ggplot(x_df_ts, aes(x=month, y=trend)) + \n  geom_line() +\n  labs(\n    title=\"Plot of Trend\", \n    x=\"Month\", \n    y=\"Trend\"\n    ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\nseasonal_plot &lt;- ggplot(x_df_ts, aes(x=month, y=seasonal)) + \n  geom_line() +\n  labs(\n    title=\"Plot of Seasonal Effect\", \n    x=\"Month\", \n    y=\"Seasonal\"\n    ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\nerror_plot &lt;- ggplot(x_df_ts, aes(x = month, y = x_t / trend / seasonal)) + \n  geom_line() +\n  labs(\n    title=\"Plot of Random Error Term\", \n    x=\"Month\", \n    y=\"Random\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\nx_plot &lt;- ggplot(x_df_ts, aes(x=month, y=x_t)) + \n  geom_line() +\n  labs(\n    title=\"Plot of Simulated Time Series\", \n    x=\"Month\", \n    y=\"x_t\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\nx_plot &lt;- x_plot  + labs(title = \"True (Simulated) Values\", x = NULL)\ntrend_plot &lt;- trend_plot + labs(title = NULL, x = NULL)\nseasonal_plot &lt;- seasonal_plot + labs(title = NULL, x = NULL)\nerror_plot &lt;- error_plot + labs(title = NULL)\n\nx_plot / trend_plot / seasonal_plot / error_plot \n\n\n\n\n\n\n\n\n\n\n\nNow, we use R to decompose the time series \\(\\{x_t\\}\\).\n\n\nShow the code\nx_decompose &lt;- x_df_ts |&gt;\n    model(feasts::classical_decomposition(x_t,\n        type = \"mult\"))  |&gt;\n    components()\n\nautoplot(x_decompose)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nHow does the (estimated) decomposition compare to the theoretical values above?\n\nHow well is the trend estimated?\nHow well is the seasonal effect estimated?\nHow well is the random effect estimated?\n\nMake changes to the simulated data and observe the effect on the plots\n\n\n\n\n\n\nWhich Model Should I Use: Additive or Multiplicative?\nCompare the following two time series.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nComplete a table like the following in your Learning Journal to compare characteristics of these two time series. Be sure to include a sketch of the respective time plots.\n\n\n\n\n\n\n\n\n\nRexburg Temperature\nS&P 500 Closing Price\n\n\n\n\nDeterministic or stochastic trend?\n\n\n\n\nIs there a seasonal effect?\n\n\n\n\nIs there evidence of cycles?\n\n\n\n\nDoes the variation get bigger over time?\n\n\n\n\nAdditive or multiplicative model?",
    "crumbs": [
      "Lesson 5",
      "Multiplicative Models"
    ]
  },
  {
    "objectID": "chapter_1_lesson_5.html#small-group-activity-apples-quarterly-revenue-30-min",
    "href": "chapter_1_lesson_5.html#small-group-activity-apples-quarterly-revenue-30-min",
    "title": "Multiplicative Models",
    "section": "Small Group Activity: Apple’s Quarterly Revenue (30 min)",
    "text": "Small Group Activity: Apple’s Quarterly Revenue (30 min)\nThe code below imports and plots the quarterly revenue for Apple Inc. (in billions of U.S. dollars).\n\n\nShow the code\napple_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/apple_revenue.csv\") |&gt;\n  mutate(\n    dates = mdy(date),\n    year = lubridate::year(dates),\n    quarter = lubridate::quarter(dates),\n    value = revenue_billions\n  ) |&gt;\n  dplyr::select(dates, year, quarter, value)  |&gt; \n  arrange(dates) |&gt;\n  mutate(index = tsibble::yearquarter(dates)) |&gt;\n  as_tsibble(index = index) |&gt;\n  dplyr::select(index, dates, year, quarter, value) |&gt;\n  rename(revenue = value) # rename value to emphasize data context\n\napple_ts |&gt;\n  autoplot(.vars = revenue) +\n  labs(\n    x = \"Quarter\",\n    y = \"Apple Revenue, Billions $US\",\n    title = \"Apple's Quarterly Revenue, Billions of U.S. Dollars\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do you notice?\n\nDoes it seem like there is a trend in the time series?\nIs there evidence of a seasonal effect? If so, during which quarter(s) are the revenues particularly high? particularly low?\nCan you attribute a reason for this behavior?\n\nIs an additive or multiplicative model more appropriate? Why?\n\n\n\nWe want to find the seasonally adjusted series for a multiplicative model. This is a multi-step process.\n\nCentered Moving Average\nFirst, we compute the centered moving average, \\(\\hat m_t\\).\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWrite a mutate statement that will compute the 4-quarter moving average for the variable “revenue” in the tsibble apple_ts. You can use the mutate statement from Chapter 1 Lesson 4 as a starting point.\n\n\n# computes the 4-quarter centered moving average (m_hat)\napple_ts &lt;- apple_ts |&gt; \n  mutate(\n    m_hat = \n      # Your code goes here\n  )\n\n\n\nTo emphasize the computation of the centered moving average, the observed data values that were used to find \\(\\hat m_t\\) for the first quarter of 2007 are shown in green in the table below.\n\n\nEstimated Quarterly Multiplicative Effect\nThe centered moving average, \\(\\hat m_t\\), is then used to compute the quarterly multiplictive effect, \\(\\hat s_t\\):\n\\[\n  \\hat s_t = \\dfrac{ x_t }{ \\hat m_t }\n\\]\n\nTable 3: Computation of the Centered Moving Average, \\(\\hat m_t\\), and the Estimated Quarterly Multiplicative Effect, \\(\\hat s_t\\)\n\n\n\n\n\nquarter\nRevenue $$x_t$$\n$$ \\hat m_t $$\n$$ \\hat s_t $$\n\n\n\n\n2005 Q1\n1.24\nNA\n______\n\n\n2005 Q2\n0.48\nNA\n______\n\n\n2005 Q3\n1.24\n1.315\n______\n\n\n2005 Q4\n1.71\n______\n______\n\n\n2006 Q1\n2.42\n1.829\n______\n\n\n2006 Q2\n1.71\n______\n______\n\n\n2006 Q3\n1.71\n2.251\n______\n\n\n2006 Q4\n2.19\n______\n0.808\n\n\n2007 Q1\n4.37\n3.136\n1.393\n\n\n2007 Q2\n3.42\n3.469\n0.986\n\n\n2007 Q3\n3.42\n3.956\n0.864\n\n\n2007 Q4\n3.14\n4.474\n0.702\n\n\n2008 Q1\n7.32\n4.801\n1.525\n\n\n2008 Q2\n4.61\n5.87\n0.785\n\n\n⋮\n⋮\n⋮\n⋮\n\n\n2022 Q4\n90.146\n96.579\n0.933\n\n\n2023 Q1\n117.154\n96.128\n1.219\n\n\n2023 Q2\n94.836\n95.902\n0.989\n\n\n2023 Q3\n81.797\nNA\nNA\n\n\n2023 Q4\n89.498\nNA\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWorking with your assigned partner, fill in the missing values of \\(\\hat m_t\\) in the table above.\nThen, find the missing values of \\(\\hat s_t\\).\n\n\n\n\n\n\nSeasonally Adjusted Factors\nNext, we need to compute the mean (across years) of \\(\\hat s_t\\) by quarter. To help us calculate this, it can be convenient to organize the values of \\(\\hat s_t\\) in a table, where the rows give the year and the columns give the quarter.\nThe overall mean of these means will be reasonably close to, but not exactly one. We adjust these values by dividing the quarterly means by the overall mean.\n\nTable 4: Computation of the Seasonally Adjusted Factors, \\(\\bar s_t\\)\n\n\n\n\n\nYear\nQ1\nQ2\nQ3\nQ4\n\n\n\n\n2005\nNA\nNA\n______\n______\n\n\n2006\n______\n______\n______\n0.808\n\n\n2007\n1.393\n0.986\n0.864\n0.702\n\n\n2008\n1.525\n0.785\n0.683\n1.325\n\n\n2009\n1.182\n0.781\n0.834\n1.005\n\n\n2010\n1.193\n0.912\n0.849\n0.963\n\n\n2011\n1.106\n0.961\n0.999\n0.734\n\n\n2012\n1.328\n1.141\n0.865\n0.828\n\n\n2013\n1.249\n1.019\n0.819\n0.856\n\n\n2014\n1.301\n1.012\n0.783\n0.818\n\n\n2015\n1.367\n1.013\n0.847\n0.891\n\n\n2016\n1.355\n0.928\n0.781\n0.855\n\n\n2017\n1.412\n0.935\n0.776\n0.864\n\n\n2018\n1.405\n0.939\n0.808\n0.968\n\n\n2019\n1.303\n0.894\n0.816\n0.956\n\n\n2020\n1.356\n0.851\n0.84\n0.835\n\n\n2021\n1.326\n1.005\n0.875\n0.872\n\n\n2022\n1.282\n0.995\n0.849\n0.933\n\n\n2023\n1.219\n0.989\nNA\nNA\n\n\nMean\n______\n______\n______\n______\n\n\n$$ \\bar s_t $$\n$$~$$ ______\n$$~$$ ______\n$$~$$ ______\n$$~$$ ______\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nThe table above gives the values of \\(\\hat s_t\\). Fill in the missing values in the first two rows of the table above. (Note you already computed these.)\nThe second-to-last row (labeled “Mean”) gives the values of \\(\\bar s_t\\). Find the mean of the \\(\\hat s_t\\) values for each quarter. Call these means \\(\\bar {\\hat s_t}\\). To simplify your computations, the sum of the values visible in the table above is summarized here:\n\n\n\n\n\n\nQuarter\nQ1\nQ2\nQ3\nQ4\n\n\n\n\nPartial Sum\n22.302\n16.146\n13.288\n15.213\n\n\n\n\n\n\n\n\nCompute the mean of the \\(\\bar {\\hat s_t}\\) values. Call this number \\(\\bar {\\bar {\\hat s_t}}\\) (This number should be relatively close to 1.)\nDivide each of the \\(\\bar {\\hat s_t}\\) values by \\(\\bar {\\bar {\\hat s_t}}\\) to get \\(\\bar s_t\\), the seasonally adjusted factor for quarter \\(t\\). (Note that the mean of the \\(\\bar s_t\\) values will be 1.)\n\n\\[ \\bar s_t = \\frac{ \\left( \\bar {\\hat s_t} \\right) }{ \\left( \\bar {\\bar {\\hat s_t}} \\right) } \\]\n\n\n\n\n\nRandom Component and the Seasonally Adjusted Time Series\nUsing R’s definition of the multiplicative model, we calculate the random component by dividing the values in the time series by the product of the trend and the seasonally adjusted factor:\n\\[\n  \\text{random component} = \\dfrac{ x_t }{ \\hat m_t \\cdot \\bar s_t }\n\\]\nThe seasonally adjusted series is computed by dividing the respective observed values by \\(\\bar s_t\\):\n\\[\n  \\text{seasonally adjusted series} = \\dfrac{ x_t }{ \\bar s_t }\n\\]\nUse these equations to calculate the values missing from the table below. The adjusted seasonal effect \\(\\bar s_t\\) (s_bar) was computed in the last row of the previous table.\n\nTable 5: Computation of the Random Component and the Seasonally Adjusted Time Series\n\n\n\n\n\nQuarter\nRevenue $$x_t$$\n$$ \\hat m_t $$\n$$ \\hat s_t $$\n$$ \\bar s_t $$\nRandom\nSeasonally Adjusted $$x_t$$\n\n\n\n\n2005 Q1\n1.24\nNA\n______\n______\n______\n______\n\n\n2005 Q2\n0.48\nNA\n______\n______\n______\n______\n\n\n2005 Q3\n1.24\n1.315\n______\n______\n______\n______\n\n\n2005 Q4\n1.71\n1.616\n______\n______\n______\n______\n\n\n2006 Q1\n2.42\n1.829\n______\n______\n______\n______\n\n\n2006 Q2\n1.71\n1.947\n______\n______\n______\n______\n\n\n2006 Q3\n1.71\n2.251\n______\n______\n______\n______\n\n\n2006 Q4\n2.19\n2.709\n0.808\n0.905\n0.893\n2.42\n\n\n2007 Q1\n4.37\n3.136\n1.393\n1.314\n1.06\n3.325\n\n\n2007 Q2\n3.42\n3.469\n0.986\n0.947\n1.041\n3.612\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮",
    "crumbs": [
      "Lesson 5",
      "Multiplicative Models"
    ]
  },
  {
    "objectID": "chapter_1_lesson_5.html#class-activity-computing-the-multiplicative-decomposition-in-r-3-min",
    "href": "chapter_1_lesson_5.html#class-activity-computing-the-multiplicative-decomposition-in-r-3-min",
    "title": "Multiplicative Models",
    "section": "Class Activity: Computing the Multiplicative Decomposition in R (3 min)",
    "text": "Class Activity: Computing the Multiplicative Decomposition in R (3 min)\nThe R code below calculates the decomposition, including the seasonally adjusted time series, beginning with the tsibble apple_ts.\n\nTable 6: First Few Rows of the Decomposition of the Apple Revenue Time Series\n\n\nShow the code\napple_decompose &lt;- apple_ts |&gt;\n  model(feasts::classical_decomposition(revenue,\n          type = \"mult\"))  |&gt;\n  components()\n\napple_decompose |&gt;\n  head(8) |&gt;\n  display_table()\n\n\n\n\n\n.model\nindex\nrevenue\ntrend\nseasonal\nrandom\nseason_adjust\n\n\n\n\nfeasts::classical_decomposition(revenue, type = \"mult\")\n2005 Q1\n1.24\nNA\n1.3141184\nNA\n0.9435984\n\n\nfeasts::classical_decomposition(revenue, type = \"mult\")\n2005 Q2\n0.48\nNA\n0.9469622\nNA\n0.5068840\n\n\nfeasts::classical_decomposition(revenue, type = \"mult\")\n2005 Q3\n1.24\n1.31500\n0.8337749\n1.1309596\n1.4872119\n\n\nfeasts::classical_decomposition(revenue, type = \"mult\")\n2005 Q4\n1.71\n1.61625\n0.9051445\n1.1688792\n1.8892010\n\n\nfeasts::classical_decomposition(revenue, type = \"mult\")\n2006 Q1\n2.42\n1.82875\n1.3141184\n1.0069932\n1.8415388\n\n\nfeasts::classical_decomposition(revenue, type = \"mult\")\n2006 Q2\n1.71\n1.94750\n0.9469622\n0.9272269\n1.8057743\n\n\nfeasts::classical_decomposition(revenue, type = \"mult\")\n2006 Q3\n1.71\n2.25125\n0.8337749\n0.9110109\n2.0509132\n\n\nfeasts::classical_decomposition(revenue, type = \"mult\")\n2006 Q4\n2.19\n2.70875\n0.9051445\n0.8932176\n2.4195031\n\n\n\n\n\n\n\n\n\nShow the code\nautoplot(apple_decompose)\n\n\n\n\n\n\n\n\n\nThe figure below illustrates the original time series (in black), the centered moving average \\(\\hat m_t\\) (in blue), and the seasonally adjusted series (in red).\n\n\nShow the code\napple_decompose |&gt;\n  ggplot() +\n  geom_line(data = apple_decompose, aes(x = index, y = revenue), color = \"black\") +\n  geom_line(data = apple_decompose, aes(x = index, y = season_adjust), color = \"#D55E00\") +\n  geom_line(data = apple_decompose, aes(x = index, y = trend), color = \"#0072B2\") +\n  labs(\n    x = \"Quarter\",\n    y = \"Quarterly Revenue, Billions\",\n    title = \"Apple Inc. Quarterly Revenue (in Billions of U.S. Dollars)\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nDo you observe a trend in the time series?\n\nWhat does this trend suggest?\n\nIn what quarter does Apple tend to have the greatest revenue?\n\nWhat would explain this phenomenon?",
    "crumbs": [
      "Lesson 5",
      "Multiplicative Models"
    ]
  },
  {
    "objectID": "chapter_1_lesson_5.html#homework-preview-5-min",
    "href": "chapter_1_lesson_5.html#homework-preview-5-min",
    "title": "Multiplicative Models",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions",
    "crumbs": [
      "Lesson 5",
      "Multiplicative Models"
    ]
  },
  {
    "objectID": "chapter_1_lesson_5.html#homework",
    "href": "chapter_1_lesson_5.html#homework",
    "title": "Multiplicative Models",
    "section": "Homework",
    "text": "Homework\n\n\n\n\n\n\nDownload Assignment\n\n\n\n\n homework_1_5.qmd \n Tables-Handout-Excel \n\n\nMoving Average\n\nThis code can be used to compute the four-quarter centered moving average.\n\n# computes the 4-quarter centered moving average (m_hat)\napple_ts &lt;- apple_ts |&gt; \n  mutate(\n    m_hat = (\n          (1/2) * lag(revenue, 2)\n          + lag(revenue, 1)\n          + revenue\n          + lead(revenue, 1)\n          + (1/2) * lead(revenue, 2)\n        ) / 4\n  )\n\n\nHandout + Tables 4 and 5\n\n Tables-Handout-Excel-key \nSolutions for the computations from this lesson\n\nTable 5: (Solutions)\n\n\n\n\n\nQuarter\nRevenue $$x_t$$\n$$ \\hat m_t $$\n$$ \\hat s_t $$\n$$ \\bar s_t $$\nRandom\nSeasonally Adjusted $$x_t$$\n\n\n\n\n2005 Q1\n1.24\nNA\nNA\n1.314\nNA\n0.944\n\n\n2005 Q2\n0.48\nNA\nNA\n0.947\nNA\n0.507\n\n\n2005 Q3\n1.24\n1.315\n0.943\n0.834\n1.131\n1.487\n\n\n2005 Q4\n1.71\n1.616\n1.058\n0.905\n1.169\n1.889\n\n\n2006 Q1\n2.42\n1.829\n1.323\n1.314\n1.007\n1.842\n\n\n2006 Q2\n1.71\n1.947\n0.878\n0.947\n0.927\n1.806\n\n\n2006 Q3\n1.71\n2.251\n0.76\n0.834\n0.911\n2.051\n\n\n2006 Q4\n2.19\n2.709\n0.808\n0.905\n0.893\n2.42\n\n\n2007 Q1\n4.37\n3.136\n1.393\n1.314\n1.06\n3.325\n\n\n2007 Q2\n3.42\n3.469\n0.986\n0.947\n1.041\n3.612\n\n\n\n\n\n\n\n\n\nTable 4: (Solutions)\n\n\n\n\n\nYear\nQ1\nQ2\nQ3\nQ4\n\n\n\n\n2005\nNA\nNA\n0.943\n1.058\n\n\n2006\n1.323\n0.878\n0.76\n0.808\n\n\n2007\n1.393\n0.986\n0.864\n0.702\n\n\n2008\n1.525\n0.785\n0.683\n1.325\n\n\n2009\n1.182\n0.781\n0.834\n1.005\n\n\n2010\n1.193\n0.912\n0.849\n0.963\n\n\n2011\n1.106\n0.961\n0.999\n0.734\n\n\n2012\n1.328\n1.141\n0.865\n0.828\n\n\n2013\n1.249\n1.019\n0.819\n0.856\n\n\n2014\n1.301\n1.012\n0.783\n0.818\n\n\n2015\n1.367\n1.013\n0.847\n0.891\n\n\n2016\n1.355\n0.928\n0.781\n0.855\n\n\n2017\n1.412\n0.935\n0.776\n0.864\n\n\n2018\n1.405\n0.939\n0.808\n0.968\n\n\n2019\n1.303\n0.894\n0.816\n0.956\n\n\n2020\n1.356\n0.851\n0.84\n0.835\n\n\n2021\n1.326\n1.005\n0.875\n0.872\n\n\n2022\n1.282\n0.995\n0.849\n0.933\n\n\n2023\n1.219\n0.989\nNA\nNA\n\n\nMean\n1.312\n0.946\n0.833\n0.904\n\n\n$$ \\bar s_t $$\n1.314\n0.947\n0.834\n0.905",
    "crumbs": [
      "Lesson 5",
      "Multiplicative Models"
    ]
  },
  {
    "objectID": "chapter_2.html",
    "href": "chapter_2.html",
    "title": "Lessons & Homework",
    "section": "",
    "text": "Chapter 2\n\n\n\n\n\n\nLessons\n\n\n\n\nLesson 1 - Covariance and Correlation\nLesson 2 - Autocorrelation Concepts\nLesson 3 - Exploration of Autocorrelation Concepts\n\n\n\n\n\n\n\n\n\nDownload Homework Assigments\n\n\n\n\n homework_2_1.qmd \n homework_2_2.qmd \n homework_2_3.qmd",
    "crumbs": [
      "Overview",
      "Lessons & Homework"
    ]
  },
  {
    "objectID": "chapter_2_lesson_1_handout.html",
    "href": "chapter_2_lesson_1_handout.html",
    "title": "Covariance and Correlation",
    "section": "",
    "text": "Class Activity: Variance and Standard Deviation\n\n\n\n\n\n\n\n\n\n\n\n\n\nTable 1: Deviations from the mean\n\n\n\n\n\n$$x_t$$\n$$x_t-\\bar x$$\n\n\n\n\n\n\n\n\n6.9\n-2.5\n\n\n\n\n\n\n7.7\n-1.7\n\n\n\n\n\n\n8.1\n-1.3\n\n\n\n\n\n\n10.8\n1.4\n\n\n\n\n\n\n13.5\n4.1\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\nTeam Activity: Computational Practice\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nWith your assigned partner, use this table to compute the values given below.\n\nTable 3: Computational Practice\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$y_t$$\n$$x_t-\\bar x$$\n$$(x_t - \\bar x)^2$$\n$$y_t-\\bar y$$\n$$(y_t-\\bar y)^2$$\n$$(x_t - \\bar x)(y_t-\\bar y)$$\n\n\n\n\n1\n-2.1\n2.8\n-1.9\n3.61\n1\n1\n-1.9\n\n\n2\n-0.2\n2.2\n\n\n\n\n\n\n\n3\n0.8\n0.9\n\n\n\n\n\n\n\n4\n0.4\n2\n\n\n\n\n\n\n\n5\n2.3\n-1\n\n\n\n\n\n\n\n6\n-2.4\n3.9\n\n\n\n\n\n\n\nsum\n-1.2\n10.8\n\n\n\n\n\n\n\n$$~$$\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nUse the table above to determine these values:\n\n\n\n\\(\\bar x =\\)\n\\(\\bar y =\\)\n\n\n\n\n\n\\(s_x =\\)\n\\(s_y =\\)\n\n\n\n\n\n\\(r =\\)\n\\(\\\\cov(x,y) =\\)\n\n\n\nHere is a scatterplot of the data."
  },
  {
    "objectID": "chapter_2_lesson_2_handout.html",
    "href": "chapter_2_lesson_2_handout.html",
    "title": "Autocorrelation Concepts",
    "section": "",
    "text": "Lag \\(k=1\\)\n\n\n\n\n\nt\nx_t\nx_{t+k}\nx_t-mean(x)\n(x_t-mean(x))^2\nx_{t+k}-mean(x)\n(x-mean(x))(x_{t+k}-mean(x))\n\n\n\n\n1\n4.4\n4.2\n-0.3\n0.09\n-0.5\n0.15\n\n\n2\n4.2\n4.2\n-0.5\n0.25\n-0.5\n0.25\n\n\n3\n4.2\n4\n-0.5\n0.25\n-0.7\n0.35\n\n\n4\n4\n4.4\n-0.7\n0.49\n-0.3\n0.21\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n9\n5.4\n5.5\n0.7\n0.49\n0.8\n0.56\n\n\n10\n5.5\n—\n0.8\n0.64\n—\n—\n\n\nsum\n47\n42.6\n0\n2.7\n0.3\n2.06\n\n\n\n\n\n\n\n\n\nLag \\(k = 2\\)\n\n\n\n\n\nt\nx_t\nx_{t+k}\nx_t-mean(x)\n(x_t-mean(x))^2\nx_{t+k}-mean(x)\n(x-mean(x))(x_{t+k}-mean(x))\n\n\n\n\n1\n4.4\n4.2\n-0.3\n0.09\n-0.5\n0.15\n\n\n2\n4.2\n4\n-0.5\n0.25\n-0.7\n0.35\n\n\n3\n4.2\n4.4\n-0.5\n0.25\n-0.3\n0.15\n\n\n4\n4\n\n\n\n\n\n\n\n5\n4.4\n\n\n\n\n\n\n\n6\n4.7\n\n\n\n\n\n\n\n7\n4.9\n5.4\n0.2\n0.04\n0.7\n0.14\n\n\n8\n5.3\n5.5\n0.6\n0.36\n0.8\n0.48\n\n\n9\n5.4\n—\n0.7\n0.49\n—\n—\n\n\n10\n5.5\n—\n0.8\n0.64\n—\n—\n\n\nsum\n47\n\n\n\n\n\n\n\n\n\n\n\n\n  \n\n\nLag \\(k = 3\\)\n\n\n\n\n\nt\nx_t\nx_{t+k}\nx_t-mean(x)\n(x_t-mean(x))^2\nx_{t+k}-mean(x)\n(x-mean(x))(x_{t+k}-mean(x))\n\n\n\n\n1\n4.4\n4\n-0.3\n0.09\n-0.7\n0.21\n\n\n2\n4.2\n4.4\n-0.5\n0.25\n-0.3\n0.15\n\n\n3\n4.2\n4.7\n-0.5\n0.25\n0\n0\n\n\n4\n4\n4.9\n-0.7\n0.49\n0.2\n-0.14\n\n\n5\n4.4\n5.3\n-0.3\n0.09\n0.6\n-0.18\n\n\n6\n4.7\n5.4\n0\n0\n0.7\n0\n\n\n7\n4.9\n5.5\n0.2\n0.04\n0.8\n0.16\n\n\n8\n5.3\n—\n0.6\n0.36\n—\n—\n\n\n9\n5.4\n—\n0.7\n0.49\n—\n—\n\n\n10\n5.5\n—\n0.8\n0.64\n—\n—\n\n\nsum\n47\n34.2\n0\n2.7\n1.3\n0.2\n\n\n\n\n\n\n\n\n\nLag \\(k = 4\\)\n\n\n\n\n\nt\nx_t\nx_{t+k}\nx_t-mean(x)\n(x_t-mean(x))^2\nx_{t+k}-mean(x)\n(x-mean(x))(x_{t+k}-mean(x))\n\n\n\n\n1\n4.4\n\n\n\n\n\n\n\n2\n4.2\n\n\n\n\n\n\n\n3\n4.2\n\n\n\n\n\n\n\n4\n4\n\n\n\n\n\n\n\n5\n4.4\n\n\n\n\n\n\n\n6\n4.7\n\n\n\n\n\n\n\n7\n4.9\n\n\n\n\n\n\n\n8\n5.3\n\n\n\n\n\n\n\n9\n5.4\n\n\n\n\n\n\n\n10\n5.5\n\n\n\n\n\n\n\nsum\n47"
  },
  {
    "objectID": "chapter_3.html",
    "href": "chapter_3.html",
    "title": "Lessons & Homework",
    "section": "",
    "text": "Chapter 3\n\n\n\n\n\n\nLessons\n\n\n\n\nLesson 1 - Leading Variables and Associated Variables\nLesson 2 - Exponential Smoothing (EWMA)\nLesson 3 - Holt-Winters Method (Additive Models) - Part 1\nLesson 3 - Holt-Winters Method (Additive Models) - Part 2\nLesson 4 - Holt-Winters Method (Multiplicative Models)\n\n\n\n\n\n\n\n\n\nDownload Homework Assignments\n\n\n\n\n homework_3_1.qmd \n homework_3_2.qmd \n homework_3_3.qmd \n homework_3_4.qmd \n homework_3_5.qmd",
    "crumbs": [
      "Overview",
      "Lessons & Homework"
    ]
  },
  {
    "objectID": "chapter_3_lesson_1_handout.html",
    "href": "chapter_3_lesson_1_handout.html",
    "title": "Leading Variables and Associated Variables",
    "section": "",
    "text": "Complete Tables 1 and 2 to calculate \\(c_k\\) for the given values of \\(k\\).\n\nTable 1: Computation of squared deviations\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$y_t$$\n$$x_t - \\bar x$$\n$$(x_t - \\bar x)^2$$\n$$y_t - \\bar y$$\n$$(y_t - \\bar y)^2$$\n\n\n\n\n1\n21\n14\n1\n1\n\n\n\n\n2\n20\n16\n0\n0\n\n\n\n\n3\n17\n18\n-3\n9\n\n\n\n\n4\n15\n17\n-5\n25\n\n\n\n\n5\n18\n12\n-2\n4\n\n\n\n\n6\n21\n10\n1\n1\n\n\n\n\n7\n21\n11\n1\n1\n\n\n\n\n8\n24\n16\n4\n16\n\n\n\n\n9\n22\n14\n2\n4\n\n\n\n\n10\n21\n22\n1\n1\n\n\n\n\nSum\n200\n150\n0\n62\n\n\n\n\n\n\n\n\n\n           \n\n\nTable 2: Computation of \\(c_k\\) and \\(r_k\\) for select values of \\(k\\)\n\n\n\n\n\n$$t$$\n$$x_t - \\bar x$$\n$$y_t - \\bar y$$\n$$~k=-4~$$\n$$~k=-3~$$\n$$~k=-2~$$\n$$~k=-1~$$\n$$~k=0~$$\n$$~k=1~$$\n$$~k=2~$$\n$$~k=3~$$\n$$~k=4~$$\n\n\n\n\n1\n1\n-1\n\n\n\n\n-1\n\n3\n5\n2\n\n\n2\n0\n1\n\n\n\n\n\n\n-5\n-2\n1\n\n\n3\n-3\n3\n\n\n\n\n\n\n-6\n3\n3\n\n\n4\n-5\n2\n\n\n\n\n\n\n2\n2\n8\n\n\n5\n-2\n-3\n\n\n\n\n\n\n-3\n-12\n-6\n\n\n6\n1\n-5\n\n\n\n\n\n\n-20\n-10\n-5\n\n\n7\n1\n-4\n\n\n\n\n\n\n-8\n-4\n—\n\n\n8\n4\n1\n\n\n\n\n\n\n1\n—\n—\n\n\n9\n2\n-1\n\n\n\n\n\n\n—\n—\n—\n\n\n10\n1\n7\n\n\n\n\n\n\n—\n—\n—\n\n\nSum\n0\n0\n\n\n\n\n\n\n-36\n-18\n3\n\n\n$$c_k$$\n\n\n\n\n\n\n\n\n-3.6\n-1.8\n0.3\n\n\n$$r_k$$\n\n\n\n\n\n\n\n\n-0.424\n-0.212\n0.035\n\n\n\n\n\n\n\n\n\n\nFigure 2: Plot of the Sample CCF"
  },
  {
    "objectID": "chapter_3_lesson_2_handout.html",
    "href": "chapter_3_lesson_2_handout.html",
    "title": "Exponential Smoothing (EWMA)",
    "section": "",
    "text": "Table 1: Calculation of \\(a_t\\) and \\(e_t\\) for a sample time series\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$a_t$$\n$$e_t$$\n\n\n\n\n1\n4.4\n\n\n\n\n2\n4.2\n\n\n\n\n3\n4.2\n\n\n\n\n4\n4\n\n\n\n\n5\n4.4\n\n\n\n\n6\n4.7\n\n\n\n\n7\n4.9\n\n\n\n\n8\n5.3\n\n\n\n\n9\n5.4\n\n\n\n\n10\n5.5"
  },
  {
    "objectID": "chapter_3_lesson_4.html",
    "href": "chapter_3_lesson_4.html",
    "title": "Holt-Winters Method (Additive Models) - Part 2",
    "section": "",
    "text": "Implement the Holt-Winter method to forecast time series\n\n\nCompute the Holt-Winters estimate by hand\nUse HoltWinters() to forecast additive model time series\nPlot the Holt-Winters decomposition of a time series (see Fig 3.10)\nPlot the Holt-Winters fitted values versus the original time series (see Fig 3.11)\nSuperimpose plots of the Holt-Winters predictions with the time series realizations (see Fig 3.13)",
    "crumbs": [
      "Lesson 4",
      "Holt-Winters Method (Additive Models) - Part 2"
    ]
  },
  {
    "objectID": "chapter_3_lesson_4.html#learning-outcomes",
    "href": "chapter_3_lesson_4.html#learning-outcomes",
    "title": "Holt-Winters Method (Additive Models) - Part 2",
    "section": "",
    "text": "Implement the Holt-Winter method to forecast time series\n\n\nCompute the Holt-Winters estimate by hand\nUse HoltWinters() to forecast additive model time series\nPlot the Holt-Winters decomposition of a time series (see Fig 3.10)\nPlot the Holt-Winters fitted values versus the original time series (see Fig 3.11)\nSuperimpose plots of the Holt-Winters predictions with the time series realizations (see Fig 3.13)",
    "crumbs": [
      "Lesson 4",
      "Holt-Winters Method (Additive Models) - Part 2"
    ]
  },
  {
    "objectID": "chapter_3_lesson_4.html#preparation",
    "href": "chapter_3_lesson_4.html#preparation",
    "title": "Holt-Winters Method (Additive Models) - Part 2",
    "section": "Preparation",
    "text": "Preparation\n\nReview Section 3.4.2 (Page 59 - top of page 60 only)",
    "crumbs": [
      "Lesson 4",
      "Holt-Winters Method (Additive Models) - Part 2"
    ]
  },
  {
    "objectID": "chapter_3_lesson_4.html#small-group-activity-holt-winters-model-for-residential-natural-gas-consumption-35-min",
    "href": "chapter_3_lesson_4.html#small-group-activity-holt-winters-model-for-residential-natural-gas-consumption-35-min",
    "title": "Holt-Winters Method (Additive Models) - Part 2",
    "section": "Small Group Activity: Holt-Winters Model for Residential Natural Gas Consumption (35 min)",
    "text": "Small Group Activity: Holt-Winters Model for Residential Natural Gas Consumption (35 min)\nThe United States Energy Information Administration (EIA) publishes data on the total residential natural gas consumption in the country. This government agency publishes monthly data beginning in January 1973. For the purpose of this example, we will only consider quarterly values beginning in 2017. The data are given in MMcf (thousand-thousand cubic feet, or millions of cubic feet). We convert the data to billions of cubic feet (Bcf) and round to the nearest integer to make the numbers a little more manageable.\n\n\nShow the code\nnat_gas &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/natural_gas_res.csv\") |&gt;\n  mutate(date = my(month)) |&gt;\n  filter(date &gt;= my(\"Jan 2017\")) |&gt;\n  mutate(quarter = yearquarter(date)) |&gt;\n  group_by(quarter) |&gt; \n  summarize(\n    gas_use_mmcf = sum(residential_nat_gas_consumption),\n    n = n()\n  ) |&gt;\n  filter(n == 3) |&gt;  # Eliminate partial quarter(s)\n  dplyr::select(-n) |&gt;\n  mutate(gas_billion_cf = round(gas_use_mmcf / 10^3))\n\n\n\n\n\n\n\nquarter\ngas_use_mmcf\ngas_billion_cf\n\n\n\n\n2017 Q1\n1990316\n1990\n\n\n2017 Q2\n602515\n603\n\n\n2017 Q3\n325786\n326\n\n\n2017 Q4\n1494706\n1495\n\n\n2018 Q1\n2330552\n2331\n\n\n2018 Q2\n729111\n729\n\n\n⋮\n⋮\n⋮\n\n\n2022 Q2\n709645\n710\n\n\n2022 Q3\n327101\n327\n\n\n2022 Q4\n1590005\n1590\n\n\n2023 Q1\n2114999\n2115\n\n\n2023 Q2\n663003\n663\n\n\n2023 Q3\n328735\n329\n\n\n\n\n\n\n\nThe quarters are defined as:\n\nQuarter 1: Jan, Feb, Mar\nQuarter 2: Apr, May, Jun\nQuarter 3: Jul, Aug, Sep\nQuarter 4: Oct, Nov, Dec\n\nThe weather is colder in the first and fourth quarters, so the demand for natural gas will be higher then. As illustrated in Figure 1, the difference in the consumption in the lowest and highest quarters of a year tends to be about 2000 Bcf.\n\n\n\n\n\n\n\n\nFigure 1: Quarterly U.S. natural gas consumption (Bcf)\n\n\n\n\n\nWe will use this information to create an initial estimate of the seasonality of the time series. We will assume that in Quarters 1 and 4, natural gas use is 1000 Bcf above the level of the time series and in Quarters 2 and 3, it is 1000 Bcf below the level of the time series. This is not accurate, but it gives us a reasonable starting point.\nThe portion of the time series we are using begins in the first quarter of 2017. So we will choose the initial values of \\(s_t\\) as: \\[\n  s_{Q1} = 1000, ~~~ s_{Q2} = -1000, ~~~ s_{Q3} = -1000, ~~~ s_{Q4} = 1000\n\\]\nWith \\(p=4\\) quarters in a year, we implement initial seasonality estimates in the Holt-Winters model as \\[\n  s_{1-p} = s_{(-3)} = 1000, ~~~ s_{2-p} = s_{(-2)} = -1000, ~~~ s_{3-p} = s_{(-1)} = -1000, ~~~ s_{4-p} = s_{0} = 1000\n\\]\n Tables-Handout-Excel \nfor parameter values reference the “check your understanding” section below this table\n\n\n\n\nTable 1: Holt-Winters filter for the quarterly natural gas consumption in the U.S. in billions of cubic feet\n\n\n\n\n\n\n$$Quarter$$\n$$t$$\n$$x_t$$\n$$a_t$$\n$$b_t$$\n$$s_t$$\n$$\\hat x_t$$\n\n\n\n\n2016 Q1\n-3\n—\n—\n—\n1000\n—\n\n\n2016 Q2\n-2\n—\n—\n—\n-1000\n—\n\n\n2016 Q3\n-1\n—\n—\n—\n-1000\n—\n\n\n2016 Q4\n0\n—\n—\n—\n1000\n—\n\n\n2017 Q1\n1\n1990\n\n\n\n\n\n\n2017 Q2\n2\n603\n\n\n\n\n\n\n2017 Q3\n3\n326\n\n\n\n\n\n\n2017 Q4\n4\n1495\n\n\n\n\n\n\n2018 Q1\n5\n2331\n1508\n-58\n805\n2313\n\n\n2018 Q2\n6\n729\n1519\n-44\n-1012\n507\n\n\n2018 Q3\n7\n318\n1464\n-46\n-1110\n354\n\n\n2018 Q4\n8\n1620\n1301\n-69\n693\n1994\n\n\n2019 Q1\n9\n2451\n1315\n-52\n871\n2186\n\n\n2019 Q2\n10\n670\n1347\n-35\n-945\n402\n\n\n2019 Q3\n11\n323\n1336\n-30\n-1091\n245\n\n\n2019 Q4\n12\n1573\n1221\n-47\n625\n1846\n\n\n2020 Q1\n13\n2089\n1183\n-45\n878\n2061\n\n\n2020 Q2\n14\n751\n1250\n-23\n-856\n394\n\n\n2020 Q3\n15\n354\n1271\n-14\n-1056\n215\n\n\n2020 Q4\n16\n1481\n1177\n-30\n561\n1738\n\n\n2021 Q1\n17\n2345\n1211\n-17\n929\n2140\n\n\n2021 Q2\n18\n690\n1264\n-3\n-800\n464\n\n\n2021 Q3\n19\n338\n1288\n2\n-1035\n253\n\n\n2021 Q4\n20\n1344\n1189\n-18\n480\n1669\n\n\n2022 Q1\n21\n2337\n1218\n-9\n967\n2185\n\n\n2022 Q2\n22\n710\n1269\n3\n-752\n517\n\n\n2022 Q3\n23\n327\n1290\n7\n-1021\n269\n\n\n2022 Q4\n24\n1590\n1260\n0\n450\n\n\n\n2023 Q1\n25\n2115\n1238\n-4\n949\n\n\n\n2023 Q2\n26\n663\n1270\n3\n-723\n\n\n\n2023 Q3\n27\n329\n1288\n6\n-1021\n\n\n\n2023 Q4\n28\n—\n—\n—\n\n\n\n\n2024 Q1\n29\n—\n—\n—\n\n\n\n\n2024 Q2\n30\n—\n—\n—\n\n\n\n\n2024 Q3\n31\n—\n—\n—\n\n\n\n\n2024 Q4\n32\n—\n—\n—\n\n\n\n\n2025 Q1\n33\n—\n—\n—\n\n\n\n\n2025 Q2\n34\n—\n—\n—\n\n\n\n\n2025 Q3\n35\n—\n—\n—\n\n\n\n\n2025 Q4\n36\n—\n—\n—\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHolt-Winters Update Equations (Additive Model)\n\n\n\nRecall the three Holt-Winters update equations for an additive model are:\n\\[\\begin{align*}\n  a_t &= \\alpha \\left( x_t - s_{t-p} \\right) + (1-\\alpha) \\left( a_{t-1} + b_{t-1} \\right) && \\text{Level} \\\\\n  b_t &= \\beta \\left( a_t - a_{t-1} \\right) + (1-\\beta) b_{t-1} && \\text{Slope} \\\\\n  s_t &= \\gamma \\left( x_t - a_t \\right) + (1-\\gamma) s_{t-p} && \\text{Seasonal}\n\\end{align*}\\]\nwhere \\(\\{x_t\\}\\) is a time series from \\(t=1\\) to \\(t=n\\) that has seasonality with a period of \\(p\\) time units; at time \\(t\\), \\(a_t\\) is the estimated level of the time series, \\(b_t\\) is the estimated slope, and \\(s_t\\) is the estimated seasonal component; and \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\) are parameters (all between 0 and 1).\nThe forecasting equation is: \\[\n  \\hat x_{n+k|n} = a_n + k b_n + s_{n+k-p}\n\\]\nThe details of these computations are given in Chapter 3 Lesson 3.\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nApply Holt-Winters filtering to these data. Use \\(\\alpha = \\beta = \\gamma = 0.2\\).\n\nFind \\(a_1\\)\nFind \\(b_1\\)\nCompute the missing values of \\(a_t\\), \\(b_t\\), and \\(s_t\\) for all quarters from Q1 of 2017 to the end of the data set.\nFind \\(\\hat x_t\\) for all rows where \\(t \\ge 1\\). Note that the expression to compute \\(\\hat x_t\\) is different for the rows with data versus the rows where forecasting is required.\nSuperimpose a sketch of your Holt-Winters filter and the associated forecast on Figure 1.",
    "crumbs": [
      "Lesson 4",
      "Holt-Winters Method (Additive Models) - Part 2"
    ]
  },
  {
    "objectID": "chapter_3_lesson_4.html#small-group-activity-application-of-holt-winters-in-r-using-the-baltimore-crime-data-20-min",
    "href": "chapter_3_lesson_4.html#small-group-activity-application-of-holt-winters-in-r-using-the-baltimore-crime-data-20-min",
    "title": "Holt-Winters Method (Additive Models) - Part 2",
    "section": "Small Group Activity: Application of Holt-Winters in R using the Baltimore Crime Data (20 min)",
    "text": "Small Group Activity: Application of Holt-Winters in R using the Baltimore Crime Data (20 min)\n\nBackground\nThe City of Baltimore publishes crime data, which can be accessed through a query. This dataset is sourced from the City of Baltimore Open Data. You can explore the data on data.world.\nUse the following code to import the data:\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\ncrime_df &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/baltimore_crime.parquet\")\n\n\nThe data set consists of 285807 rows and 12 columns. There are a few key variables:\n\nDate and Time: Records the date and time of each incident.\nLocation: Detailed coordinates of each incident.\nCrime Type: Description of the type of crime.\n\nWhen exploring a new time series, it is crucial to carefully examine the data. Here are a few rows of the original data set. Note that the data are not sorted in time order.\n\n\n\n\n\nCrimeDate\nCrimeTime\nCrimeCode\nLocation\nDescription\nInside.Outside\nWeapon\nPost\nDistrict\nNeighborhood\nLocation.1\nTotal.Incidents\n\n\n\n\n11/12/2016\n02:35:00\n3B\n300 SAINT PAUL PL\nROBBERY - STREET\nO\nNA\n111\nCENTRAL\nDowntown\n(39.2924100000, -76.6140800000)\n1\n\n\n11/12/2016\n02:56:00\n3CF\n800 S BROADWAY\nROBBERY - COMMERCIAL\nI\nFIREARM\n213\nSOUTHEASTERN\nFells Point\n(39.2824200000, -76.5928800000)\n1\n\n\n11/12/2016\n03:00:00\n6D\n1500 PENTWOOD RD\nLARCENY FROM AUTO\nO\nNA\n413\nNORTHEASTERN\nStonewood-Pentwood-Winston\n(39.3480500000, -76.5883400000)\n1\n\n\n11/12/2016\n03:00:00\n6D\n6600 MILTON LN\nLARCENY FROM AUTO\nO\nNA\n424\nNORTHEASTERN\nWestfield\n(39.3626300000, -76.5516100000)\n1\n\n\n11/12/2016\n03:00:00\n6E\n300 W BALTIMORE ST\nLARCENY\nO\nNA\n111\nCENTRAL\nDowntown\n(39.2893800000, -76.6197100000)\n1\n\n\n11/12/2016\n03:00:00\n4E\n6900 MCCLEAN BLVD\nCOMMON ASSAULT\nI\nHANDS\n423\nNORTHEASTERN\nHamilton Hills\n(39.3707000000, -76.5670900000)\n1\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n01/01/2011\n23:00:00\n7A\n2500 ARUNAH AV\nAUTO THEFT\nO\nNA\n721\nWESTERN\nEvergreen Lawn\n(39.2954200000, -76.6592800000)\n1\n\n\n01/01/2011\n23:25:00\n4E\n100 N MONROE ST\nCOMMON ASSAULT\nI\nHANDS\n714\nWESTERN\nPenrose/Fayette Street Outreach\n(39.2899900000, -76.6470700000)\n1\n\n\n01/01/2011\n23:38:00\n4D\n800 N FREMONT AV\nAGG. ASSAULT\nI\nHANDS\n123\nWESTERN\nUpton\n(39.2981200000, -76.6339100000)\n1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nUsing the command crime_df |&gt; summary(), we learn that the Total.Incidents always equals 1. What does each row in the data frame represent?\n\n\n\nWe now summarize the data into a daily tsibble.\n\n\nShow the code\n# Data Summary and Aggregation\n# Group by dates column and summarize from Total.Incidents column\ndaily_summary_df &lt;- crime_df |&gt;\n  rename(dates = CrimeDate) |&gt;\n  group_by(dates) |&gt;\n  summarise(incidents = sum(Total.Incidents))\n\n# Data Transformation and Formatting\n# Select relevant columns, format dates, and arrange the data\ncrime_data &lt;- daily_summary_df |&gt;\n  mutate(dates = mdy(dates)) |&gt;\n  mutate(\n    month = month(dates),\n    day = day(dates),\n    year = year(dates)\n  ) |&gt;\n  arrange(dates) |&gt;\n  dplyr::select(dates, month, day, year, incidents)\n  \n# Convert formatted data to a tsibble with dates as the index\ncrime_tsibble &lt;- as_tsibble(crime_data, index = dates) \n\n\nHere are a few rows of the data after summing the crime incidents each day.\n\n\n\n\n\ndates\nmonth\nday\nyear\nincidents\n\n\n\n\n2011-01-01\n1\n1\n2011\n185\n\n\n2011-01-02\n1\n2\n2011\n102\n\n\n2011-01-03\n1\n3\n2011\n106\n\n\n2011-01-04\n1\n4\n2011\n113\n\n\n2011-01-05\n1\n5\n2011\n131\n\n\n2011-01-06\n1\n6\n2011\n107\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n2016-11-10\n11\n10\n2016\n109\n\n\n2016-11-11\n11\n11\n2016\n115\n\n\n2016-11-12\n11\n12\n2016\n69\n\n\n\n\n\n\n\nHere is a time plot of the number of crimes reported in Baltimore daily.\n\n\nShow the code\n# Time series plot of total incidents over time\ncrime_plot &lt;- autoplot(crime_tsibble, .vars = incidents) +\n  labs(\n    x = \"Time\",\n    y = \"Total Crime Incidents\",\n    title = \"Total Crime Incidents Over Time\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n# Display the plot\ncrime_plot\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do you notice about this time plot?\n\nDescribe the trend\nIs there evidence of seasonality?\nIs the additive or multiplicative model appropriate?\nWhich date has the highest number of recorded crimes? Can you determine a reason for this spike?\n\n\n\n\nThe following table summarizes the number of days in each month for which crime data were reported.\n\n\nShow the code\ncrime_data |&gt;\n  mutate(month_char = format(as.Date(dates), '%b') ) |&gt;\n  group_by(month, month_char, year) |&gt;\n  summarise(n = n(), .groups = \"keep\") |&gt;\n  group_by() |&gt;\n  arrange(year, month) |&gt;\n  dplyr::select(-month) |&gt;\n  rename(Year = year) |&gt;\n  pivot_wider(names_from = month_char, values_from = n) |&gt;\n  display_table()\n\n\n\n\n\nYear\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\n\n\n2011\n31\n28\n31\n30\n31\n30\n31\n31\n30\n31\n30\n31\n\n\n2012\n31\n29\n31\n30\n31\n30\n31\n31\n30\n31\n30\n31\n\n\n2013\n31\n28\n31\n30\n31\n30\n31\n31\n30\n31\n30\n31\n\n\n2014\n31\n28\n31\n30\n31\n30\n31\n31\n30\n31\n30\n31\n\n\n2015\n31\n28\n31\n30\n31\n30\n31\n31\n30\n31\n30\n31\n\n\n2016\n31\n29\n31\n30\n31\n30\n31\n31\n30\n31\n12\nNA\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do you observe about the data?\nWhat are some problems that could arise from incomplete data?\nHow do you recommend we address the missing data?\n\n\n\n\n\nMonthly Summary\nWe could analyze the data at the daily level, but for simplicity we will model the monthly totals.\n\n\nShow the code\ncrime_monthly_ts &lt;- crime_tsibble |&gt;\n  as_tibble() |&gt;\n  mutate(months = yearmonth(dates)) |&gt;\n  group_by(months) |&gt;\n  summarize(value = sum(incidents)) |&gt;\n  as_tsibble(index = months) \n\n# Plot mean annual total incidents using autoplot\nautoplot(crime_monthly_ts, .vars = value) +\n  labs(\n    x = \"Year\",\n    y = \"Total Monthly Crime Incidents\",\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nThere is incomplete data for 2016, as data were not provided after 11/12/2016.  We will omit any data after October 2016.\n\n\nShow the code\ncrime_monthly_ts1 &lt;- crime_monthly_ts |&gt;\n  filter(months &lt; yearmonth(mdy(\"11/1/2016\")))\n\n\nWe apply Holt-Winters filtering on the monthly Baltimore crimes data with an additive model:\n\n\nShow the code\ncrime_hw &lt;- crime_monthly_ts1 |&gt;\n  tsibble::fill_gaps() |&gt;\n  model(Additive = ETS(value ~\n        trend(\"A\") +\n        error(\"A\") +\n        season(\"A\"),\n        opt_crit = \"amse\", nmse = 1))\nreport(crime_hw)\n\n\nSeries: value \nModel: ETS(A,A,A) \n  Smoothing parameters:\n    alpha = 0.4102911 \n    beta  = 0.0001719 \n    gamma = 0.001008159 \n\n  Initial states:\n     l[0]      b[0]      s[0]     s[-1]    s[-2]    s[-3]    s[-4]    s[-5]\n 4318.875 -4.475258 -83.12823 -38.36167 374.3369 217.8943 416.0997 428.0887\n    s[-6]    s[-7]     s[-8]     s[-9]    s[-10]    s[-11]\n 301.8743 366.7473 -141.8365 -334.7158 -1039.907 -467.0918\n\n  sigma^2:  45518.72\n\n     AIC     AICc      BIC \n1064.040 1075.810 1102.265 \n\n\nWe can compute some values to assess the fit of the model:\n\n\nShow the code\n# SS of random terms\nsum(components(crime_hw)$remainder^2, na.rm = T)\n\n# RMSE\nforecast::accuracy(crime_hw)$RMSE\n\n# Standard devation of number of incidents each month\nsd(crime_monthly_ts1$value)\n\n\n\nThe sum of the square of the random terms is: 2.4580111^{6}.\nThe root mean square error (RMSE) is: 187.388486.\nThe standard deviation of the number of incidents each month is 22.9761197.\n\nFigure 2 illustrates the Holt-Winters decomposition of the Baltimore crime data.\n\n\nShow the code\nautoplot(components(crime_hw))\n\n\n\n\n\n\n\n\nFigure 2: Monthly Total Number of Crime Reported in Baltimore\n\n\n\n\n\nIn Figure 3, we can observe the relationship between the Holt-Winters filter and the time series of the number of crimes each month.\n\n\nShow the code\naugment(crime_hw) |&gt;\n  ggplot(aes(x = months, y = value)) +\n    coord_cartesian(ylim = c(0,5500)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted\")) +\n    labs(color = \"\")\n\n\n\n\n\n\n\n\nFigure 3: Superimposed plots of the number of crimes each month and the Holt-Winters filter\n\n\n\n\n\nFigure 4 contains the information from Figure 3, with the addition of an additional four years of forecasted values. The light blue bands give the 95% prediction bands for the forecast.\n\n\nShow the code\ncrime_forecast &lt;- crime_hw |&gt;\n  forecast(h = \"4 years\") \ncrime_forecast |&gt;\n  autoplot(crime_monthly_ts1, level = 95) +\n  coord_cartesian(ylim = c(0,5500)) +\n  geom_line(aes(y = .fitted, color = \"Fitted\"),\n    data = augment(crime_hw)) +\n  scale_color_discrete(name = \"\")\n\n\n\n\n\n\n\n\nFigure 4: Superimposed plots of the number of crimes each month and the Holt-Winters filter, with four additional years forecasted\n\n\n\n\n\n\n\nRethinking Baltimore\nThe monthly crime data shows a distinct pattern arcing through the annual cycle. Consider the data for 2011.\n\n\n\n\nTable 2: Total count of crimes reported in Baltimore in 2011 by month\n\n\n\n\n\n\nJan\nFeb\nMar\nApr\nMay\nJun\nJul\nAug\nSep\nOct\nNov\nDec\n\n\n\n\n3440\n3108\n4269\n4149\n4516\n4427\n4669\n4574\n4377\n4644\n4411\n4067\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nStarting with January, determine whether the number of crimes goes up or down as you move from one month to the next.\nWhat might explain this pattern?\nUse the function days_in_month() to adjust the time series and re-run the analysis. What do you notice?",
    "crumbs": [
      "Lesson 4",
      "Holt-Winters Method (Additive Models) - Part 2"
    ]
  },
  {
    "objectID": "chapter_3_lesson_4.html#homework-preview-5-min",
    "href": "chapter_3_lesson_4.html#homework-preview-5-min",
    "title": "Holt-Winters Method (Additive Models) - Part 2",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_3_4.qmd \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNatural Gas Holt-Winters Filter\n Tables-Handout-Excel-key \n\n\n\n\n\n\nQuarterly U.S. natural gas consumption (Bcf)\n\n\n\n\n\n\n\nHolt-Winters filter for the quarterly natural gas consumption in the U.S. in billions of cubic feet\n\n\n$$Quarter$$\n$$t$$\n$$x_t$$\n$$a_t$$\n$$b_t$$\n$$s_t$$\n$$\\hat x_t$$\n\n\n\n\n2016 Q1\n-3\n—\n—\n—\n1000\n—\n\n\n2016 Q2\n-2\n—\n—\n—\n-1000\n—\n\n\n2016 Q3\n-1\n—\n—\n—\n-1000\n—\n\n\n2016 Q4\n0\n—\n—\n—\n1000\n—\n\n\n2017 Q1\n1\n1990\n1990\n36\n800\n1990\n\n\n2017 Q2\n2\n603\n1941\n19\n-1068\n873\n\n\n2017 Q3\n3\n326\n1833\n-6\n-1101\n732\n\n\n2017 Q4\n4\n1495\n1561\n-59\n787\n2348\n\n\n2018 Q1\n5\n2331\n1508\n-58\n805\n2313\n\n\n2018 Q2\n6\n729\n1519\n-44\n-1012\n507\n\n\n2018 Q3\n7\n318\n1464\n-46\n-1110\n354\n\n\n2018 Q4\n8\n1620\n1301\n-69\n693\n1994\n\n\n2019 Q1\n9\n2451\n1315\n-52\n871\n2186\n\n\n2019 Q2\n10\n670\n1347\n-35\n-945\n402\n\n\n2019 Q3\n11\n323\n1336\n-30\n-1091\n245\n\n\n2019 Q4\n12\n1573\n1221\n-47\n625\n1846\n\n\n2020 Q1\n13\n2089\n1183\n-45\n878\n2061\n\n\n2020 Q2\n14\n751\n1250\n-23\n-856\n394\n\n\n2020 Q3\n15\n354\n1271\n-14\n-1056\n215\n\n\n2020 Q4\n16\n1481\n1177\n-30\n561\n1738\n\n\n2021 Q1\n17\n2345\n1211\n-17\n929\n2140\n\n\n2021 Q2\n18\n690\n1264\n-3\n-800\n464\n\n\n2021 Q3\n19\n338\n1288\n2\n-1035\n253\n\n\n2021 Q4\n20\n1344\n1189\n-18\n480\n1669\n\n\n2022 Q1\n21\n2337\n1218\n-9\n967\n2185\n\n\n2022 Q2\n22\n710\n1269\n3\n-752\n517\n\n\n2022 Q3\n23\n327\n1290\n7\n-1021\n269\n\n\n2022 Q4\n24\n1590\n1260\n0\n450\n1710\n\n\n2023 Q1\n25\n2115\n1238\n-4\n949\n2187\n\n\n2023 Q2\n26\n663\n1270\n3\n-723\n547\n\n\n2023 Q3\n27\n329\n1288\n6\n-1021\n267\n\n\n2023 Q4\n28\n—\n—\n—\n450\n1744\n\n\n2024 Q1\n29\n—\n—\n—\n949\n2249\n\n\n2024 Q2\n30\n—\n—\n—\n-723\n583\n\n\n2024 Q3\n31\n—\n—\n—\n-1021\n291\n\n\n2024 Q4\n32\n—\n—\n—\n450\n1768\n\n\n2025 Q1\n33\n—\n—\n—\n949\n2273\n\n\n2025 Q2\n34\n—\n—\n—\n-723\n607\n\n\n2025 Q3\n35\n—\n—\n—\n-1021\n315\n\n\n2025 Q4\n36\n—\n—\n—\n450\n1792\n\n\n\n\n\n\n\n\nBalitmore Crime Spike\n\n\n# Dates with high criminal activity\ncrime_data |&gt; arrange(desc(incidents)) |&gt; head()\n\n# A tibble: 6 × 5\n  dates      month   day  year incidents\n  &lt;date&gt;     &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt;     &lt;dbl&gt;\n1 2015-04-27     4    27  2015       419\n2 2016-06-05     6     5  2016       254\n3 2011-10-14    10    14  2011       199\n4 2011-07-30     7    30  2011       193\n5 2013-06-22     6    22  2013       192\n6 2013-12-20    12    20  2013       192\n\n\nOn April 27, 2015, 419 crimes were recorded. These are associated with protests over arrest of Freddie Gray.\n\nBalitmore (Mean Crimes Per Day)\n\n\n\nShow the code\ncrime_monthly_ts2 &lt;- crime_monthly_ts1 |&gt; \n  mutate(avg_value = value / days_in_month(ym(months))) |&gt;\n  dplyr::select(-value)\n\ncrime_hw2 &lt;- crime_monthly_ts2 |&gt;\n  tsibble::fill_gaps() |&gt;\n  model(Additive = ETS(avg_value ~\n        trend(\"A\") +\n        error(\"A\") +\n        season(\"A\"),\n        opt_crit = \"amse\", nmse = 1))\nreport(crime_hw)\n\n\nSeries: value \nModel: ETS(A,A,A) \n  Smoothing parameters:\n    alpha = 0.4102911 \n    beta  = 0.0001719 \n    gamma = 0.001008159 \n\n  Initial states:\n     l[0]      b[0]      s[0]     s[-1]    s[-2]    s[-3]    s[-4]    s[-5]\n 4318.875 -4.475258 -83.12823 -38.36167 374.3369 217.8943 416.0997 428.0887\n    s[-6]    s[-7]     s[-8]     s[-9]    s[-10]    s[-11]\n 301.8743 366.7473 -141.8365 -334.7158 -1039.907 -467.0918\n\n  sigma^2:  45518.72\n\n     AIC     AICc      BIC \n1064.040 1075.810 1102.265 \n\n\n\n\nShow the code\nautoplot(components(crime_hw2))\n\n\n\n\n\n\n\n\nFigure 5: Monthly Total Number of Crime Reported in Baltimore\n\n\n\n\n\n\n\nShow the code\naugment(crime_hw2) |&gt;\n  ggplot(aes(x = months, y = avg_value)) +\n    coord_cartesian(ylim = c(0,200)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted\")) +\n    labs(color = \"\")\n\n\n\n\n\n\n\n\nFigure 6: Superimposed plots of the number of crimes each month and the Holt-Winters filter\n\n\n\n\n\nFigure 4 contains the information from Figure 3, with the addition of an additional four years of forecasted values. The light blue bands give the 95% prediction bands for the forecast.\n\n\nShow the code\ncrime_forecast &lt;- crime_hw2 |&gt;\n  forecast(h = \"4 years\") \ncrime_forecast |&gt;\n  autoplot(crime_monthly_ts2, level = 95) +\n  coord_cartesian(ylim = c(0,200)) +\n  geom_line(aes(y = .fitted, color = \"Fitted\"),\n    data = augment(crime_hw2)) +\n  scale_color_discrete(name = \"\")\n\n\n\n\n\n\n\n\nFigure 7: Superimposed plots of the number of crimes each month and the Holt-Winters filter, with four additional years forecasted",
    "crumbs": [
      "Lesson 4",
      "Holt-Winters Method (Additive Models) - Part 2"
    ]
  },
  {
    "objectID": "chapter_3_lesson_4.html#references",
    "href": "chapter_3_lesson_4.html#references",
    "title": "Holt-Winters Method (Additive Models) - Part 2",
    "section": "References",
    "text": "References\n\nC. C. Holt (1957) Forecasting seasonals and trends by exponentially weighted moving averages, ONR Research Memorandum, Carnegie Institute of Technology 52. (Reprint at https://doi.org/10.1016/j.ijforecast.2003.09.015).\nP. R. Winters (1960). Forecasting sales by exponentially weighted moving averages. Management Science, 6, 324–342. (Reprint at https://doi.org/10.1287/mnsc.6.3.324.)",
    "crumbs": [
      "Lesson 4",
      "Holt-Winters Method (Additive Models) - Part 2"
    ]
  },
  {
    "objectID": "chapter_3_lesson_5.html",
    "href": "chapter_3_lesson_5.html",
    "title": "Holt-Winters Method (Multiplicative Models)",
    "section": "",
    "text": "Implement the Holt-Winter method to forecast time series\n\n\nExplain the Holt-Winters method equations for multiplicative decomposition models\nExplain the purpose of the paramters \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\)\nInterpret the coefficient estimates \\(a_t\\), \\(b_t\\), and \\(s_t\\) of the Holt-Winters smoothing algorithm\nExplain the Holt-Winters forecasting equation for multiplicative decomposition models, Equation (3.23)\nUse HoltWinters() to forecast multiplicative model time series\nPlot the Holt-Winters decomposition of a TS (see Fig 3.10)\nPlot the Holt-Winters fitted values versus the original time series (see Fig 3.11)\nSuperimpose plots of the Holt-Winters predictions with the time series realizations (see Fig 3.13)",
    "crumbs": [
      "Lesson 5",
      "Holt-Winters Method (Multiplicative Models)"
    ]
  },
  {
    "objectID": "chapter_3_lesson_5.html#learning-outcomes",
    "href": "chapter_3_lesson_5.html#learning-outcomes",
    "title": "Holt-Winters Method (Multiplicative Models)",
    "section": "",
    "text": "Implement the Holt-Winter method to forecast time series\n\n\nExplain the Holt-Winters method equations for multiplicative decomposition models\nExplain the purpose of the paramters \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\)\nInterpret the coefficient estimates \\(a_t\\), \\(b_t\\), and \\(s_t\\) of the Holt-Winters smoothing algorithm\nExplain the Holt-Winters forecasting equation for multiplicative decomposition models, Equation (3.23)\nUse HoltWinters() to forecast multiplicative model time series\nPlot the Holt-Winters decomposition of a TS (see Fig 3.10)\nPlot the Holt-Winters fitted values versus the original time series (see Fig 3.11)\nSuperimpose plots of the Holt-Winters predictions with the time series realizations (see Fig 3.13)",
    "crumbs": [
      "Lesson 5",
      "Holt-Winters Method (Multiplicative Models)"
    ]
  },
  {
    "objectID": "chapter_3_lesson_5.html#preparation",
    "href": "chapter_3_lesson_5.html#preparation",
    "title": "Holt-Winters Method (Multiplicative Models)",
    "section": "Preparation",
    "text": "Preparation\n\nRead Sections 3.4.2-3.4.3, 3.5",
    "crumbs": [
      "Lesson 5",
      "Holt-Winters Method (Multiplicative Models)"
    ]
  },
  {
    "objectID": "chapter_3_lesson_5.html#learning-journal-exchange-10-min",
    "href": "chapter_3_lesson_5.html#learning-journal-exchange-10-min",
    "title": "Holt-Winters Method (Multiplicative Models)",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading another student’s?\nWhat would you recommend the other student add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 5",
      "Holt-Winters Method (Multiplicative Models)"
    ]
  },
  {
    "objectID": "chapter_3_lesson_5.html#class-discussion-multiplicative-seasonality-10-min",
    "href": "chapter_3_lesson_5.html#class-discussion-multiplicative-seasonality-10-min",
    "title": "Holt-Winters Method (Multiplicative Models)",
    "section": "Class Discussion: Multiplicative Seasonality (10 min)",
    "text": "Class Discussion: Multiplicative Seasonality (10 min)\nWe can assume either additive or multiplicative seasonality. In the previous two lessons, we explored additive seasonality. In this lesson, we consider the case where the seasonality is multiplicative.\nAdditive seasonality is appropriate when the variation in the time series is roughly constant for any level. We assume multiplicative seasonality when the variation gets larger as the level increases.\n\nForecasting\nHere are the forecasting equations we use, based on the model that is appropriate for the time series.\n\n\n\n\n\n\n\n\n\nAdditive Seasonality\nMultiplicative Seasonality\n\n\n\n\nAdditive Slope\n\\[ \\hat x_{n+k \\mid n} = \\left( a_n + k \\cdot b_n \\right) + s_{n+k-p} \\]\n\\[ \\hat x_{n+k \\mid n} = \\left( a_n + k \\cdot b_n \\right) \\cdot s_{n+k-p} \\]\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWith your partner, for the forecasting equations above, identify where the additive or multiplicative terms are represented for both the slope and the seasonality.\n\nHow is an additive slope represented in the forecasting equation? \nHow is additive seasonality represented in the forecasting equation?\nHow is multiplicative seasonality represented in the forecasting equation?\n\n\n\n\n\n\nUpdate Equations (Multiplicative Seasonals)\nThe update equations for the seasonals are:\n\\[\\begin{align*}\n  a_t &= \\alpha \\left( \\frac{x_t}{s_{t-p}} \\right) + (1-\\alpha) \\left( a_{t-1} + b_{t-1} \\right) && \\text{Level} \\\\\n  b_t &= \\beta \\left( a_t - a_{t-1} \\right) + (1-\\beta) b_{t-1} && \\text{Slope} \\\\\n  s_t &= \\gamma \\left( \\frac{x_t}{a_{t}} \\right) + (1-\\gamma) s_{t-p} && \\text{Seasonal}\n\\end{align*}\\]\nNote that when the seasonal effect is additive, we subtract it from the time series to remove its effect. If the seasonal effect is multiplicative, we divide.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nWork with your partner to answer the following questions about the update equations.\n\\[\na_t = \\alpha \\cdot \\underbrace{ \\left( \\frac{x_t}{s_{t-p}} \\right) }_{A} + (1-\\alpha) \\cdot  \\underbrace{ \\left( a_{t-1} + b_{t-1} \\right) }_{B}\n~~~~~~~~~~~~~~~~~~~~ \\text{Level}\n\\]\n\nInterpret the term \\(A = \\dfrac{x_t}{s_{t-p}}\\).\nInterpret the term \\(B = a_{t-1} - b_{t-1}\\).\nExplain why this expression for \\(a_t\\) estimates the level of the time series at time \\(t\\).\n\n\\[\nb_t = \\beta \\cdot \\underbrace{ \\left( a_t - a_{t-1} \\right) }_{C} + (1-\\beta) \\cdot \\underbrace{ b_{t-1} }_{D}\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\text{Slope}\n\\]\n\nInterpret the term \\(C = a_t - a_{t-1}\\).\nInterpret the term \\(D = b_{t-1}\\).\nExplain why this expression for \\(b_t\\) estimates the slope of the time series at time \\(t\\).\n\n\\[\ns_t = \\gamma \\cdot \\underbrace{ \\left( \\frac{x_t}{a_{t}} \\right) }_{E} + (1-\\gamma) \\cdot \\underbrace{ s_{t-p} }_{F}\n~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ \\text{Seasonal}\n\\]\n\nInterpret the term \\(E = \\dfrac{x_t}{a_{t}}\\).\nInterpret the term \\(F = s_{t-p}\\).\nExplain why this expression for \\(s_t\\) estimates the seasonal component of the time series at time \\(t\\).\nWhen the seasonal component appears on the right-hand side of the update equations, it always given as \\(s_{t-p}\\). Why do we use the estimate of the seasonal effect \\(p\\) periods ago? Why not apply a more recent value?\nWhat do the following sets of terms have in common?\n\n\\(\\{A, C, E \\}\\)\n\\(\\{B, D, F \\}\\)\n\nExplain why the Holt-Winters method for multiplicative seasonals works.",
    "crumbs": [
      "Lesson 5",
      "Holt-Winters Method (Multiplicative Models)"
    ]
  },
  {
    "objectID": "chapter_3_lesson_5.html#small-group-activity-holt-winters-model-for-byu-idaho-enrollment-data-25-min",
    "href": "chapter_3_lesson_5.html#small-group-activity-holt-winters-model-for-byu-idaho-enrollment-data-25-min",
    "title": "Holt-Winters Method (Multiplicative Models)",
    "section": "Small Group Activity: Holt-Winters Model for BYU-Idaho Enrollment Data (25 min)",
    "text": "Small Group Activity: Holt-Winters Model for BYU-Idaho Enrollment Data (25 min)\nWe will now apply Holt-Winters filtering to the BYU-Idaho Enrollment data. First, we examine the time plot in Figure 1.\n Tables-Handout-Excel \n\n\nShow the code\n# read in the data from a csv and make the tsibble\nbyui_enrollment_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/byui_enrollment_2012.csv\") |&gt;\n  rename(\n    semester = \"TermCode\",\n    year = \"Year\",\n    enrollment = \"On Campus Enrollment (Campus HC)\"\n  ) |&gt;\n  mutate(\n    term = \n      case_when(\n        left(semester, 2) == \"WI\" ~ 1,\n        left(semester, 2) == \"SP\" ~ 2,\n        left(semester, 2) == \"FA\" ~ 3,\n        TRUE ~ NA\n      )\n  ) |&gt;\n  filter(!is.na(term)) |&gt;\n  mutate(dates = yearmonth( ym( paste(year, term * 4 - 3) ) ) ) |&gt;\n  mutate(enrollment_1000 = enrollment / 1000) |&gt;\n  dplyr::select(semester, dates, enrollment_1000) |&gt;\n  as_tsibble(index = dates) \n\nbyui_enrollment_ts_expanded &lt;- byui_enrollment_ts |&gt;\n  as_tibble() |&gt;\n  hw_additive_slope_multiplicative_seasonal_rounded(\"dates\", \"enrollment_1000\", p = 3, predict_periods = 9) |&gt;\n  mutate(xhat_t = ifelse(t %in% c(1:36), round(a_t * s_t, 1), xhat_t)) |&gt;\n  select(-dates, -enrollment_1000)\n\n# This is hard-coded for this data set, because I am in a hurry to get done.\n# This revises the semester codes\nbyui_enrollment_ts_expanded$semester[1:3] &lt;- c(\"SP11\", \"FA11\", \"WI12\")\nbyui_enrollment_ts_expanded$semester[40:48] &lt;- c(\"SP24\", \"FA24\", \"WI25\",\n                                                 \"SP25\", \"FA25\", \"WI26\",\n                                                 \"SP26\", \"FA26\", \"WI27\")\n\nbyui_enrollment_ts_expanded |&gt;\n  tail(nrow(byui_enrollment_ts_expanded) - 3) |&gt;\n  ggplot(aes(x = date, y = x_t)) +\n    geom_line(color = \"black\", linewidth = 1) +\n    coord_cartesian(ylim = c(0, 22.5)) +\n    labs(\n      x = \"Date\",\n      y = \"Enrollment (in Thousands)\",\n      title = \"BYU-Idaho On-Campus Enrollments (in Thousands)\",\n      color = \"Components\"\n    ) +\n    theme_minimal() +\n    theme(legend.position = \"none\") +\n    theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFigure 1: Time plot of BYU-Idaho campus enrollments\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhich of the Holt-Winter models described above is appropriate for this situation? Justify your answer.\n\n\n\nTable 1 summarizes the intermediate values for Holt-Winters filtering with multiplicative seasonals.\n\n\n\n\nTable 1: Holt-Winters smoothing for BYU-Idaho campus enrollments\n\n\n\n\n\n\n$$Semester$$\n$$t$$\n$$x_t$$\n$$a_t$$\n$$b_t$$\n$$s_t$$\n$$\\hat x_t$$\n\n\n\n\nSP11\n-2\n—\n—\n—\n\n—\n\n\nFA11\n-1\n—\n—\n—\n\n—\n\n\nWI12\n0\n—\n—\n—\n\n—\n\n\nSP12\n1\n13.7\n\n\n\n\n\n\nFA12\n2\n16.2\n14.2\n0.082\n1.03\n14.6\n\n\nWI13\n3\n15.5\n14.5\n0.126\n1.01\n14.6\n\n\nSP13\n4\n14\n14.5\n0.101\n0.99\n14.4\n\n\nFA13\n5\n15.6\n14.7\n0.121\n1.04\n15.3\n\n\nWI14\n6\n15.6\n\n\n\n\n\n\nSP14\n7\n12.9\n\n\n\n\n\n\nFA14\n8\n16.2\n14.8\n0.08\n1.05\n15.5\n\n\nWI15\n9\n16.7\n15.2\n0.144\n1.04\n15.8\n\n\nSP15\n10\n13.7\n15.1\n0.095\n0.96\n14.5\n\n\nFA15\n11\n17.6\n15.5\n0.156\n1.07\n16.6\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\nFA21\n29\n20.4\n18.5\n0.086\n1.12\n20.7\n\n\nWI22\n30\n18.4\n18.3\n0.029\n1.05\n19.2\n\n\nSP22\n31\n14.2\n17.9\n-0.057\n0.87\n15.6\n\n\nFA22\n32\n19.4\n17.7\n-0.086\n1.12\n19.8\n\n\nWI23\n33\n17.7\n17.5\n-0.109\n1.04\n18.2\n\n\nSP23\n34\n12.8\n16.9\n-0.207\n0.85\n14.4\n\n\nFA23\n35\n18.7\n16.7\n-0.206\n1.12\n18.7\n\n\nWI24\n36\n17.6\n16.6\n-0.185\n1.04\n17.3\n\n\nSP24\n37\n—\n—\n—\n\n\n\n\nFA24\n38\n—\n—\n—\n\n\n\n\nWI25\n39\n—\n—\n—\n\n\n\n\nSP25\n40\n—\n—\n—\n\n\n\n\nFA25\n41\n—\n—\n—\n1.12\n17.6\n\n\nWI26\n42\n—\n—\n—\n1.04\n16.1\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nApply Holt-Winters filtering to these data.\n\\[\n\\alpha = 0.2, \\beta = 0.2, \\gamma = 0.2\n\\]\n\nIdentify the value of \\(p\\)\nLet \\(a_1 = x_1\\)\nCompute \\[\n  b_1 =\n\\frac{\n  \\left(\n    \\dfrac{x_{p+1} - x_{1}}{p} +\n    \\dfrac{x_{p+2} - x_{2}}{p} +\n    \\cdots +\n    \\dfrac{x_{2p} - x_{p}}{p}\n  \\right)\n}{p}\n\\]\nLet \\(s_{1-p} = s_{2-p} = \\cdots = s_{3-p} = 1\\), and set \\(s_1 = s_{1-p}\\).\nCompute the values of \\(a_t\\), \\(b_t\\), and \\(s_t\\) for all rows with observed time series data.\nFind \\(\\hat x_t = a_t \\cdot s_t\\) for all rows with data.\nCompute the prediction \\(\\hat x_{n+k|n} = \\left( a_t + k \\cdot b_n \\right) \\cdot s_{n+k-p}\\) for the future values.\nSuperimpose a sketch of your Holt-Winters filter and the associated forecast on Figure 1.",
    "crumbs": [
      "Lesson 5",
      "Holt-Winters Method (Multiplicative Models)"
    ]
  },
  {
    "objectID": "chapter_3_lesson_5.html#small-group-activity-applying-holt-winters-in-r-to-the-apple-quarterly-revenue-data-10-min",
    "href": "chapter_3_lesson_5.html#small-group-activity-applying-holt-winters-in-r-to-the-apple-quarterly-revenue-data-10-min",
    "title": "Holt-Winters Method (Multiplicative Models)",
    "section": "Small Group Activity: Applying Holt-Winters in R to the Apple Quarterly Revenue Data (10 min)",
    "text": "Small Group Activity: Applying Holt-Winters in R to the Apple Quarterly Revenue Data (10 min)\nRecall the Apple, Inc., revenue values reported by Bloomberg:\n\n\nShow the code\napple_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/apple_revenue.csv\") |&gt;\n  mutate(\n    dates = mdy(date),\n    year = lubridate::year(dates),\n    quarter = lubridate::quarter(dates),\n    value = revenue_billions\n  ) |&gt;\n  dplyr::select(dates, year, quarter, value)  |&gt; \n  arrange(dates) |&gt;\n  mutate(index = tsibble::yearquarter(dates)) |&gt;\n  as_tsibble(index = index) |&gt;\n  dplyr::select(index, dates, year, quarter, value) |&gt;\n  rename(revenue = value) # rename value to emphasize data context\n\napple_ts |&gt;\n  autoplot(.vars = revenue) +\n  labs(\n    x = \"Quarter\",\n    y = \"Apple Revenue, Billions $US\",\n    title = \"Apple's Quarterly Revenue, Billions of U.S. Dollars\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nHere are a few rows of the summarized data.\n\n\n\n\n\nindex\ndates\nyear\nquarter\nrevenue\n\n\n\n\n2005 Q1\n2005-01-25\n2005\n1\n1.24\n\n\n2005 Q2\n2005-04-25\n2005\n2\n0.48\n\n\n2005 Q3\n2005-07-25\n2005\n3\n1.24\n\n\n2005 Q4\n2005-10-30\n2005\n4\n1.71\n\n\n2006 Q1\n2006-01-25\n2006\n1\n2.42\n\n\n2006 Q2\n2006-04-25\n2006\n2\n1.71\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n2023 Q2\n2023-05-04\n2023\n2\n94.836\n\n\n2023 Q3\n2023-08-03\n2023\n3\n81.797\n\n\n2023 Q4\n2023-11-02\n2023\n4\n89.498\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do you notice about this time plot?\n\nDescribe the trend\nIs there evidence of seasonality?\nIs the additive or multiplicative model appropriate?\n\n\n\n\nWe apply Holt-Winters filtering to the quarterly Apple revenue data with a multiplicative model:\n\n\nShow the code\napple_hw &lt;- apple_ts |&gt;\n  # tsibble::fill_gaps() |&gt;\n  model(Additive = ETS(revenue ~\n        trend(\"A\") +\n        error(\"A\") +\n        season(\"M\"),\n        opt_crit = \"amse\", nmse = 1))\nreport(apple_hw)\n\n\nSeries: revenue \nModel: ETS(A,A,M) \n  Smoothing parameters:\n    alpha = 0.8465418 \n    beta  = 0.0142208 \n    gamma = 0.0001000036 \n\n  Initial states:\n       l[0]       b[0]     s[0]     s[-1]    s[-2]     s[-3]\n 0.05455891 -0.0543255 1.003384 0.9450199 1.100051 0.9515453\n\n  sigma^2:  32.8701\n\n     AIC     AICc      BIC \n604.1173 606.8446 625.0939 \n\n\nWe can compute some values to assess the fit of the model:\n\n\nShow the code\n# SS of random terms\nsum(components(apple_hw)$remainder^2, na.rm = T)\n\n# RMSE\nforecast::accuracy(apple_hw)$RMSE\n\n# Standard devation of the quarterly revenues\nsd(apple_ts$revenue)\n\n\n\nThe sum of the square of the random terms is: 2235.1651229.\nThe root mean square error (RMSE) is: 5.423105.\nThe standard deviation of the number of incidents each month is 33.0205.\n\nFigure 2 illustrates the Holt-Winters decomposition of the Apple revenue data.\n\n\nShow the code\nautoplot(components(apple_hw))\n\n\n\n\n\n\n\n\nFigure 2: Apple, Inc., Quarterly Revenue (in Billions)\n\n\n\n\n\nIn Figure 3, we can observe the relationship between the Holt-Winters filter and the Apple revenue time series.\n\n\nShow the code\naugment(apple_hw) |&gt;\n  ggplot(aes(x = index, y = revenue)) +\n    geom_line() +\n    geom_line(aes(y = .fitted, color = \"Fitted\")) +\n    labs(color = \"\")\n\n\n\n\n\n\n\n\nFigure 3: Superimposed plots of the Apple revenue and the Holt-Winters filter\n\n\n\n\n\nFigure 4 contains the information from Figure 3, with the addition of an additional four years of forecasted values. The light blue bands give a 95% prediction bands for the forecast.\n\n\nShow the code\napple_forecast &lt;- apple_hw |&gt;\n  forecast(h = \"4 years\") \n\napple_forecast |&gt;\n  autoplot(apple_ts, level = 95) +\n  geom_line(aes(y = .fitted, color = \"Fitted\"),\n    data = augment(apple_hw)) +\n  scale_color_discrete(name = \"\")\n\n\n\n\n\n\n\n\nFigure 4: Superimposed plots of Apple’s quarterly revenue and the Holt-Winters filter, with four additional years forecasted",
    "crumbs": [
      "Lesson 5",
      "Holt-Winters Method (Multiplicative Models)"
    ]
  },
  {
    "objectID": "chapter_3_lesson_5.html#homework-preview-5-min",
    "href": "chapter_3_lesson_5.html#homework-preview-5-min",
    "title": "Holt-Winters Method (Multiplicative Models)",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_3_5.qmd \n\n\nBYU-Idaho Enrollment\n\n Tables-Handout-Excel-key \n\n\n\n\n\n\n\n\n\n\nTable 1: Holt-Winters smoothing for BYU-Idaho campus enrollments\n\n\n\n\n\n$$Semester$$\n$$t$$\n$$x_t$$\n$$a_t$$\n$$b_t$$\n$$s_t$$\n$$\\hat x_t$$\n\n\n\n\nSP11\n-2\n—\n—\n—\n1\n—\n\n\nFA11\n-1\n—\n—\n—\n1\n—\n\n\nWI12\n0\n—\n—\n—\n1\n—\n\n\nSP12\n1\n13.7\n13.7\n-0.022\n1\n13.7\n\n\nFA12\n2\n16.2\n14.2\n0.082\n1.03\n14.6\n\n\nWI13\n3\n15.5\n14.5\n0.126\n1.01\n14.6\n\n\nSP13\n4\n14\n14.5\n0.101\n0.99\n14.4\n\n\nFA13\n5\n15.6\n14.7\n0.121\n1.04\n15.3\n\n\nWI14\n6\n15.6\n14.9\n0.137\n1.02\n15.2\n\n\nSP14\n7\n12.9\n14.6\n0.05\n0.97\n14.2\n\n\nFA14\n8\n16.2\n14.8\n0.08\n1.05\n15.5\n\n\nWI15\n9\n16.7\n15.2\n0.144\n1.04\n15.8\n\n\nSP15\n10\n13.7\n15.1\n0.095\n0.96\n14.5\n\n\nFA15\n11\n17.6\n15.5\n0.156\n1.07\n16.6\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\nFA22\n32\n19.4\n17.7\n-0.086\n1.12\n19.8\n\n\nWI23\n33\n17.7\n17.5\n-0.109\n1.04\n18.2\n\n\nSP23\n34\n12.8\n16.9\n-0.207\n0.85\n14.4\n\n\nFA23\n35\n18.7\n16.7\n-0.206\n1.12\n18.7\n\n\nWI24\n36\n17.6\n16.6\n-0.185\n1.04\n17.3\n\n\nSP24\n37\n—\n—\n—\n0.85\n14\n\n\nFA24\n38\n—\n—\n—\n1.12\n18.2\n\n\nWI25\n39\n—\n—\n—\n1.04\n16.7\n\n\nSP25\n40\n—\n—\n—\n0.85\n13.5\n\n\nFA25\n41\n—\n—\n—\n1.12\n17.6\n\n\nWI26\n42\n—\n—\n—\n1.04\n16.1\n\n\nSP26\n43\n—\n—\n—\n0.85\n13\n\n\nFA26\n44\n—\n—\n—\n1.12\n16.9\n\n\nWI27\n45\n—\n—\n—\n1.04\n15.5",
    "crumbs": [
      "Lesson 5",
      "Holt-Winters Method (Multiplicative Models)"
    ]
  },
  {
    "objectID": "chapter_3_lesson_5.html#references",
    "href": "chapter_3_lesson_5.html#references",
    "title": "Holt-Winters Method (Multiplicative Models)",
    "section": "References",
    "text": "References\n\nC. C. Holt (1957) Forecasting seasonals and trends by exponentially weighted moving averages, ONR Research Memorandum, Carnegie Institute of Technology 52. (Reprint at https://doi.org/10.1016/j.ijforecast.2003.09.015).\nP. R. Winters (1960). Forecasting sales by exponentially weighted moving averages. Management Science, 6, 324–342. (Reprint at https://doi.org/10.1287/mnsc.6.3.324.)",
    "crumbs": [
      "Lesson 5",
      "Holt-Winters Method (Multiplicative Models)"
    ]
  },
  {
    "objectID": "chapter_3_lesson_BassModel.html",
    "href": "chapter_3_lesson_BassModel.html",
    "title": "Bass Model",
    "section": "",
    "text": "Implement the Bass model\n\n-   Describe the types of times series for which the Bass model is appropriate\n-   Apply the Bass model to a time series"
  },
  {
    "objectID": "chapter_3_lesson_BassModel.html#learning-outcomes",
    "href": "chapter_3_lesson_BassModel.html#learning-outcomes",
    "title": "Bass Model",
    "section": "",
    "text": "Implement the Bass model\n\n-   Describe the types of times series for which the Bass model is appropriate\n-   Apply the Bass model to a time series"
  },
  {
    "objectID": "chapter_3_lesson_BassModel.html#preparation",
    "href": "chapter_3_lesson_BassModel.html#preparation",
    "title": "Bass Model",
    "section": "Preparation",
    "text": "Preparation\n\nRead Section 3.3"
  },
  {
    "objectID": "chapter_3_lesson_BassModel.html#learning-journal-exchange-10-mins",
    "href": "chapter_3_lesson_BassModel.html#learning-journal-exchange-10-mins",
    "title": "Bass Model",
    "section": "Learning Journal Exchange (10 mins)",
    "text": "Learning Journal Exchange (10 mins)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading another student’s?\nWhat would you recommend the other student add to their learning journal?\nSign the Learning Journal review sheet for your peer"
  },
  {
    "objectID": "chapter_3_lesson_BassModel.html#class-activity-interpreting-the-bass-model-30-mins",
    "href": "chapter_3_lesson_BassModel.html#class-activity-interpreting-the-bass-model-30-mins",
    "title": "Bass Model",
    "section": "Class Activity: Interpreting the Bass Model (30 mins)",
    "text": "Class Activity: Interpreting the Bass Model (30 mins)\nThe Bass formula, given as Equation (3.6) in the textbook is:\n\\[\n  N_{t+1} = N_t + p(m-N_t) + q N_t (m-N_t) / m\n\\]\n\nWhat situation is the Bass formula designed to model?\nWhat do the following terms represent?\n\n\\(N_t\\)\n\\(N_{t+1} - N_t\\)\n\\(m\\)\n\\(m-N_t\\)\n\\(\\dfrac{N_{t+1} - N_t}{m-N_t}\\)\n\\(p\\)\n\\(\\dfrac{q N_t}{m}\\)\n\nRearrange Equation (3.6) to find an expression that equals \\(\\dfrac{N_{t+1} - N_t}{m-N_t}\\).\nThe book states that “The rationale for the model is that initial sales will be to people who are interested in the novelty of the product, whereas later sales will be to people who are drawn to the product after seeing their friends and acquaintances use it.” How does the expression that results from rearranging Equation (3.6) accord with this quote?\n\nThe explicit solution to the Bass formula is given in Equation (3.7) of the textbook:\n\\[\n  N_t = m \\dfrac{\n                    1 - e^{-(p+q)t}\n                }{\n                    1 + \\frac{q}{p} e^{-(p+q)t}\n                }\n\\]\n\nUsing any tool (i.e., R, Mathematica, Desmos, graphing calculator), plot Equation (3.7) for various choices of \\(p\\) and \\(q\\).\nVerify that Equation (3.7) is the solution to the Bass formula by doing the following.\n\nUsing Equation (3.7), write down an expression for \\(N_{t+1}\\).\nSubstitute the value for \\(N_t\\) given in Equation (3.7) into the right-hand side of Equation (3.6). Simplify the resulting expression to show that it is equal to \\(N_{t+1}\\)."
  },
  {
    "objectID": "chapter_3_lesson_BassModel.html#class-activity-xxxxxxx-15-mins",
    "href": "chapter_3_lesson_BassModel.html#class-activity-xxxxxxx-15-mins",
    "title": "Bass Model",
    "section": "Class Activity: XXXXXXX (15 mins)",
    "text": "Class Activity: XXXXXXX (15 mins)"
  },
  {
    "objectID": "chapter_3_lesson_BassModel.html#recap-5-min",
    "href": "chapter_3_lesson_BassModel.html#recap-5-min",
    "title": "Bass Model",
    "section": "Recap (5 min)",
    "text": "Recap (5 min)\nWorking with your partner, prepare to explain the following concepts to the class:\nClass Activity\n\nSolutions to Class Activity\n\\[\\begin{align*}\n  N_{t+1}\n    &= m \\dfrac{ 1 - e^{-(p+q)(t+1)} }{ 1 + \\frac{q}{p} e^{-(p+q)(t+1)} } \\\\\n    &= m \\dfrac{ 1 - e^{-(p+q)t}e^{-(p+q)} }{ 1 + \\frac{q}{p} e^{-(p+q)t}e^{-(p+q)} }\n\\end{align*}\\] \\[\\begin{align*}\n  N_{t+1}\n    &= N_t + p(m-N_t) + \\frac{q N_t}{m} (m-N_t) \\\\\n    &= \\left[ m \\dfrac{ 1 - e^{-(p+q)t} }{ 1 + \\frac{q}{p} e^{-(p+q)t} } \\right] + p\\left(m-\\left[ m \\dfrac{ 1 - e^{-(p+q)t} }{ 1 + \\frac{q}{p} e^{-(p+q)t} } \\right]\\right) + \\frac{q N_t}{m} \\left(m-\\left[ m \\dfrac{ 1 - e^{-(p+q)t} }{ 1 + \\frac{q}{p} e^{-(p+q)t} } \\right]\\right)  \n\\end{align*}\\]\nLet’s simplify a common term:\n\\[\\begin{align*}\n  \\left(m-\\left[ m \\dfrac{ 1 - e^{-(p+q)t} }{ 1 + \\frac{q}{p} e^{-(p+q)t} } \\right]\\right)\n    &= m\\left(1-\\left[ \\dfrac{ 1 - e^{-(p+q)t} }{ 1 + \\frac{q}{p} e^{-(p+q)t} } \\right]\\right) \\\\\n    &= m\\left(\\dfrac{ \\left[ 1 + \\frac{q}{p} e^{-(p+q)t} \\right]-\\left[  1 - e^{-(p+q)t} \\right] }{ 1 + \\frac{q}{p} e^{-(p+q)t} } \\right) \\\\\n    &= m\\left(\\dfrac{ \\frac{q}{p} e^{-(p+q)t} + e^{-(p+q)t} }{ 1 + \\frac{q}{p} e^{-(p+q)t} } \\right)\n\\end{align*}\\]\nNote that\n\\[\\begin{align*}\n  \\frac{q N_t}{m} (m-N_t)\n    &= \\frac{q N_t}{m}\n        \\cdot\n        m\\left(\\dfrac{ \\frac{q}{p} e^{-(p+q)t} + e^{-(p+q)t} }{ 1 + \\frac{q}{p} e^{-(p+q)t} } \\right) \\\\\n    &= q\n        \\cdot\n          m \\dfrac{ 1 - e^{-(p+q)t} }{ 1 + \\frac{q}{p} e^{-(p+q)t} }\n        \\cdot\n          \\left(\\dfrac{ \\frac{q}{p} e^{-(p+q)t} + e^{-(p+q)t} }{ 1 + \\frac{q}{p} e^{-(p+q)t} } \\right) \\\\\n\\end{align*}\\]\nPutting it all together, we have:\n\\[\\begin{align*}\n  N_{t+1}\n    &= N_t + p(m-N_t) + \\frac{q N_t}{m} (m-N_t)\\\\\n    &= \\left[ m \\dfrac{ 1 - e^{-(p+q)t} }{ 1 + \\frac{q}{p} e^{-(p+q)t} } \\right]\n        + p\\left(m-\\left[ m \\dfrac{ 1 - e^{-(p+q)t} }{ 1 + \\frac{q}{p} e^{-(p+q)t} } \\right]\\right)\n        + \\frac{q N_t}{m} \\left(m-\\left[ m \\dfrac{ 1 - e^{-(p+q)t} }{ 1 + \\frac{q}{p} e^{-(p+q)t} } \\right]\\right)  \\\\\n    &= \\left[ m \\dfrac{ 1 - e^{-(p+q)t} }{ 1 + \\frac{q}{p} e^{-(p+q)t} } \\right]\n        + p \\cdot m\\left(\\dfrac{ \\frac{q}{p} e^{-(p+q)t} + e^{-(p+q)t} }{ 1 + \\frac{q}{p} e^{-(p+q)t} } \\right)\n        + \\frac{q N_t}{m} \\cdot m\\left(\\dfrac{ \\frac{q}{p} e^{-(p+q)t} + e^{-(p+q)t} }{ 1 + \\frac{q}{p} e^{-(p+q)t} } \\right) \\\\\n    &=\n      \\frac{\n        \\left[ m \\left( 1 - e^{-(p+q)t} \\right) \\right]\n          + p \\cdot m\\left( \\frac{q}{p} e^{-(p+q)t} + e^{-(p+q)t} \\right)\n          + \\frac{q N_t}{m} \\cdot m\\left( \\frac{q}{p} e^{-(p+q)t} + e^{-(p+q)t} \\right)\n      }{ 1 + \\frac{q}{p} e^{-(p+q)t} }\\\\\n    &=\n      \\frac{\n         m - m e^{-(p+q)t}\n          +  mq e^{-(p+q)t} + mp e^{-(p+q)t}\n          + q N_t \\left( \\frac{q}{p} e^{-(p+q)t} + e^{-(p+q)t} \\right)\n      }{ 1 + \\frac{q}{p} e^{-(p+q)t} }\\\\\n    &=\n\\end{align*}\\]\n\\[\\begin{align*}\n~\\\\\n\\end{align*}\\]"
  },
  {
    "objectID": "chapter_4_lesson_1.html",
    "href": "chapter_4_lesson_1.html",
    "title": "White Noise and Random Walks - Part 1",
    "section": "",
    "text": "Characterize the properties of discrete white noise\n\n\nDefine Residual error\nDefine discrete white noise (DWN)\nDefine Gaussian white noise\nSimulate Gaussian white noise with R\nPlot DWN simulation results\nState DWN second order properties\nExplain how to estimate (or fit) a DWN process\nState the assumptions needed to categorize residual error series as white noise\n\n\n\n\nCharacterize the properties of a random walk\n\n\nDefine a random walk\n\n\n\n\nSimulate realizations from basic time series models in R\n\n\nSimulate a random walk\nPlot a random walk",
    "crumbs": [
      "Lesson 1",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "chapter_4_lesson_1.html#learning-outcomes",
    "href": "chapter_4_lesson_1.html#learning-outcomes",
    "title": "White Noise and Random Walks - Part 1",
    "section": "",
    "text": "Characterize the properties of discrete white noise\n\n\nDefine Residual error\nDefine discrete white noise (DWN)\nDefine Gaussian white noise\nSimulate Gaussian white noise with R\nPlot DWN simulation results\nState DWN second order properties\nExplain how to estimate (or fit) a DWN process\nState the assumptions needed to categorize residual error series as white noise\n\n\n\n\nCharacterize the properties of a random walk\n\n\nDefine a random walk\n\n\n\n\nSimulate realizations from basic time series models in R\n\n\nSimulate a random walk\nPlot a random walk",
    "crumbs": [
      "Lesson 1",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "chapter_4_lesson_1.html#preparation",
    "href": "chapter_4_lesson_1.html#preparation",
    "title": "White Noise and Random Walks - Part 1",
    "section": "Preparation",
    "text": "Preparation\n\nRead Sections 4.1-4.2, 4.3.1-4.3.5",
    "crumbs": [
      "Lesson 1",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "chapter_4_lesson_1.html#learning-journal-exchange-10-min",
    "href": "chapter_4_lesson_1.html#learning-journal-exchange-10-min",
    "title": "White Noise and Random Walks - Part 1",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading another student’s?\nWhat would you recommend the other student add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 1",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "chapter_4_lesson_1.html#class-activity-white-noise-15-min",
    "href": "chapter_4_lesson_1.html#class-activity-white-noise-15-min",
    "title": "White Noise and Random Walks - Part 1",
    "section": "Class Activity: White Noise (15 min)",
    "text": "Class Activity: White Noise (15 min)\n\nDefinition\nIn this class, we are learning to investigate different types of time series. Up to this point, we have focused mostly on time series with distinct seasonal behavior. We will not focus on what are called stochastic processes or random processes, where there is not necessarily a seasonal component. We first focus on white noise.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nBased on your understanding from the reading, explain the concept of white noise to your partner.\nCan you give an example of a time series that would represent white noise?\n\n\n\n\n\n\n\n\n\nDefinition of a Discrete White Noise (DWN) Process\n\n\n\nA time series \\(\\{w_t: t = 1, 2, \\ldots, n\\}\\) is a discrete white noise (DWN) if the variables \\(w_1, w_2, \\ldots, w_n\\) are independent and identically distributed with mean 0. The assumption that the variables are identically distributed implies that there is a common variance denoted \\(\\sigma\\). The assumption of independence means that the covariance (and correlation) between different variables will be zero: \\(cov(w_i, w_j) = 0\\) and \\(cor(w_i, w_j) = 0\\) if \\(i \\ne j\\).\nIf the variables are normally distributed, i.e. \\(w_i \\sim N(0,\\sigma^2)\\), the DWN is called a Gaussian white noise process. The normal distribution is also known as the Gaussian distribution, after Carl Friedrich Gauss.\n\n\n\n\nSimulation\nThe following simulation illustrates a white noise time series.\n \n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do you notice about this time series?\nWhat characteristics do you observe in the correlogram?\n\n\n\n\n\nType I Errors\nIn your introductory statistics course, you probably learned about Type I error. Here is a quick refresher.\n\n\n\n\n\n\nType I Errors\n\n\n\nSuppose we will conduct a hypothesis test with a level of significance equal to \\(\\alpha = 0.05\\). If the null hypothesis is true, there is a probability of 0.05 that we will reject the null hypothesis. Due to sampling variation, we will reject a true null hypothesis 5% of the time. We refer to this as making a Type I Error.\n\n\nWhen we create a correlogram, we actually conduct one hypothesis test for each value of \\(k\\). With so many hypothesis tests, it is not surprising if some of them show a significant correlation due to chance alone. In this case, we tend to disregard correlations that are barely significant and inexplicable.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nDo the following with a partner:\n\nClick on the [Simulate!] button above to generate a new simulated realization of the DWN process.\nOut of the 20 autocorrelations represented in the correlogram, count the number that are statistically significant.\nRepeat Steps 1. and 2. ten times, so you will have displayed 200 autocorrelations.\n\n\nWhat percentage of your autocorrelations were statistically significant?\nCompare your results with other teams.\nWhat percentage of these would you expect to be statistically significant, assuming the true autocorrelations are all zero?\n\n\n\n\n\nVisualizing White Noise\nThe data in the file white_noise.parquet were generated by a Gaussian white noise process.\n\n\nShow the code\n# This code was used to create the white noise data file\n\n# Set random seed\nset.seed(10)\n\n# Specify means and standard deviation\nn &lt;- 2500                           # number of points\nwhite_noise_sigma &lt;- rnorm(1, 5, 1) # choose a random standard deviation\n\n# Simulate normal data\ndata.frame(x = rnorm(n, 0, white_noise_sigma)) |&gt;\n  rio::export(\"data/white_noise.parquet\")\n\n\n\n# White noise data\nwhite_noise_df &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/white_noise.parquet\")\n\nThe first 250 points in this time series are illustrated here:\n\n\nShow the code\nwhite_noise_df |&gt; \n  mutate(t = 1:nrow(white_noise_df)) |&gt;\n  head(250) |&gt;  \n  ggplot(aes(x = t, y = x)) + \n    geom_line() +\n    theme_bw() +\n    labs(\n      x = \"Time\",\n      y = \"Values\",\n      title = \"First 250 Values of a Gaussian White Noise Time Series\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nHere is a histogram of the 2500 values from this DWN distribution.\n\n\nShow the code\nwhite_noise_df |&gt;\n  mutate(density = dnorm(x, mean(white_noise_df$x), sd(white_noise_df$x))) |&gt;\n  ggplot(aes(x = x)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"#56B4E9\", binwidth = 1) +\n    geom_line(aes(x = x, y = density)) +\n    theme_bw() +\n    labs(\n      x = \"Values\",\n      y = \"Frequency\",\n      title = \"Histogram of Values from a Gaussian White Noise Process\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nNotice that the values follow a normal distribution. This suggests the data are from a Gaussian white noise distribution.\n\n\nSecond-Order Properties of Discrete White Noise\nWhen we refer to the second-order properties of a time series, we are talking about its variance and covariance. The mean is a first-order property, the covariance is a second-order property.\n\n\n\n\n\n\nSecond-Order Properties of a Discrete White Noise Process\n\n\n\nIf \\(\\{w_t\\}_{t=1}^n\\) is a DWN time series, then the population has the following properties.\n\\[ \\mu_w = 0 \\] and \\[\n  cov(w_t, w_{t+k}) =\n    \\begin{cases}\n      \\sigma^2, & k = 0 \\\\\n      0,        & k \\ne 0\n    \\end{cases}\n\\] The correlation function is therefore\n\\[\n  \\rho_k =\n    \\begin{cases}\n      1, & k = 0 \\\\\n      0, & k \\ne 0\n    \\end{cases}\n\\]\n\n\nNote that the properties given above are theoretical properties of the population, not estimates computed using a sample. The sample autocorrelations will not equal zero, due to randomness inherent in sampling.\n\n\nFitting the White Noise Model\nTypically, a DWN series arises in the random component of another time series. If we have fully explained the level and seasonality in the time series, then the only component left is the random component, which would ideally follow a DWN process.\n\n\n\n\n\n\nIdentifying of a Discrete White Noise Process\n\n\n\nA DWN process will have the following properties:\n\nThere is a discrete observations.\nThe mean of the observations is zero.\nThe variance of the observations is finite.\nSuccessive observations are uncorrelated.\n\n\n\nSince the mean of a DWN time series is zero, the only parameter we need to fit is the variance.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nCompute the sample variance for the DWN data in the file white_noise.parquet.",
    "crumbs": [
      "Lesson 1",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "chapter_4_lesson_1.html#class-activity-random-walks-15-min",
    "href": "chapter_4_lesson_1.html#class-activity-random-walks-15-min",
    "title": "White Noise and Random Walks - Part 1",
    "section": "Class Activity: Random Walks (15 min)",
    "text": "Class Activity: Random Walks (15 min)\n\nDefinitions\nConsider moving on a number line, where your movements are determined by a discrete white noise (DWN) process. Each successive value indicates how far you will move along the number line from your current position. This is mathematically equivalent to allowing your position at time \\(t\\) to be the sum of all the observed DWN values up to time \\(t\\).\n\n\n\n\n\n\nDefinition of a Random Walk\n\n\n\nLet \\(\\{x_t\\}\\) be a time series. Then, \\(\\{x_t\\}\\) is a random walk if it can be expressed as \\[\n  x_{t} = x_{t-1} + w_{t}\n\\] where \\(\\{w_t\\}\\) is a random process.\n\n\nThe value \\(x_t\\) can be considered as the cumulative summation of the first \\(t\\) values of the \\(w_t\\) series. In many cases, \\(w_t\\) is a discrete white noise series, and it is often modeled as a Gaussian white noise series. However, \\(w_t\\) could be as simple as a coin toss, as illustrated in the next activity.\n\n\nSimulating a Random Walk\nIn this activity, we will simulate a discrete-time, discrete-space random walk.\n\n\n\n\n\n\nDo the following:\n\nStart the time series at \\(x_0 = 0\\).\nToss a coin.\n\nIf the coin shows heads, then \\(x_t = x_{t-1}+1\\)\nIf the coin shows tails, then \\(x_t = x_{t-1}-1\\)\n\nPlot the new point on the time plot.\nComplete steps 2 and 3 a total of \\(n=60\\) times. (One realization is illustrated below.)\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nHow would you describe a random walk to someone who has not taken this class?\nHow is a random walk related to a discrete white noise (DWN) process?\nGive a real-world example of a process that could be modeled by a random walk.\n\n\n\n\n\nRepresentations for a Random Walk\nRecall the definition of a random walk:\n\\(\\{x_t\\}\\) is a random walk if it can be expressed as \\[\n  x_{t} = x_{t-1} + w_{t}\n\\] where \\(\\{w_t\\}\\) is a white noise series.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nThere are other ways to represent a random walk.\n\nNotice that \\[\\begin{align*}\n  x_{t} &= x_{t-1} + w_{t} \\\\\n  x_{t-1} &= x_{t-2} + w_{t-1} \\\\\n  ⋮ ~~~ & ~~~~~~~~~~~~~~~ ⋮\n\\end{align*}\\] Use this to write \\(x_t\\) in terms of \\(x_{t-2}\\), \\(w_t\\), and \\(w_{t-1}\\).\nWrite \\(x_t\\) in terms of \\(x_{t-3}\\), \\(w_t\\), \\(w_{t-1}\\), and \\(w_{t-2}\\).\nExplain why it is possible to write \\(x_t\\) as \\[\n  x_{t} = \\sum\\limits_{i=-\\infty}^{t} w_{i} = w_{t} + w_{t-1} + w_{t-2} + w_{t-3} + \\cdots\n\\] where \\(\\{w_t\\}\\) is a DWN time series.\nNote that if the random walk is finite, we can write \\(x_t\\) as: \\[\n  x_{t} = w_1 + w_2 + w_3 + \\cdots + w_{t-3} + w_{t-2} + w_{t-1} + w_{t}\n\\] where \\(x_1=w_1\\).",
    "crumbs": [
      "Lesson 1",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "chapter_4_lesson_1.html#class-activity-backward-shift-operator-10-min",
    "href": "chapter_4_lesson_1.html#class-activity-backward-shift-operator-10-min",
    "title": "White Noise and Random Walks - Part 1",
    "section": "Class Activity: Backward Shift Operator (10 min)",
    "text": "Class Activity: Backward Shift Operator (10 min)\n\nDefinition of the Backward Shift Operator\nThis process of back substitution is so common, we define notation to handle it.\n\n\n\n\n\n\nDefinition of the Backward Shift Operator\n\n\n\nWe define the backward shift operator or the lag operator, \\(\\mathbf{B}\\), as: \\[\n  \\mathbf{B} x_t = x_{t-1}\n\\] where \\(\\{x_t\\}\\) is any time series.\nWe can apply this operator repeatedly. We will use exponential notation to indicate this.\n\\[\n  \\mathbf{B}^2 x_t = \\mathbf{B} \\mathbf{B} x_t = \\mathbf{B} ( \\mathbf{B} x_t ) = \\mathbf{B} x_{t-1} = x_{t-2}\n\\]\nIn general, \\[\n  \\mathbf{B}^n x_t = \\underbrace{\\mathbf{B} \\cdot \\mathbf{B} \\cdot \\cdots \\cdot \\mathbf{B}}_{n ~ \\text{terms}} x_t = \\mathbf{B}^{n-1} ( \\mathbf{B} x_t ) = \\mathbf{B}^{n-1} ( x_{t-1} ) = \\mathbf{B}^{n-2} ( x_{t-2} ) = \\cdots = \\mathbf{B} x_{t-(n-1)} = x_{t-n}\n\\]\n\n\n\n\nProperties of the Backshift Operator\nThe backwards shift operator is a linear operator. So, if \\(a\\), \\(b\\), \\(c\\), and \\(d\\) are constants, then \\[\n(a \\mathbf{B} + b)x_t = a \\mathbf{B} x_t + b x_t\n\\] The distributive property also holds. \\[\\begin{align*}\n(a \\mathbf{B} + b)(c \\mathbf{B} + d) x_t\n  &= c (a \\mathbf{B} + b) \\mathbf{B} x_t  + d(a \\mathbf{B} + b) x_t \\\\\n  &= a \\mathbf{B} (c \\mathbf{B} + d) x_t + b (c \\mathbf{B} + d) x_t \\\\\n  &= \\left( ac \\mathbf{B}^2 + (ad+bc) \\mathbf{B} + bd \\right) x_t \\\\\n  &= ac \\mathbf{B}^2 x_t + (ad+bc) \\mathbf{B} x_t + (bd) x_t\n\\end{align*}\\]\nWe will practice applying this operator.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nLet \\(\\{x_t\\}\\) be a time series with the following values.\n\n\\(x_{1} = 5\\),\\(~\\) \\(x_{2} = 10\\),\\(~\\) \\(x_{3} = 13\\),\\(~\\) \\(x_{4} = 8\\),\\(~\\) \\(x_{5} = 4\\),\\(~\\) \\(x_{6} = 3\\),\\(~\\) \\(x_{7} = 9\\),\\(~\\) \\(x_{8} = 2\\)\n\nEvaluate the following.\n\n\\(\\mathbf{B} x_8\\)\n\\(\\mathbf{B}^5 x_8\\)\n\\((\\mathbf{B}^5 - \\mathbf{B} ) x_8\\)\n\\(( \\mathbf{B}^2 - 6 \\mathbf{B} + 9 ) x_8\\)\n\\(( (\\mathbf{B} - 6 )\\mathbf{B} + 9 ) x_8\\)\n\\(( \\mathbf{B} - 3 )^2 x_8 =  ( \\mathbf{B} - 3 ) \\left[ ( \\mathbf{B} - 3 ) x_8 \\right]\\)\n\\(( 1 - \\frac{1}{2} \\mathbf{B} - \\frac{1}{4} \\mathbf{B}^2 - \\frac{1}{8} \\mathbf{B}^3 ) x_8\\)",
    "crumbs": [
      "Lesson 1",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "chapter_4_lesson_1.html#class-activity-properties-of-random-walks-5-min",
    "href": "chapter_4_lesson_1.html#class-activity-properties-of-random-walks-5-min",
    "title": "White Noise and Random Walks - Part 1",
    "section": "Class Activity: Properties of Random Walks (5 min)",
    "text": "Class Activity: Properties of Random Walks (5 min)\n\nSimulation\nThe following simulation illustrates a random walk.\n \n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do you notice about this time series?\nWhat characteristics do you observe in the correlogram?\nHow does this compare to the time series and correlogram for the DWN process?\n\n\n\n\n\nSecond-Order Properties of a Random Walk\nThe second-order properties of a random walk are summarized below.\n\n\n\n\n\n\nSecond-Order Properties of a Random Walk\n\n\n\nIf \\(\\{x_t\\}_{t=1}^n\\) is a random walk, then the population has the following properties.\n\\[ \\mu_x = 0 \\] and \\[\n  cov(x_t, x_{t+k}) = t \\sigma^2\n\\]\n\n\n\n\n\n\nClick here for a proof of the equation for \\(cov(x_t,x_{t+k})\\)\n\n\n\n\n\nWhy is \\(cov(x_t, x_{t+k}) = t \\sigma^2\\)?\nFirst, note that that since the terms in the white noise series are independent,\n\\[\ncov ( w_i, w_j ) =\n  \\begin{cases}\n    \\sigma^2, & \\text{if } ~ i=j \\\\\n    0, & \\text{otherwise}\n  \\end{cases}\n\\]\nAlso, when random variables are independent, the covariance of a sum is the sum of the covariance.\nHence, \\[\\begin{align*}\n  cov(x_t, x_{t+k})\n    &= cov ( \\sum_{i=1}^t w_i, \\sum_{j=1}^{t+K} w_j ) \\\\\n    &= \\sum_{i=j} cov ( w_i, w_j ) \\\\\n    &= \\sum_{i=1}^t \\sigma^2 \\\\\n    &= t \\sigma^2\n\\end{align*}\\]\n\n\n\nIf \\(k&gt;0\\) and \\(t&gt;0\\), the correlation function is\n\\[\n  \\rho_k\n  =\n    \\frac{\n            cov(x_t, x_{t+k})\n          }{\n            \\sqrt{var(x_t)} \\sqrt{var(x_{t+k})}\n          }\n  =\n    \\frac{t \\sigma^2}{\\sqrt{t \\sigma^2} \\sqrt{(t+k) \\sigma^2}}\n  =\n    \\frac{1}{\\sqrt{1+\\frac{k}{t}}}\n\\]\n\n\nNote that the covariance of a random walk process depends on \\(t\\). Hence, random walks are non-stationary. The variance is unbounded as \\(t\\) increases. That implies a random walk will not provide good predictions in the long term.\nNote that if \\(0 &lt; k \\ll t\\), then \\(\\rho_k \\approx 1\\). Because of this, a correlogram for a random walk will typically demonstrate positive autocorrelations that start near 1 and slowly decrease as \\(k\\) increases. This is exactly what we observed in the simulation above.",
    "crumbs": [
      "Lesson 1",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "chapter_4_lesson_1.html#homework-preview-5-min",
    "href": "chapter_4_lesson_1.html#homework-preview-5-min",
    "title": "White Noise and Random Walks - Part 1",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_4_1.qmd \n\n\nWhite Noise\n\nThe sample variance of the DWN data is computed using the R command var(white_noise_df$x) as 26.29.\n\nBackward Shift Operator\n\n\n\\(x_{1} = 5\\),\\(~\\) \\(x_{2} = 10\\),\\(~\\) \\(x_{3} = 13\\),\\(~\\) \\(x_{4} = 8\\),\\(~\\) \\(x_{5} = 4\\),\\(~\\) \\(x_{6} = 3\\),\\(~\\) \\(x_{7} = 9\\),\\(~\\) \\(x_{8} = 2\\)\n\nCheck Your Understanding Solutions:\n\n\\(\\mathbf{B} x_8 = x_7 = 9\\)\n\\(\\mathbf{B}^5 x_8 = x_3 = 13\\)\n\\((\\mathbf{B}^5 - \\mathbf{B} ) x_8 = x_3 - x_7 = 13 - 9 = -1\\)\n\\(( \\mathbf{B}^2 - 6 \\mathbf{B} + 9 ) x_8 = x_6 - 6 (x_7) + 9 (x_{t}) = 3 - 6 (9) + 9 (2) = -33\\)\n\\(( (\\mathbf{B} - 6) \\mathbf{B} + 9 ) x_8 = -33\\)\n\\(( \\mathbf{B} - 3 )^2 x_8 = -33\\)\n\\(( 1 - \\frac{1}{2} B - \\frac{1}{4} B^2 - \\frac{1}{8} B^3 ) x_8 = x_8 - \\frac{1}{2} x_7 - \\frac{1}{4} x_6 - \\frac{1}{8} x_5 = 2 - \\frac{1}{2} (9) - \\frac{1}{4} (3) - \\frac{1}{8} (4) = -3.75\\)",
    "crumbs": [
      "Lesson 1",
      "White Noise and Random Walks - Part 1"
    ]
  },
  {
    "objectID": "chapter_4_lesson_2.html",
    "href": "chapter_4_lesson_2.html",
    "title": "White Noise and Random Walks - Part 2",
    "section": "",
    "text": "Characterize the properties of a random walk\n\n\nDefine the second order properties of a random walk\nDefine the backward shift operator\nUse the backward shift operator to state a random walk as a sequence of white noise realizations\nDefine a random walk with drift\n\n\n\n\nSimulate realizations from basic time series models in R\n\n\nSimulate a random walk\nPlot a random walk\n\n\n\n\nFit time series models to data and interpret fitted parameters\n\n\nMotive the need for differencing in time series analysis\nDefine the difference operator\nExplain the relationship between the difference operator and the backward shift operator\nTest whether a series is a random walk using first differences\nExplain how to estimate a random walk with increasing slope using Holt-Winters\nEstimate the drift parameter of a random walk",
    "crumbs": [
      "Lesson 2",
      "White Noise and Random Walks - Part 2"
    ]
  },
  {
    "objectID": "chapter_4_lesson_2.html#learning-outcomes",
    "href": "chapter_4_lesson_2.html#learning-outcomes",
    "title": "White Noise and Random Walks - Part 2",
    "section": "",
    "text": "Characterize the properties of a random walk\n\n\nDefine the second order properties of a random walk\nDefine the backward shift operator\nUse the backward shift operator to state a random walk as a sequence of white noise realizations\nDefine a random walk with drift\n\n\n\n\nSimulate realizations from basic time series models in R\n\n\nSimulate a random walk\nPlot a random walk\n\n\n\n\nFit time series models to data and interpret fitted parameters\n\n\nMotive the need for differencing in time series analysis\nDefine the difference operator\nExplain the relationship between the difference operator and the backward shift operator\nTest whether a series is a random walk using first differences\nExplain how to estimate a random walk with increasing slope using Holt-Winters\nEstimate the drift parameter of a random walk",
    "crumbs": [
      "Lesson 2",
      "White Noise and Random Walks - Part 2"
    ]
  },
  {
    "objectID": "chapter_4_lesson_2.html#preparation",
    "href": "chapter_4_lesson_2.html#preparation",
    "title": "White Noise and Random Walks - Part 2",
    "section": "Preparation",
    "text": "Preparation\n\nRead Sections 4.3.6-4.3.7 and 4.4",
    "crumbs": [
      "Lesson 2",
      "White Noise and Random Walks - Part 2"
    ]
  },
  {
    "objectID": "chapter_4_lesson_2.html#learning-journal-exchange-10-min",
    "href": "chapter_4_lesson_2.html#learning-journal-exchange-10-min",
    "title": "White Noise and Random Walks - Part 2",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading another student’s?\nWhat would you recommend the other student add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 2",
      "White Noise and Random Walks - Part 2"
    ]
  },
  {
    "objectID": "chapter_4_lesson_2.html#class-activity-differencing-a-time-series-15-min",
    "href": "chapter_4_lesson_2.html#class-activity-differencing-a-time-series-15-min",
    "title": "White Noise and Random Walks - Part 2",
    "section": "Class Activity: Differencing a Time Series (15 min)",
    "text": "Class Activity: Differencing a Time Series (15 min)\n\nExample: McDonald’s Stock Prices\nComputing the difference between successive terms of a random walk leads to a discrete white noise series.\n\\[\\begin{align*}\nx_t &= x_{t-1} + w_t \\\\\nx_t - x_{t-1} &= w_t\n\\end{align*}\\]\nIn many cases, differencing sequential terms of a non-stationary process can lead to a stationary process of differences. We can use the code below to obtain the daily closing stock prices for any publicly-traded company.\n\n\nShow the code\n# Set symbol and date range\nsymbol &lt;- \"MCD\"\ncompany &lt;- \"McDonald's\"\ndate_start &lt;- \"2020-07-01\"\ndate_end &lt;- \"2024-01-01\"\n\n# Fetch stock prices (can be used to get new data)\nstock_df &lt;- tq_get(symbol, from = date_start, to = date_end, get = \"stock.prices\")\n\n# Transform data into tsibble\nstock_ts &lt;- stock_df %&gt;%\n  mutate(\n    dates = date, \n    value = adjusted\n  ) %&gt;%\n  dplyr::select(dates, value) %&gt;%\n  as_tibble() %&gt;% \n  arrange(dates) |&gt;\n  mutate(diff = value - lag(value)) |&gt;\n  as_tsibble(index = dates, key = NULL)\n\n\nThe time plot in Figure 1 presents the closing price of McDonald’s stock from 01/07/2020 to 01/01/2024. Figure 2 gives the differences in the closing prices of the stock as a time series.\n\n\n\n\n\nShow the code\nplot_ly(stock_ts, x = ~dates, y = ~value, type = 'scatter', mode = 'lines') %&gt;%\n  layout(\n    xaxis = list(title = paste0(\"Dates (\", format(ymd(date_start), \"%d/%m/%Y\"), \" to \", format(ymd(date_end), \"%d/%m/%Y\"), \")\" ) ),\n    yaxis = list(title = \"Closing Price (US$)\"),\n    title = paste0(\"Time Plot of \", symbol, \" Daily Closing Price\")\n  )\n\n\n\n\n\n\n\n\nFigure 1: Plot of the daily closing price of the stock\n\n\n\n\n\n\n\n\n\nShow the code\n# Generate time series plot using plot_ly\nplot_ly(stock_ts, x = ~dates, y = ~diff, type = 'scatter', mode = 'lines') %&gt;%\n  layout(\n    xaxis = list(title = paste0(\"Dates (\", format(ymd(date_start), \"%d/%m/%Y\"), \" to \", format(ymd(date_end), \"%d/%m/%Y\"), \")\" ) ),\n    yaxis = list(title = \"Closing Price (US$)\"),\n    title = paste0(\"Difference of \", symbol, \" Daily Closing Price\")\n  )\n\n\n\n\n\n\n\n\nFigure 2: Plot of the stock price differences\n\n\n\n\n\n\n\nFigure 3 is the correlogram for the original McDonald’s stock price time series. Figure 4 gives the correlogram for the differences in successive closing stock prices.\n\n\n\n\n\nShow the code\nacf(stock_ts$value, plot=TRUE, type = \"correlation\", lag.max = 25)\n\n\n\n\n\n\n\n\nFigure 3: Correlogram of the stock prices\n\n\n\n\n\n\n\n\n\n\nShow the code\nacf(stock_ts$diff |&gt; na.omit(), plot=TRUE, type = \"correlation\", lag.max = 25)\n\n\n\n\n\n\n\n\nFigure 4: Correlogram of the stock prices\n\n\n\n\n\n\n\n\nFigure 5 is a histogram of the differences. On the right, we give the variance of the differences in the stock prices. This is a simple measure of the volatility of the stock, or in other words, how much the price changes in a day.\n\n\n\n\n\n\n\n\n\nShow the code\n# Histogram of differences in stock prices\nstock_ts |&gt;\n  mutate(\n    density = dnorm(diff, mean(stock_ts$diff, na.rm = TRUE), sd(stock_ts$diff, na.rm = TRUE))\n  ) |&gt;\n  ggplot(aes(x = diff)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"#56B4E9\", binwidth = 1) +\n    geom_line(aes(x = diff, y = density)) +\n    theme_bw() +\n    labs(\n      x = \"Difference\",\n      y = \"Frequency\",\n      title = \"Histogram of Difference in the Closing Stock Prices\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\nFigure 5: Histogram of the stock price differences\n\n\n\n\n\nThe variance of the differences is 5.783.\n\n\n\nNotice that the values in the correlogram of the stock prices start at 1 and slowly decay as \\(k\\) increases. There are no significant autocorrelations in the differenced values. This is exactly what we would expect from a random walk. It is also interesting that the differences are nearly normally distributed and uncorrelated.\n\n\nDefintion of the Difference Operator\nDifferencing nonstationary time series often leads to a stationary series, so we will define a formal operator to express this process.\n\n\n\n\n\n\nDefinition of the Difference Operator\n\n\n\nThe difference operator, \\(\\nabla\\), is defined as:\n\\[\\nabla x_t = x_t - x_{t-1} = (1-\\mathbf{B}) x_t\\]\nHigher-order differencing can be denoted\n\\[\\nabla^n x_t = (1-\\mathbf{B})^n x_t\\]\n\n\nTo see what this expression gives us, note that \\(\\nabla\\) gives a new time series that is comprised of the differences between successive terms of the original time series. The operator \\(\\nabla^2\\) generates a time series that is comprised of the differences between successive terms of the differenced time series. It is the difference of the differences, or the second difference.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n Tables-Handout-Excel \nConsider the following time series, where \\(n=8\\):\n\n\\(x_{1} = 5\\),\\(~\\) \\(x_{2} = 10\\),\\(~\\) \\(x_{3} = 13\\),\\(~\\) \\(x_{4} = 8\\),\\(~\\) \\(x_{5} = 4\\),\\(~\\) \\(x_{6} = 3\\),\\(~\\) \\(x_{7} = 9\\),\\(~\\) \\(x_{8} = 2\\)\n\n\n\n\n\nFind the first differences, \\(\\nabla x_t\\)\nFind the second differences, \\(\\nabla^2 x_t\\).\nFill in the missing steps: \\[\\begin{align*}\n\\nabla^2 x_8 &= (1-\\mathbf{B} )^2 x_8 \\\\\n  &= (1-\\mathbf{B} ) \\left[ (1-\\mathbf{B} ) x_8 \\right] \\\\\n  &  ~~~~~~~~~~~~~~~~~~~~~~ ⋮ \\\\\n  &= (x_8-x_7)-(x_7-x_6)\n\\end{align*}\\] and check that this is equal to the last term in the sequence of second differences.\n\n\n\n\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$\\nabla x_t$$\n$$\\nabla^2 x_t$$\n\n\n\n\n1\n5\n\n\n\n\n2\n10\n\n\n\n\n3\n13\n\n\n\n\n4\n8\n\n\n\n\n5\n4\n\n\n\n\n6\n3\n\n\n\n\n7\n9\n\n\n\n\n8\n2\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSmall-Group Activity: Computing Differences\nThe difference operator can be helpful in identifying the functional underpinnings of a trend. If a function is linear, then the first differences of equally-spaced values will be constant. If a function is quadratic, then the second differences of equally-spaced values will be constant. If a function is cubic, then the third differences of equally-spaced values will be constant, and so on.\nCompute the differences specified below.\n\nLinear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$\\nabla x_t$$\n\n\n\n\n\n\n1\n7.5\n\n\n\n\n\n2\n10\n\n\n\n\n\n3\n12.5\n\n\n\n\n\n4\n15\n\n\n\n\n\n5\n17.5\n\n\n\n\n\n6\n20\n\n\n\n\n\n7\n22.5\n\n\n\n\n\n8\n25\n\n\n\n\n\n9\n27.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuadratic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$\\nabla x_t$$\n$$\\nabla^2 x_t$$\n\n\n\n\n\n1\n15\n\n\n\n\n\n2\n2\n\n\n\n\n\n3\n-7\n\n\n\n\n\n4\n-12\n\n\n\n\n\n5\n-13\n\n\n\n\n\n6\n-10\n\n\n\n\n\n7\n-3\n\n\n\n\n\n8\n8\n\n\n\n\n\n9\n23\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCubic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$\\nabla x_t$$\n$$\\nabla^2 x_t$$\n$$\\nabla^3 x_t$$\n\n\n\n\n1\n-2.7\n\n\n\n\n\n2\n0\n\n\n\n\n\n3\n1\n\n\n\n\n\n4\n0.9\n\n\n\n\n\n5\n0.3\n\n\n\n\n\n6\n-0.2\n\n\n\n\n\n7\n0\n\n\n\n\n\n8\n1.5\n\n\n\n\n\n9\n4.9",
    "crumbs": [
      "Lesson 2",
      "White Noise and Random Walks - Part 2"
    ]
  },
  {
    "objectID": "chapter_4_lesson_2.html#small-group-activity-differencing-stock-prices-15-min",
    "href": "chapter_4_lesson_2.html#small-group-activity-differencing-stock-prices-15-min",
    "title": "White Noise and Random Walks - Part 2",
    "section": "Small-Group Activity: Differencing Stock Prices (15 min)",
    "text": "Small-Group Activity: Differencing Stock Prices (15 min)\nIn this activity, you will apply what you have learned to a new stock.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nModify the code used to get the prices of McDonald’s stock to download closing stock prices for a different publicly-traded company over a time period of your choice.\n\n\nShow the code\n# Set symbol and date range\nsymbol &lt;- \"MCD\"               # Stock trading symbol for McDonald's\ndate_start &lt;- \"2020-07-01\"\ndate_end &lt;- \"2024-01-01\"\n\n# Fetch stock prices (can be used to get new data)\nstock_df &lt;- tq_get(symbol, from = date_start, to = date_end, get = \"stock.prices\")\n\n# Transform data into tsibble\nstock_ts &lt;- stock_df %&gt;%\n  mutate(\n    dates = date, \n    value = adjusted\n  ) %&gt;%\n  dplyr::select(dates, value) %&gt;%\n  as_tibble() %&gt;% \n  arrange(dates) |&gt;\n  mutate(diff = value - lag(value)) |&gt;\n  as_tsibble(index = dates, key = NULL)\n\n\nplot_ly(stock_ts, x = ~dates, y = ~value, type = 'scatter', mode = 'lines') %&gt;%\n  layout(\n    xaxis = list(title = paste0(\"Dates (\", format(ymd(date_start), \"%d/%m/%Y\"), \" to \", format(ymd(date_end), \"%d/%m/%Y\"), \")\" ) ),\n    yaxis = list(title = \"Closing Price (US$)\"),\n    title = paste0(\"Time Plot of \", symbol, \" Daily Closing Price\")\n  )\n\n# Generate time series plot using plot_ly\nplot_ly(stock_ts, x = ~dates, y = ~diff, type = 'scatter', mode = 'lines') %&gt;%\n  layout(\n    xaxis = list(title = paste0(\"Dates (\", format(ymd(date_start), \"%d/%m/%Y\"), \" to \", format(ymd(date_end), \"%d/%m/%Y\"), \")\" ) ),\n    yaxis = list(title = \"Closing Price (US$)\"),\n    title = paste0(\"Difference of \", symbol, \" Daily Closing Price\")\n)\n\n# Autocorrelation function for stock prices\nacf(stock_ts$value, plot=TRUE, type = \"correlation\", lag.max = 25)\n\n# Autocorrelation function for differences\nacf(stock_ts$diff |&gt; na.omit(), plot=TRUE, type = \"correlation\", lag.max = 25)\n\n# Histogram of differences in stock prices\nstock_ts |&gt;\n  mutate(\n    density = dnorm(diff, mean(stock_ts$diff, na.rm = TRUE), sd(stock_ts$diff, na.rm = TRUE))\n  ) |&gt;\n  ggplot(aes(x = diff)) +\n  geom_histogram(aes(y = after_stat(density)),\n                 color = \"white\", fill = \"#56B4E9\", binwidth = 1) +\n  geom_line(aes(x = diff, y = density)) +\n  theme_bw() +\n  labs(\n    x = \"Difference\",\n    y = \"Frequency\",\n    title = \"Histogram of Difference in the Closing Stock Prices\"\n  ) +\n  theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n\n# Variance of the differences\nvar(stock_ts$diff, na.rm = TRUE)|&gt; round(3)\n\n\nDo the following.\n\nIndicate which company you have chosen, the stock symbol, and the time period.\nCreate a time plot of the daily closing stock prices.\nProduce a time plot of the differences in the daily closing stock prices.\nCreate a correlogram of the stock prices\nCreate a correlogram of the differences\nGenerate a histogram of the difference in the stock prices and superimpose the corresponding normal density.\nCompute the variance of the differences\nCompare your results with those from the other teams of students.",
    "crumbs": [
      "Lesson 2",
      "White Noise and Random Walks - Part 2"
    ]
  },
  {
    "objectID": "chapter_4_lesson_2.html#optional-activity-integrated-autoregressive-model-10-min",
    "href": "chapter_4_lesson_2.html#optional-activity-integrated-autoregressive-model-10-min",
    "title": "White Noise and Random Walks - Part 2",
    "section": "Optional Activity: Integrated Autoregressive Model (10 min)",
    "text": "Optional Activity: Integrated Autoregressive Model (10 min)\nClick here for further information about Section 4.4.2 in the book\n\nThis is a time plot of the quarterly exchange rate (UK Pounds to NZ Dollars, 1991-2000). This is the data set given in the textbook.\n\n\nShow the code\nz &lt;- read_table(\"https://byuistats.github.io/timeseries/data/pounds_nz.dat\")\ndate_seq &lt;- seq(\n    lubridate::ymd(\"1991-01-01\"),\n    by = \"3 months\",\n    length.out = nrow(z))\nz_ts &lt;- z |&gt;\n    mutate(\n        date = date_seq,\n        quarter = tsibble::yearquarter(date)) |&gt;\n    as_tsibble(index = quarter)\nz_ts |&gt;\n  autoplot(.vars = xrate) +\n    labs(\n      title = paste(\"Time Plot of Exchange Rates\"),\n      subtitle = \"(UK Pounds to NZ Dollars, 1991-2000)\",\n      x = \"Quarter\",\n      y = \"Exchange Rate\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5),\n      plot.subtitle = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nConsider the correlogram of the first differences in the exchange rates.\n\n\nShow the code\nz_ts |&gt;\n    mutate(diff = xrate - lag(xrate)) |&gt;\n    ACF(diff) |&gt;\n    autoplot() +\n    labs(\n      title = paste(\"Correlogram of First Differences of Exchange Rates\"),\n      x = \"Lag\",\n      y = \"Difference\"\n    ) +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nNotice that for a lag of \\(k=1\\), there is still a significant autocorrelation in the differences. This suggests that a more sophisticated model might be necessary. Note that for \\(k&gt;1\\), the autocorrelations in the differences are not significant. So, a random walk might be a good model for the differences.\nWe can add an additional terms to the random walk model using the slope estimate from Holt-Winters. Assume the next term in the time series can be modeled as the previous term plus an estimated slope plus a white noise component. This gives us the first equation below. We will use the Holt-Winters update equation to estimate the slope. This is the second equation below.\n\\[\n\\begin{cases}\n~~~x_t     = x_{t-1} + b_{t-1} + w_t \\\\\n   b_{t-1} = \\beta \\left( x_{t-1} - x_{t-2} \\right) + (1-\\beta) b_{t-2}\n\\end{cases}\n\\]\nWe can constrain the Holt-Winters parameters \\(\\alpha=1\\) and \\(\\gamma=0\\), and find the estimated value of \\(\\beta\\).\n\n\nShow the code\nz_model &lt;- z_ts |&gt;\n    model(Additive = ETS(xrate ~\n        trend(\"A\", alpha = 1) +\n        error(\"A\") + season(\"N\", gamma = 0),\n        opt_crit = \"amse\", nmse = 1)) \n\nz_model |&gt;\n  coef()\n\n\n# A tibble: 5 × 3\n  .model   term  estimate\n  &lt;chr&gt;    &lt;chr&gt;    &lt;dbl&gt;\n1 Additive alpha   1     \n2 Additive beta    0.155 \n3 Additive gamma   0     \n4 Additive l[0]    2.91  \n5 Additive b[0]    0.0104\n\n\nNote: The value of beta obtained using the feasts model() statement is 0.155. This is slightly different than the value obtained using the base R HoltWinters() command: 0.167. This is likely due to differences in the implementation of the Holt-Winters model, including the initial conditions.\nThis leads to the system of equations:\n\\[\n\\begin{cases}\n~~~x_t     = x_{t-1} + b_{t-1} + w_t \\\\\n   b_{t-1} = 0.155 \\left( x_{t-1} - x_{t-2} \\right) + 0.845 ~ b_{t-2}\n\\end{cases}\n\\]\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nStart with the fitted system of equations \\[\n\\begin{cases}\n~~~x_t     = x_{t-1} + b_{t-1} + w_t \\\\\n   b_{t-1} = 0.155 \\left( x_{t-1} - x_{t-2} \\right) + 0.845 ~ b_{t-2}\n\\end{cases}\n\\] Show that it can be written as \\[\n  \\left( 1 - \\mathbf{B})^2 x_t = (1 - 0.845 \\mathbf{B} \\right) w_t\n\\] by completing the following steps.\n\nWrite the system of equations in terms of the backward shift operator.\nSolve for the term \\(\\mathbf{B} b_t\\) in the first equation and substitute the resulting expression into the second equation.\nCombine like terms and simplify.\n\n\n\nWe will examine the residuals from the Holt-Winters model:\n\n\nShow the code\nz_hw &lt;- z_model |&gt;\n        residuals()\n\nACF(z_hw, .resid) |&gt;\n    autoplot() +\n    labs(\n      title = paste(\"Correlogram of Residuals from Holt-Winters Filtering\"),\n      x = \"Date\",\n      y = \"ACF\"\n    ) +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nNotice that there are no significant autocorrelations, so we conclude that the Holt-Winters model yields reasonable estimates of the time series.",
    "crumbs": [
      "Lesson 2",
      "White Noise and Random Walks - Part 2"
    ]
  },
  {
    "objectID": "chapter_4_lesson_2.html#class-activity-random-walk-with-drift-15-min",
    "href": "chapter_4_lesson_2.html#class-activity-random-walk-with-drift-15-min",
    "title": "White Noise and Random Walks - Part 2",
    "section": "Class Activity: Random Walk with Drift (15 min)",
    "text": "Class Activity: Random Walk with Drift (15 min)\n\n\n\n\n\n\nWe will now consider the daily closing price of Abercrombie & Fitch stock (Symbol = ANF). Here is a time series plot of the closing stock prices.\n\n\nShow the code\n# Set symbol and date range\nsymbol &lt;- \"ANF\"                # Abercrombie & Fitch stock trading symbol\ndate_start &lt;- \"2023-05-01\"\ndate_end &lt;- \"2024-02-20\"\n\n# Fetch stock prices\ndf_stock &lt;- tq_get(symbol, from = date_start, to = date_end, get = \"stock.prices\")\n\n# Transform data into tsibble\ndf_tsibble &lt;- df_stock |&gt;\n  mutate(\n    dates = date, \n    value = close\n  ) |&gt;\n  dplyr::select(dates, value) |&gt;\n  as_tibble() |&gt; \n  arrange(dates) |&gt;\n  as_tsibble(index = dates, key = NULL)\n\n# Generate time series plot using plot_ly\nplot_ly(df_tsibble, x = ~dates, y = ~value, type = 'scatter', mode = 'lines') |&gt;\n  layout(\n    xaxis = list(title = \"Date\"),\n    yaxis = list(title = \"Value\"),\n    title = paste0(\"Time Plot of \", symbol, \" Daily Closing Price (\", format(ymd(date_start), \"%d %b %Y\"), \" - \", format(ymd(date_end), \"%d %b %Y\"),\")\")\n  )\n\n\n\n\n\n\n\n\nFigure 6: Time plot of the daily prices of ANF stock\n\n\n\n\nWe now generate a time plot and a correlogram of the differences. (No stock prices are recorded on weekends or holidays. Due to the gaps in the data, we will use the base R acf command, rather than the feasts ACF command.)\n\n\nShow the code\ndf_tsibble$diff = df_tsibble$value - lag(df_tsibble$value) \ndf_tsibble |&gt;\n  na.omit() |&gt;\n  autoplot(.vars = diff) + \n    labs(\n      title = paste(\"Time Plot of Differences in Daily\", symbol, \"Stock Prices\"),\n      subtitle = \n        paste0(\n          format(ymd(date_start), \"%d %b %Y\"),\n            \" - \",\n          format(ymd(date_end), \"%d %b %Y\")\n          ),\n      x = \"Date\",\n      y = \"Difference\",\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5),\n      plot.subtitle = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\nFigure 7: Time plot of differences in the daily prices of ANF stock\n\n\n\n\n\n\n\nShow the code\nacf(df_tsibble$diff |&gt; na.omit(), main = paste(\"ACF of First Difference of\", symbol, \"Stock Prices\"))\n\n\n\n\n\n\n\n\nFigure 8: Correlogram of first differences for the daily prices of ANF stock\n\n\n\n\n\nThere is no significant autocorrelation in the differences. They appear to be modeled reasonably well by white noise.\nWe now compute the mean and standard deviation of the differences.\n\nmean_diff &lt;- df_tsibble$diff |&gt; mean(na.rm = TRUE)\nsd_diff &lt;- df_tsibble$diff |&gt; sd(na.rm = TRUE)\nn_diff &lt;- df_tsibble$diff |&gt; na.omit() |&gt; length()\n\nThe mean of the differences is 0.486. The standard deviation of the differences is 1.732. There are 201 differences.\nWe can use the t-distribution to create a 95% confidence interval for the drift parameter. The critical \\(t\\) value is given by qt(0.975, df = 201 - 1), yielding a value of \\(t^*_{0.975} = 1.972\\).\nSo, our 95% confidence interval is computed as: \\[\n  \\left(\n    \\bar x - t^*_{0.975} \\cdot \\frac{s}{\\sqrt{n}}\n    , ~\n    \\bar x + t^*_{0.975} \\cdot \\frac{s}{\\sqrt{n}}\n  \\right)\n\\] \\[\n  \\left(\n    0.486 - 1.972 \\cdot  \\frac{1.732}{\\sqrt{201}}\n    , ~\n    0.486 + 1.972 \\cdot  \\frac{1.732}{\\sqrt{201}}\n  \\right)\n\\] \\[\n  (0.245\n  , ~\n  0.726)\n\\]\nThis confidence interval does not contain 0, so we conclude that there is evidence of a positive drift in the price of Abercrombie & Fitch stock over this period.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUse the McDonald’s stock price data to do the following.\n\nWhat is the estimate of the drift parameter?\nWhat is the standard deviation of the differences?\nWhat is the 95% confidence interval for the drift parameter?\nIs there evidence to suggest that the time series can be modeled as a random walk with drift? Why or why not?",
    "crumbs": [
      "Lesson 2",
      "White Noise and Random Walks - Part 2"
    ]
  },
  {
    "objectID": "chapter_4_lesson_2.html#homework-preview-5-min",
    "href": "chapter_4_lesson_2.html#homework-preview-5-min",
    "title": "White Noise and Random Walks - Part 2",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_4_2.qmd \n\n\n Tables-Handout-Excel-key \nDifference Operator\n\n\n\\(x_{1} = 5\\),\\(~\\) \\(x_{2} = 10\\),\\(~\\) \\(x_{3} = 13\\),\\(~\\) \\(x_{4} = 8\\),\\(~\\) \\(x_{5} = 4\\),\\(~\\) \\(x_{6} = 3\\),\\(~\\) \\(x_{7} = 9\\),\\(~\\) \\(x_{8} = 2\\)\n\nCheck Your Understanding Solutions:\n\n\n\n\nThe values of the first differences are: \\[\n\\nabla x_t =\n\\{\n5,\n~~\n3,\n~~\n-5,\n~~\n-4,\n~~\n-1,\n~~\n6,\n~~\n-7\\}\\]\nThe values of the second differences are: \\[\n\\nabla^2 x_t =\n\\{\n-2,\n~~\n-8,\n~~\n1,\n~~\n3,\n~~\n7,\n~~\n-13\\}\\]\nBy substitution, we get:\n\n\\[\\begin{align*}\n(1-\\mathbf{B} )^2 x_8\n&= (1-\\mathbf{B} ) \\left[ (1-\\mathbf{B} ) x_8 \\right] \\\\\n&= (1-\\mathbf{B} ) ( x_8 - x_7 ) \\\\\n&= ( x_8 - x_7 ) - \\mathbf{B} ( x_8 - x_7 ) \\\\\n&= ( x_8 - x_7 ) - ( x_7 - x_6 ) \\\\\n&= ( \\nabla x_8 ) - ( \\nabla x_7 ) \\\\\n&= \\nabla ( \\nabla x_8 ) \\\\\n&= \\nabla^2 x_8\n\\end{align*}\\]\n\\[\n  \\nabla^2 x_8\n    = \\nabla \\left( \\nabla x_8 \\right)\n    = ( x_8 - x_7 ) - ( x_7 - x_6 )\n    = (2 - 9) - (9 - 3) \\\\\n    = -13\n\\] This is the value of the last term in the sequence of second differences.\n\n\n\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$\\nabla x_t$$\n$$\\nabla^2 x_t$$\n\n\n\n\n1\n5\n\n\n\n\n2\n10\n5\n\n\n\n3\n13\n3\n-2\n\n\n4\n8\n-5\n-8\n\n\n5\n4\n-4\n1\n\n\n6\n3\n-1\n3\n\n\n7\n9\n6\n7\n\n\n8\n2\n-7\n-13\n\n\n\n\n\n\n\n\n\n\n\n\n\nComputing Differences\n\n\nLinear\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$\\nabla x_t$$\n\n\n\n\n\n\n1\n7.5\n\n\n\n\n\n2\n10\n2.5\n\n\n\n\n3\n12.5\n2.5\n\n\n\n\n4\n15\n2.5\n\n\n\n\n5\n17.5\n2.5\n\n\n\n\n6\n20\n2.5\n\n\n\n\n7\n22.5\n2.5\n\n\n\n\n8\n25\n2.5\n\n\n\n\n9\n27.5\n2.5\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuadratic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$\\nabla x_t$$\n$$\\nabla^2 x_t$$\n\n\n\n\n\n1\n15\n\n\n\n\n\n2\n2\n-13\n\n\n\n\n3\n-7\n-9\n4\n\n\n\n4\n-12\n-5\n4\n\n\n\n5\n-13\n-1\n4\n\n\n\n6\n-10\n3\n4\n\n\n\n7\n-3\n7\n4\n\n\n\n8\n8\n11\n4\n\n\n\n9\n23\n15\n4\n\n\n\n\n\n\n\n\n\n\n\n\n\nCubic\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n$$t$$\n$$x_t$$\n$$\\nabla x_t$$\n$$\\nabla^2 x_t$$\n$$\\nabla^3 x_t$$\n\n\n\n\n1\n-2.7\n\n\n\n\n\n2\n0\n2.7\n\n\n\n\n3\n1\n1\n-1.7\n\n\n\n4\n0.9\n-0.1\n-1.1\n0.6\n\n\n5\n0.3\n-0.6\n-0.5\n0.6\n\n\n6\n-0.2\n-0.5\n0.1\n0.6\n\n\n7\n0\n0.2\n0.7\n0.6\n\n\n8\n1.5\n1.5\n1.3\n0.6\n\n\n9\n4.9\n3.4\n1.9\n0.6\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIntegrated Autoregressive Model\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nStart with this fitted system of equations. Show that it can be written as \\[\n  (1 - \\mathbf{B})^2 x_t = (1 - 0.845) w_t\n\\] by completing the following steps.\n\nWrite the system of equations in terms of the backward shift operator. \\[\n\\begin{cases}\n~~~x_t     = x_{t-1} + b_{t-1} + w_t \\\\\n   b_{t-1} = 0.155 \\left( x_{t-1} - x_{t-2} \\right) + 0.845 ~ b_{t-2}\n\\end{cases}\n\\] \\[\n\\begin{cases}\n~~~x_t     = \\mathbf{B} x_{t} + \\mathbf{B} b_{t} + w_t \\\\\n   \\mathbf{B} b_{t} = 0.155 \\left( \\mathbf{B} x_{t} - \\mathbf{B}^2 x_{t} \\right) + 0.845 ~ \\mathbf{B}^2 b_{t}\n\\end{cases}\n\\]\nSolve for the term \\(\\mathbf{B}b_t\\) in the first equation and substitute the resulting expression into the second equation.\n\nSolving for \\(\\mathbf{B} b_{t}\\): \\[\n\\mathbf{B} b_{t} = x_t -\\mathbf{B} x_{t} - w_t = (1-\\mathbf{B}) x_t - w_t\n\\] First, we simplify the second equation: \\[\\begin{align*}\n  \\mathbf{B} b_{t}\n    &= 0.155 \\left( \\mathbf{B} x_{t} - \\mathbf{B}^2 x_{t} \\right) + 0.845 ~ \\mathbf{B}^2 b_{t}  \\\\\n    &= 0.155 \\left(  1 - \\mathbf{B} \\right) \\mathbf{B} x_{t}   + 0.845 ~ \\mathbf{B} \\left[ \\mathbf{B} b_{t} \\right]\n\\end{align*}\\] Substitutingthe expression for \\(\\mathbf{B} b_{t}\\) into the second equation: \\[\\begin{align*}\n  (1-\\mathbf{B}) x_t - w_t\n    &= 0.155 \\left(  1 - \\mathbf{B} \\right) \\mathbf{B} x_{t}   + 0.845 ~ \\mathbf{B} \\left[ (1-\\mathbf{B}) x_t - w_t \\right]  \\\\\n    &= 0.155 \\left(  1 - \\mathbf{B} \\right) \\mathbf{B} x_{t}   + 0.845 ~ \\mathbf{B} (1-\\mathbf{B}) x_t - 0.845 ~ \\mathbf{B} w_t \\\\\n    &= \\left(  1 - \\mathbf{B} \\right) \\mathbf{B} x_{t} - 0.845 ~ \\mathbf{B} w_t   \\\\\n\\end{align*}\\]\n\nCombine like terms and simplify.\n\nCombining like terms: \\[\\begin{align*}\n  (1-\\mathbf{B}) x_t - w_t\n    &= \\left(  1 - \\mathbf{B} \\right) \\mathbf{B} x_{t} - 0.845 ~ \\mathbf{B} w_t   \\\\\n  (1-\\mathbf{B}) x_t - \\left(  1 - \\mathbf{B} \\right) \\mathbf{B} x_{t}\n    &=  w_t - 0.845 ~ \\mathbf{B} w_t   \\\\\n  \\left( 1-\\mathbf{B} \\right) \\left(  1 - \\mathbf{B} \\right) x_{t}\n    &=  \\left( 1 - 0.845 ~ \\mathbf{B} \\right) w_t   \\\\\n  \\left( 1-\\mathbf{B} \\right)^2 x_{t}\n    &=  \\left( 1 - 0.845 ~ \\mathbf{B} \\right) w_t   \\\\\n\\end{align*}\\]\nThis is a special case of the ARIMA models in Chapter 7.\n\n\n\n\n\nMcDonald’s Drift Parameter\n\nSolutions to Class Activity\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUse the McDonald’s stock price data to do the following.\n\nWhat is the estimate of the drift parameter?\n\nAs seen above, there is no significant autocorrelation in the differences of the McDonald’s stock prices. They appear to be modeled well by white noise.\nWe now compute the mean and standard deviation of the differences.\n\nmean(stock_ts$diff, na.rm = TRUE)\n\n[1] 0.1396449\n\n\n\nWhat is the standard deviation of the differences?\n\n\nsd(stock_ts$diff, na.rm = TRUE)\n\n[1] 2.404863\n\n\n\nWhat is the 95% confidence interval for the drift parameter?\n\nThe number of differences, \\(n\\), is given by:\n\nlength(stock_ts$diff |&gt; na.omit())\n\n[1] 880\n\n\nThe mean of the differences is 0.14. The standard deviation of the differences is 2.405. There are 880 differences.\nWe can use the t-distribution to create a 95% confidence interval for the drift parameter. The critical \\(t\\) value is given by qt(0.975, df = 880 - 1), yielding a value of \\(t^*_{0.975} = 1.963\\).\nSo, our 95% confidence interval is computed as: \\[\n  \\left(\n    \\bar x - t^*_{0.975} \\cdot \\frac{s}{\\sqrt{n}}\n    , ~\n    \\bar x + t^*_{0.975} \\cdot \\frac{s}{\\sqrt{n}}\n  \\right)\n\\] \\[\n  \\left(\n    0.14 - 1.963 \\cdot  \\frac{2.405}{\\sqrt{880}}\n    , ~\n    0.14 + 1.963 \\cdot  \\frac{2.405}{\\sqrt{880}}\n  \\right)\n\\] \\[\n  (-0.019\n  , ~\n  0.299)\n\\]\n\nIs there evidence to suggest that the time series can be modeled as a random walk with drift? Why or why not?\n\nIf this confidence contains 0, we cannot conclude that there is evidence of a positive drift in the price of McDonald’s stock over the period.",
    "crumbs": [
      "Lesson 2",
      "White Noise and Random Walks - Part 2"
    ]
  },
  {
    "objectID": "chapter_4_lesson_3.html",
    "href": "chapter_4_lesson_3.html",
    "title": "Autoregressive (AR) Models",
    "section": "",
    "text": "Characterize the properties of an \\(AR(p)\\) stochastic process\n\n\nDefine an \\(AR(p)\\) stochastic process\nExpress an \\(AR(p)\\) process using the backward shift operator\nState an \\(AR(p)\\) forecast (or prediction) function\nIdentify stationarity of an \\(AR(p)\\) process using the backward shift operator\nDetermine the stationarity of an \\(AR(p)\\) process using a characteristic equation\n\n\n\n\nCheck model adequacy using diagnostic plots like correlograms of residuals\n\n\nCharacterize a random walk’s second order characteristics using a correlogram\nDefine partial autocorrelations\nExplain how to use a partial correlogram to decide what model would be suitable to estimate an \\(AR(p)\\) process\nDemonstrate the use of partial correlogram via simulation",
    "crumbs": [
      "Lesson 3",
      "Autoregressive (AR) Models"
    ]
  },
  {
    "objectID": "chapter_4_lesson_3.html#learning-outcomes",
    "href": "chapter_4_lesson_3.html#learning-outcomes",
    "title": "Autoregressive (AR) Models",
    "section": "",
    "text": "Characterize the properties of an \\(AR(p)\\) stochastic process\n\n\nDefine an \\(AR(p)\\) stochastic process\nExpress an \\(AR(p)\\) process using the backward shift operator\nState an \\(AR(p)\\) forecast (or prediction) function\nIdentify stationarity of an \\(AR(p)\\) process using the backward shift operator\nDetermine the stationarity of an \\(AR(p)\\) process using a characteristic equation\n\n\n\n\nCheck model adequacy using diagnostic plots like correlograms of residuals\n\n\nCharacterize a random walk’s second order characteristics using a correlogram\nDefine partial autocorrelations\nExplain how to use a partial correlogram to decide what model would be suitable to estimate an \\(AR(p)\\) process\nDemonstrate the use of partial correlogram via simulation",
    "crumbs": [
      "Lesson 3",
      "Autoregressive (AR) Models"
    ]
  },
  {
    "objectID": "chapter_4_lesson_3.html#preparation",
    "href": "chapter_4_lesson_3.html#preparation",
    "title": "Autoregressive (AR) Models",
    "section": "Preparation",
    "text": "Preparation\n\nRead Section 4.5",
    "crumbs": [
      "Lesson 3",
      "Autoregressive (AR) Models"
    ]
  },
  {
    "objectID": "chapter_4_lesson_3.html#learning-journal-exchange-10-min",
    "href": "chapter_4_lesson_3.html#learning-journal-exchange-10-min",
    "title": "Autoregressive (AR) Models",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading another student’s?\nWhat would you recommend the other student add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 3",
      "Autoregressive (AR) Models"
    ]
  },
  {
    "objectID": "chapter_4_lesson_3.html#class-activity-definition-of-autoregressive-ar-models-10-min",
    "href": "chapter_4_lesson_3.html#class-activity-definition-of-autoregressive-ar-models-10-min",
    "title": "Autoregressive (AR) Models",
    "section": "Class Activity: Definition of Autoregressive (AR) Models (10 min)",
    "text": "Class Activity: Definition of Autoregressive (AR) Models (10 min)\nWe now define an autoregressive (or AR) model.\n\n\n\n\n\n\nDefinition of an Autoregressive (AR) Model\n\n\n\nThe time series \\(\\{x_t\\}\\) is an autoregressive process of order \\(p\\), denoted as \\(AR(p)\\), if \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t ~~~~~~~~~~~~~~~~~~~~~~~ (4.15)\n\\]\nwhere \\(\\{w_t\\}\\) is white noise and the \\(\\alpha_i\\) are the model parameters with \\(\\alpha_p \\ne 0\\).\n\n\nIn short, this means that the next observation of a time series depends linearly on the previous \\(p\\) terms and a random white noise component.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nShow that we can write Equation (4.15) as a polynomial of order \\(p\\) in terms of the backward shift operator: \\[\n   \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) x_t = w_t\n\\]\n\n\n\nWe have seen some special cases of this model already.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nGive another name for an \\(AR(0)\\) model.\nShow that the random walk is the special case \\(AR(1)\\) with \\(\\alpha_1 = 1\\). (See Chapter 4, Lesson 1.)\nShow that the exponential smoothing model is the special case where \\[\\alpha_i = \\alpha(1-\\alpha)^i\\] for \\(i = 1, 2, \\ldots\\) and \\(p \\rightarrow \\infty\\). (See Chapter 3, Lesson 2.)\n\n\n\nWe now explore the autoregressive properties of this model.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nShow that the \\(AR(p)\\) model is a regression of \\(x_t\\) on past terms from the same series. Hint: write the \\(AR(p)\\) model in more familiar terms, letting \\[y_i = x_t, ~~ x_1 = x_{t-1}, ~~ x_2 = x_{t-2}, ~~ \\ldots, ~~ x_p = x_{t-p}, ~~ \\text{and} ~~ \\epsilon_i = w_t\\]\nExplain why the prediction at time \\(t\\) is given by \\[\n\\hat x_t = \\hat \\alpha_1 x_{t-1} + \\hat \\alpha_2 x_{t-2} + \\cdots + \\hat \\alpha_{p-1} x_{t-(p-1)} + \\hat \\alpha_p x_{t-p}\n\\]\nExplain why the model parameters (the \\(\\alpha\\)’s) can be estimated by minimizing the sum of the squared error terms: \\[\\sum_{t=1}^n \\left( \\hat w_t \\right)^2 = \\sum_{t=1}^n \\left( x_t - \\hat x_t \\right)^2\\]\nWhat is the reason this is called an autoregressive model?",
    "crumbs": [
      "Lesson 3",
      "Autoregressive (AR) Models"
    ]
  },
  {
    "objectID": "chapter_4_lesson_3.html#class-activity-exploring-ar1-models-10-min",
    "href": "chapter_4_lesson_3.html#class-activity-exploring-ar1-models-10-min",
    "title": "Autoregressive (AR) Models",
    "section": "Class Activity: Exploring \\(AR(1)\\) Models (10 min)",
    "text": "Class Activity: Exploring \\(AR(1)\\) Models (10 min)\n\nDefinition\nRecall that an \\(AR(p)\\) model is of the form \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t\n\\] So, an \\(AR(1)\\) model is expressed as \\[\n  x_t = \\alpha x_{t-1} + w_t\n\\] where \\(\\{w_t\\}\\) is a white noise series with mean zero and variance \\(\\sigma^2\\).\n\n\nSecond-Order Properties of an \\(AR(1)\\) Model\nWe now explore the second-order properties of this model.\n\n\n\n\n\n\nSecond-Order Properties of an \\(AR(1)\\) Model\n\n\n\nIf \\(\\{x_t\\}_{t=1}^n\\) is an \\(AR(1)\\) prcess, then its the first- and second-order properties are summarized below.\n\\[\n\\begin{align*}\n  \\mu_x &= 0 \\\\  \n  \\gamma_k = cov(x_t, x_{t+k}) &= \\frac{\\alpha^k \\sigma^2}{1-\\alpha^2}\n\\end{align*}\n\\]\n\n\n\n\n\n\nClick here for a proof of the equation for \\(cov(x_t,x_{t+k})\\)\n\n\n\n\n\nWhy is \\(cov(x_t, x_{t+k}) = \\dfrac{\\alpha^k \\sigma^2}{1-\\alpha^2}\\)?\nIf \\(\\{x_t\\}\\) is a stable \\(AR(1)\\) process (which means that $||&lt;1) can be written as:\n\\[\\begin{align*}\n  (1-\\alpha \\mathbf{B}) x_t &= w_t \\\\\n  \\implies x_t &= (1-\\alpha \\mathbf{B})^{-1} w_t \\\\\n    &= w_t + \\alpha w_{t-1} + \\alpha^2 w_{t-2} + \\alpha^3 w_{t-3} + \\cdots \\\\\n    &= \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i}\n\\end{align*}\\]\nFrom this, we can deduce that the mean is\n\\[\n  E(x_t)\n    = E\\left( \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i} \\right)\n    = \\sum\\limits_{i=0}^\\infty \\alpha^i E\\left( w_{t-i} \\right)\n    = 0\n\\]\nThe autocovariance is computed similarly as:\n\\[\\begin{align*}\n  \\gamma_k = cov(x_t, x_{t+k})\n    &= cov \\left(\n      \\sum\\limits_{i=0}^\\infty \\alpha^i w_{t-i}, \\\\\n      \\sum\\limits_{j=0}^\\infty \\alpha^j w_{t+k-j} \\right) \\\\\n    &= \\sum\\limits_{j=k+i} \\alpha^i \\alpha^j cov ( w_{t-i}, w_{t+k-j} ) \\\\\n    &= \\alpha^k \\sigma^2 \\sum\\limits_{i=0}^\\infty \\alpha^{2i} \\\\\n    &= \\frac{\\alpha^k \\sigma^2}{1-\\alpha^2}\n\\end{align*}\\]\nSee Equations (2.15) and (4.2).\n\n\n\n\n\n\n\nCorrelogram of an \\(AR(1)\\) process\n\n\n\n\n\n\nCorrelogram of an AR(1) Process\n\n\n\nThe autocorrelation function for an AR(1) process is\n\\[\n  \\rho_k = \\alpha^k ~~~~~~ (k \\ge 0)\n\\] where \\(|\\alpha| &lt; 1\\).\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nUse the equation for the autocovariance function, \\(\\gamma_k\\), to show that \\[\n  \\rho_k = \\alpha^k\n\\] for \\(k \\ge 0\\) when \\(|\\alpha|&lt;1\\).\nUse this to explain why the correlogram decays to zero more quickly when \\(\\alpha\\) is small.\n\n\n\n\n\nSmall Group Activity: Simulation of an \\(AR(1)\\) Process\n \n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nIn each of the following cases, what do you observe in the correlogram? (If you expect to see significant results and you do not, try increasing the number of points.)\n\n\\(\\alpha = 1\\)\n\\(\\alpha = 0.5\\)\n\\(\\alpha = 0.1\\)\n\\(\\alpha = 0\\)\n\n\\(\\alpha = -0.1\\)\n\\(\\alpha = -0.5\\)\n\\(\\alpha = -1\\)",
    "crumbs": [
      "Lesson 3",
      "Autoregressive (AR) Models"
    ]
  },
  {
    "objectID": "chapter_4_lesson_3.html#class-activity-partial-autocorrelation-10-min",
    "href": "chapter_4_lesson_3.html#class-activity-partial-autocorrelation-10-min",
    "title": "Autoregressive (AR) Models",
    "section": "Class Activity: Partial Autocorrelation (10 min)",
    "text": "Class Activity: Partial Autocorrelation (10 min)\n\nDefinition of Partial Autocorrelation\n\n\n\n\n\n\nPartial Autocorrleation\n\n\n\nThe partial autocorrelation at lag \\(k\\) is defined as the portion of the correlation that is not explained by shorter lags.\n\n\nFor example, the partial correlation for lag 4 is the correlation not explained by lags 1, 2, or 3.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat is the value of the partial autocorrelation function for an \\(AR(2)\\) process for all lags greater than 2? \n\n\n\nOn page 81, the textbook states that in general, the partial autocorrelation at lag \\(k\\) is the \\(k^{th}\\) coefficient of a fitted \\(AR(k)\\) model. This implies that if the underlying process is \\(AR(p)\\), then all the coefficients \\(\\alpha_k=0\\) if \\(k&gt;p\\). So, an \\(AR(p)\\) process will yield partial correlations that are zero after lag \\(p\\). So, a correlogram of partial autocorrelations can be helpful to determine the order of an appropriate \\(AR\\) process to model a time series.\n\n\n\n\n\n\nACF and PACF of an \\(AR(p)\\) Process\n\n\n\nFor an \\(AR(p)\\) process, we observe the following:\n\n\n\n\n\nAR(p)\n\n\n\n\nACF\nTails off\n\n\nPACF\nCuts off after lag \\(p\\)\n\n\n\n\n\n\n\n\n\n\n\n\n\nExample: McDonald’s Stock Price\nHere is a partial autocorrelation plot for the McDonald’s stock price data:\n\n\nShow the code\n# Set symbol and date range\nsymbol &lt;- \"MCD\"\ncompany &lt;- \"McDonald's\"\n\n# Retrieve static file\nstock_df &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/stock_price_mcd.parquet\")\n\n# Transform data into tibble\nstock_ts &lt;- stock_df %&gt;%\n  mutate(\n    dates = date, \n    value = adjusted\n  ) %&gt;%\n  select(dates, value) %&gt;%\n  as_tibble() %&gt;% \n  arrange(dates) |&gt;\n  mutate(diff = value - lag(value)) |&gt;\n  as_tsibble(index = dates, key = NULL) \n\npacf(stock_ts$value, plot=TRUE, lag.max = 25)\n\n\n\n\n\n\n\n\n\nThe only significant partial correlation is at lag \\(k=1\\). This suggests that an \\(AR(1)\\) process could be used to model the McDonald’s stock prices.\n\n\nPartial Autocorrelation Plots of Various \\(AR(p)\\) Processes\nHere are some time plots, correlograms, and partial correlograms for \\(AR(p)\\) processes with various values of \\(p\\).\n\n\nShiny App",
    "crumbs": [
      "Lesson 3",
      "Autoregressive (AR) Models"
    ]
  },
  {
    "objectID": "chapter_4_lesson_3.html#class-activity-stationary-and-non-stationary-ar-processes-15-min",
    "href": "chapter_4_lesson_3.html#class-activity-stationary-and-non-stationary-ar-processes-15-min",
    "title": "Autoregressive (AR) Models",
    "section": "Class Activity: Stationary and Non-Stationary AR Processes (15 min)",
    "text": "Class Activity: Stationary and Non-Stationary AR Processes (15 min)\n\n\n\n\n\n\nDefinition of the Characteristic Equation\n\n\n\nTreating the symbol \\(\\mathbf{B}\\) formally as a number (either real or complex), the polynomial\n\\[\n  \\theta_p(\\mathbf{B}) x_t = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) x_t\n\\]\nis called the characteristic polynomial of an AR process.\nIf we set the characteristic polynomial to zero, we get the characteristic equation:\n\\[\n  \\theta_p(\\mathbf{B}) = \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) = 0\n\\]\n\n\nThe roots of the characteristic polynomial are the values of \\(\\mathbf{B}\\) that make the polynomial equal to zero–i.e., the values of \\(\\mathbf{B}\\) that make \\(\\theta_p(\\mathbf{B}) = 0\\). These are also called the solutions of the characteristic equation. The roots of the characteristic polynomial can be real or complex numbers.\nWe now explore an important result for AR processes that uses the absolute value of complex numbers.\n\n\n\n\n\n\nIdentifying Stationary Processes\n\n\n\nAn AR process will be stationary if the absolute value of the solutions of the characteristic equation are all strictly greater than 1.\n\n\nFirst, we will find the roots of the characteristic polynomial (i.e. the solutions of the characteristic equation) and then we will determine if the absolute value of these solutions is greater than 1.\nWe can use the polyroot function to find the roots of polynomials in R. For example, to find the roots of the polynomial \\(x^2-x-6\\), we apply the command\n\npolyroot(c(-6,-1,1))\n\n[1]  3+0i -2+0i\n\n\nNote the order of the coefficients. They are given in increasing order of the power of \\(x\\).\nOf course, we could simply factor the polynomial: \\[\n  x^2-x-6 = (x-3)(x+2) \\overset{set}{=} 0\n\\] which implies that \\[\n  x = 3 ~~~ \\text{or} ~~~ x = -2\n\\]\n\n\n\n\n\n\nDefinition of the Absolute Value in the Complex Plane\n\n\n\nLet \\(z = a+bi\\) be any complex number. It can be represented by the point \\((a,b)\\) in the complex plane. We define the absolute value of \\(z\\) as the distance from the origin to the point:\n\\[\n  |z| = \\sqrt{a^2 + b^2}\n\\]\n\n\nPractice computing the absolute value of a complex number. \\(\\ \\)\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nFind the absolute value of the following (complex) numbers:\n\n\\(-3\\)\n\\(4i\\)\n\\(-3+4i\\)\n\\(-\\dfrac{\\sqrt{3}}{4} + \\dfrac{1}{4} i\\)\n\\(\\dfrac{1}{\\sqrt{2}} - \\dfrac{1}{\\sqrt{2}} i\\)\n\\(5-12i\\)\n\n\n\nWe will now practice assessing whether an AR process is stationary using the characteristic equation.\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nFor each of the following AR processes, do the following:\n\nWrite the AR process in terms of the backward shift operator.\nSolve the characteristic equation.\nDetermine if the AR process is stationary.\n\n\n\\(AR(1)\\) process: \\[\nx_t = x_{t-1} + w_t\n\\]  \n\\(AR(1)\\) process: \\[\nx_t = \\frac{1}{3} x_{t-1} + w_t\n\\]  \n\\(AR(2)\\) process: \\[\nx_t = - \\frac{1}{4} x_{t-1} + \\frac{1}{8} x_{t-2} + w_t\n\\]   \n\\(AR(2)\\) process: \\[\nx_t = - \\frac{2}{3} x_{t-1} + \\frac{1}{3} x_{t-2} + w_t\n\\]   \n\\(AR(2)\\) process: \\[\nx_t = -x_{t-1} - 2 x_{t-2} + w_t\n\\]   \n\\(AR(2)\\) process: \\[\nx_t = \\frac{3}{2} x_{t-1} - x_{t-2} + w_t\n\\]    \n\\(AR(2)\\) process: \\[\nx_t = 4 x_{t-2} + w_t\n\\]    \n\\(AR(3)\\) process: \\[\nx_t = \\frac{2}{3} x_{t-1} + \\frac{1}{4} x_{t-2} - \\frac{1}{6} x_{t-3} + w_t\n\\]  \n\n\nChoose one stationary \\(AR(2)\\) process and one non-stationary \\(AR(2)\\) process. For each, do the following:\n\n\nSimulate at least 1000 sequential observations.\nMake a time plot of the simulated values.\nMake a correlogram of the simulated values.\nPlot the partial correlogram of the simulated values.\n\nThe following code chunk may be helpful--or you can use the simulation above.\n\n\nShow the code\n# Number of observations\nn_obs &lt;- 1000\n\n# Generate sequence of dates\nstart_date &lt;- my(paste(1, floor(year(now())-n_obs/365)))\ndate_seq &lt;- seq(start_date,\n    start_date + days(n_obs - 1),\n    by = \"1 days\")\n\n# Simulate random component\nw &lt;- rnorm(n_obs)\n\n# Set first few values of x\nx &lt;- rep(0, n_obs)\nx[1] &lt;- w[1]\nx[2] &lt;- 0.6 * x[1] + w[2]\n\n# Set all remaining values of x\nfor (t in 3:n_obs) {\n  x[t] &lt;- 0.6 * x[t-1] + 0.25 * x[t-2] + w[t]\n}\n\n# Create the tsibble\nsim_ts &lt;- data.frame(dates = date_seq, x = x) |&gt;\n  as_tsibble(index = dates) \n\n# Generate the plots\nsim_ts |&gt; autoplot(.vars = x)\nacf(sim_ts$x, plot=TRUE, lag.max = 25)\npacf(sim_ts$x, plot=TRUE, lag.max = 25)\n\n\n\nWhat do you observe about the difference in the behavior of the stationary and non-stationary processes?",
    "crumbs": [
      "Lesson 3",
      "Autoregressive (AR) Models"
    ]
  },
  {
    "objectID": "chapter_4_lesson_3.html#homework-preview-5-min",
    "href": "chapter_4_lesson_3.html#homework-preview-5-min",
    "title": "Autoregressive (AR) Models",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_4_3.qmd \n\n\nAutoregressive Model Definition\n\n\n\n\n\n\n\n\nCheck Your Understanding Solutions\n\n\n\n\nShow that we can write Equation (4.15) as a polynomial of order \\(p\\) in terms of the backward shift operator: \\[\n   \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) x_t = w_t\n\\]\n\nSolution:\n\\[\n    x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t\n\\] After subtracting, we have: \\[\n  \\begin{align*}\n    w_t\n      &= x_t - \\alpha_1 x_{t-1} - \\alpha_2 x_{t-2} - \\alpha_3 x_{t-3} - \\cdots - \\alpha_{p-1} x_{t-(p-1)} - \\alpha_p x_{t-p} \\\\\n      &= x_t - \\alpha_1 \\mathbf{B} x_{t} - \\alpha_2 \\mathbf{B}^2 x_{t} - \\alpha_3 \\mathbf{B}^3 x_{t} - \\cdots - \\alpha_{p-1} \\mathbf{B}^{p-1} x_{t} - \\alpha_p \\mathbf{B}^p x_{t} \\\\\n      &= \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2  - \\alpha_3 \\mathbf{B}^3 - \\cdots - \\alpha_{p-1} \\mathbf{B}^{p-1} - \\alpha_p \\mathbf{B}^p \\right) x_t\n  \\end{align*}\n\\]\n\n\n\nGive another name for an \\(AR(0)\\) model.\n\nSolution:\nWhite noise\n\n\n\nShow that the random walk is the special case \\(AR(1)\\) with \\(\\alpha_1 = 1\\). (See Chapter 4, Lesson 1.)\n\nSolution:\nIf we let \\(\\alpha_1=1\\) in an \\(AR(1)\\) model, we get: \\[\n  \\begin{align*}\n  x_t &= \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t \\\\\n    &= \\alpha_1 x_{t-1} + w_t \\\\\n    &= x_{t-1} + w_t \\\\\n  \\end{align*}\n\\] which is the definition of a random walk, as given in Chapter 4, Lesson 1.\n\n\n\nShow that the exponential smoothing model is the special case where \\[\\alpha_i = \\alpha(1-\\alpha)^i\\] for \\(i = 1, 2, \\ldots\\) and \\(p \\rightarrow \\infty\\). (See Chapter 3, Lesson 2.)\n\nSolution: \\[\\begin{align*}\n    x_t &= \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t \\\\\n      &= \\alpha(1-\\alpha)^1 x_{t-1} + \\alpha(1-\\alpha)^2 x_{t-2} + \\alpha(1-\\alpha)^3 x_{t-3} + \\cdots + \\alpha(1-\\alpha)^{p-1} x_{t-(p-1)} + \\alpha(1-\\alpha)^p x_{t-p} + w_t \\\\\n\\end{align*}\\]\nThis is Equation (3.18) in Chapter 3, Lesson 2.\n\n\n\nShow that the \\(AR(p)\\) model is a regression of \\(x_t\\) on previous terms in the series. (This is why it is called an “autoregressive model.”) Hint: write the \\(AR(p)\\) model in more familiar terms, letting \\[y_i = x_t, ~~ x_1 = x_{t-1}, ~~ x_2 = x_{t-2}, ~~ \\ldots, ~~ x_p = x_{t-p}, ~~ \\epsilon_i = w_t, ~~ \\text{and} ~~ \\beta_j = \\alpha_j\\]\n\nSolution: \\[\n\\begin{align*}\n    x_t &= \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t \\\\\n    y_i &= \\beta_1 x_{1i} ~+~ \\beta_2 x_{2i} ~+~ \\beta_3 x_{3i} ~+ \\cdots +~ \\beta_{p-1,i} x_{p-1,i} ~~~+~ \\beta_p x_{p,i} + \\epsilon_i \\\\\n\\end{align*}\n\\]\nThis is a multiple linear regression equation with zero intercept.\n\n\n\nExplain why the prediction at time \\(t\\) is given by \\[\n\\hat x_t = \\hat \\alpha_1 x_{t-1} + \\hat \\alpha_2 x_{t-2} + \\cdots + \\hat \\alpha_{p-1} x_{t-(p-1)} + \\hat \\alpha_p x_{t-p}\n\\]\n\nSolution:\nThe prediction at time \\(t\\) in a multiple regression setting would be: \\[\n    \\hat y_i = \\hat \\beta_1 x_{1i} ~~~+~ \\hat \\beta_2 x_{2i} ~+~ \\hat \\beta_3 x_{3i} ~~+ \\cdots +~~ \\hat \\beta_{p-1} x_{p-1,i} ~~+~ \\hat \\beta_p x_{p,i}\n\\] Translated to the \\(AR(p)\\) setting, this becomes: \\[\n    \\hat x_t = \\hat \\alpha_1 x_{t-1} + \\hat \\alpha_3 x_{t-2} + \\hat \\alpha_3 x_{t-3} + \\cdots + \\hat \\alpha_{p-1} x_{t-(p-1)} + \\hat \\alpha_p x_{t-p}\n\\]\n\n\n\nExplain why the model parameters (the \\(\\alpha\\)’s) can be estimated by minimizing the sum of the squared error terms: \\[\\sum_{t=1}^n \\left( \\hat w_t \\right)^2 = \\sum_{t=1}^n \\left( x_t - \\hat x_t \\right)^2\\]\n\nSolution:\nThis is exactly how the multiple linear regression coefficients are estimated…minimizing the sum of the squared error terms.\n\n\n\nWhat is the reason this is called an autoregressive model?\n\nSolution:\nThis is called an autoregressive model because we regress the current tern on the previous terms in the series.\n\n\n\nAbsolute Value\n\n\n\n\n\n\n\nCheck Your Understanding Solutions\n\n\n\nFind the absolute value of the following (complex) numbers:\n\n\\(|-3| = \\sqrt{(-3)^2 + 0^2} = 3\\)\n\\(|4i| = \\sqrt{(0)^2 + (4)^2} = 4\\)\n\\(|-3+4i| = \\sqrt{(-3)^2 + (4)^2} = 5\\)\n\\(\\left| - \\dfrac{\\sqrt{3}}{4} + \\dfrac{1}{4} i \\right| = \\sqrt{\\left( \\dfrac{\\sqrt{-3}}{4} \\right)^2 + \\left( \\dfrac{1}{4} \\right)^2} = \\sqrt{\\dfrac{3}{16} + \\dfrac{1}{16}} = \\dfrac{1}{2}\\)\n\\(\\left| \\dfrac{1}{\\sqrt{2}} - \\dfrac{1}{\\sqrt{2}} i \\right| = \\sqrt{\\left( \\frac{1}{\\sqrt{2}} \\right)^2 + \\left( \\frac{-1}{\\sqrt{2}} \\right)^2} = 1\\)\n\\(|5-12i| = \\sqrt{(5)^2 + (-12)^2} = 13\\)\n\n\n\n\nUsing the Characteristic Polynomial to Assess Stationarity\n\n\n\n\n\n\n\nCheck Your Understanding Solutions\n\n\n\nFor each of the following AR processes, do the following:\n\nWrite the AR process in terms of the backward shift operator.\nSolve the characteristic equation.\nDetermine if the AR process is stationary.\n\n\n\\(AR(1)\\) process: \\[\nx_t = x_{t-1} + w_t\n\\]\n\n\n\nSolution:\n\n\n\n\\(AR(1)\\) process: \\[\nx_t = \\frac{1}{3} x_{t-1} + w_t\n\\]\n\nSolution:\n\\[\\mathbf{B} = 3\\] This is a stationary AR process.\n\n\n\n\\(AR(2)\\) process: \\[\nx_t = - \\frac{1}{4} x_{t-1} + \\frac{1}{8} x_{t-2} + w_t\n\\]\n\nSolution:\n\\[ -\\frac{1}{8} \\left(\\mathbf{B}^2 - 2 \\mathbf{B} - 8 \\right) x_t = w_t \\] \\[B = -4, ~~~ B = 2\\] This is a stationary AR process.\n\n\n\n\\(AR(2)\\) process: \\[\nx_t = - \\frac{2}{3} x_{t-1} + \\frac{1}{3} x_{t-2} + w_t\n\\]\n\nSolution:\n\\[ -\\frac{1}{3} \\left(\\mathbf{B}^2 - 2 \\mathbf{B} - 3 \\right) x_t = w_t \\] \\[B = -1, ~~~ B = 3\\] This is a non-stationary AR process.\n\n\n\n\\(AR(2)\\) process: \\[\nx_t = -x_{t-1} - 2 x_{t-2} + w_t\n\\]\n\nSolution:\n\\[ \\left(\\mathbf{B}^2 + 1/2 * \\mathbf{B} + 1/2 \\right) x_t = w_t \\] \\[ B = \\frac{1}{4} \\pm \\frac{\\sqrt{7}}{4} i \\] \\[ |B| = \\sqrt{\\frac{1}{16} + \\frac{7}{16} i } = \\frac{1}{\\sqrt{2}} \\le 1 \\] This is a non-stationary AR process.\n\n\n\n\\(AR(2)\\) process: \\[\nx_t = \\frac{3}{2} x_{t-1} - x_{t-2} + w_t\n\\]\n\nSolution:\n\\[ \\left(\\mathbf{B}^2 - \\frac{3}{2} \\mathbf{B} + 1 \\right) x_t = w_t \\] \\[ B = \\frac{3}{4} \\pm \\frac{\\sqrt{7}}{4} i \\] \\[|B| = 1\\] This is a non-stationary AR process.\n\n\n\n\\(AR(2)\\) process: \\[\nx_t = 4 x_{t-2} + w_t\n\\]\n\nSolution:\n\\[ \\left(1 - 4\\mathbf{B}^2 \\right) x_t = w_t \\] \\[B = \\pm \\frac{1}{2}\\] \\[|B| = \\frac{1}{2}\\] This is a non-stationary AR process.\n\n\n\n\\(AR(3)\\) process: \\[\nx_t = \\frac{2}{3} x_{t-1} + \\frac{1}{4} x_{t-2} - \\frac{1}{6} x_{t-3} + w_t\n\\]\n\nSolution:\n\\[ B = 2, ~ -2, ~ \\frac{3}{2} \\] This is a stationary AR process.",
    "crumbs": [
      "Lesson 3",
      "Autoregressive (AR) Models"
    ]
  },
  {
    "objectID": "chapter_5.html",
    "href": "chapter_5.html",
    "title": "Lessons & Homework",
    "section": "",
    "text": "Chapter 5\n\n\n\n\n\n\nLessons\n\n\n\n\nLesson 1 - Linear Models, GLS, and Seasonal Indicator Variables\nLesson 2 - Harmonic Seasonal Variables - Part 1\nLesson 3 - Harmonic Seasonal Variables - Part 2\nLesson 4 - Transformations and Non-Linear Models\nLesson 5 - Forecasting, Inverse Transformation, and Bias Correction\n\n\n\n\n\n\n\n\n\nDownload Homework Assignments\n\n\n\n\n homework_5_1.qmd \n homework_5_2.qmd \n homework_5_3.qmd \n homework_5_4.qmd \n homework_5_5.qmd",
    "crumbs": [
      "Overview",
      "Lessons & Homework"
    ]
  },
  {
    "objectID": "chapter_5_lesson_1_handout.html",
    "href": "chapter_5_lesson_1_handout.html",
    "title": "Linear Models, GLS, and Seasonal Indicator Variables",
    "section": "",
    "text": "# Read in Women's Clothing Retail Sales data\nretail_ts &lt;- rio::import(\"data/retail_by_business_type.parquet\") |&gt;\n  filter(naics == 44812) |&gt;\n  filter(month &gt;= yearmonth(my(\"Jan 2004\")) & month &lt;= yearmonth(my(\"Dec 2006\"))) \n\n\n\n\n\n\n\n\n\n\nUse the estimated regression equation \\[\n  \\hat x_t = 2635 + 24 ~ t\n\\] to do the following:\n\nFind the equation for the line parallel to the regression line that passes through the origin.\nCompute the deviation of each observed point from the line you just obtained.\nCompute the value of \\(\\hat \\beta_i\\) for each month, where \\(i = 1, 2, \\ldots, 12\\).\nCompute the estimate of the time series for \\(t = 1, 2, \\ldots, 36\\).\nPredict the value of the time series for the next six months.\n\n\n\n\n\n\n\n\n\n\n$$Date$$\n$$t$$\n$$x_t$$\n$$\\hat \\alpha_1 t$$\n$$x_t - \\hat \\alpha_1 t$$\n$$\\hat\\beta_i$$\n$$\\hat x_t$$\n\n\n\n\nJan 2004\n1\n2267\n24\n2243\n2062.33\n2086.33\n\n\nFeb 2004\n2\n2402\n48\n2354\n\n\n\n\nMar 2004\n3\n2915\n\n\n\n\n\n\nApr 2004\n4\n3023\n\n\n\n\n\n\nMay 2004\n5\n3005\n\n\n\n\n\n\nJun 2004\n6\n2777\n144\n2633\n2569.67\n2713.67\n\n\nJul 2004\n7\n2595\n168\n2427\n2297\n2465\n\n\nAug 2004\n8\n2644\n192\n2452\n2345\n2537\n\n\nSep 2004\n9\n2774\n216\n2558\n2463\n2679\n\n\nOct 2004\n10\n2990\n240\n2750\n2574.33\n2814.33\n\n\nNov 2004\n11\n3090\n264\n2826\n2730\n2994\n\n\nDec 2004\n12\n4472\n288\n4184\n4173\n4461\n\n\nJan 2005\n13\n2338\n312\n2026\n2062.33\n2374.33\n\n\nFeb 2005\n14\n2453\n336\n2117\n\n\n\n\nMar 2005\n15\n3105\n\n\n\n\n\n\nApr 2005\n16\n3177\n\n\n\n\n\n\nMay 2005\n17\n3164\n\n\n\n\n\n\nJun 2005\n18\n3045\n432\n2613\n2569.67\n3001.67\n\n\nJul 2005\n19\n2741\n456\n2285\n2297\n2753\n\n\nAug 2005\n20\n2871\n480\n2391\n2345\n2825\n\n\nSep 2005\n21\n2963\n504\n2459\n2463\n2967\n\n\nOct 2005\n22\n3120\n528\n2592\n2574.33\n3102.33\n\n\nNov 2005\n23\n3342\n552\n2790\n2730\n3282\n\n\nDec 2005\n24\n4756\n576\n4180\n4173\n4749\n\n\n\n\n\n``````````{=html}\n\n\n\n\n$$Date$$\n$$t$$\n$$x_t$$\n$$\\hat \\alpha_1 t$$\n$$x_t - \\hat \\alpha_1 t$$\n$$\\hat\\beta_i$$\n$$\\hat x_t$$\n\n\n\n\nJan 2006\n25\n2518\n600\n1918\n2062.33\n2662.33\n\n\nFeb 2006\n26\n2518\n624\n1894\n\n\n\n\nMar 2006\n27\n3194\n\n\n\n\n\n\nApr 2006\n28\n3379\n\n\n\n\n\n\nMay 2006\n29\n3340\n\n\n\n\n\n\nJun 2006\n30\n3183\n720\n2463\n2569.67\n3289.67\n\n\nJul 2006\n31\n2923\n744\n2179\n2297\n3041\n\n\nAug 2006\n32\n2960\n768\n2192\n2345\n3113\n\n\nSep 2006\n33\n3164\n792\n2372\n2463\n3255\n\n\nOct 2006\n34\n3197\n816\n2381\n2574.33\n3390.33\n\n\nNov 2006\n35\n3414\n840\n2574\n2730\n3570\n\n\nDec 2006\n36\n5019\n864\n4155\n4173\n5037\n\n\nJan 2007\n37\n—\n888\n—\n2062.33\n2950.33\n\n\nFeb 2007\n38\n—\n912\n—\n\n\n\n\nMar 2007\n39\n—\n\n—\n\n\n\n\nApr 2007\n40\n—\n\n—\n\n\n\n\nMay 2007\n41\n—\n\n—\n\n\n\n\nJun 2007\n42\n—\n1008\n—\n2569.67\n3577.67"
  },
  {
    "objectID": "chapter_5_lesson_3.html",
    "href": "chapter_5_lesson_3.html",
    "title": "Harmonic Seasonal Variables - Part 2",
    "section": "",
    "text": "Fit linear regression models to time series data\n\n\nState the additive model with harmonic seasonal component\nSimulate a time series with harmonic seasonal components\nIdentify an appropriate function to model the trend in a given time series\nIdentify a parsimonious set of harmonic terms for use in a regression model\nFit the additive model with harmonic seasonal component to real-world data\nEvaluate residuals using a correlogram and partial correlogram to ensure they meet the assumptions\n\n\n\n\nApply model selection criteria\n\n\nUse AIC to aid in model selection",
    "crumbs": [
      "Lesson 3",
      "Harmonic Seasonal Variables - Part 2"
    ]
  },
  {
    "objectID": "chapter_5_lesson_3.html#learning-outcomes",
    "href": "chapter_5_lesson_3.html#learning-outcomes",
    "title": "Harmonic Seasonal Variables - Part 2",
    "section": "",
    "text": "Fit linear regression models to time series data\n\n\nState the additive model with harmonic seasonal component\nSimulate a time series with harmonic seasonal components\nIdentify an appropriate function to model the trend in a given time series\nIdentify a parsimonious set of harmonic terms for use in a regression model\nFit the additive model with harmonic seasonal component to real-world data\nEvaluate residuals using a correlogram and partial correlogram to ensure they meet the assumptions\n\n\n\n\nApply model selection criteria\n\n\nUse AIC to aid in model selection",
    "crumbs": [
      "Lesson 3",
      "Harmonic Seasonal Variables - Part 2"
    ]
  },
  {
    "objectID": "chapter_5_lesson_3.html#preparation",
    "href": "chapter_5_lesson_3.html#preparation",
    "title": "Harmonic Seasonal Variables - Part 2",
    "section": "Preparation",
    "text": "Preparation\n\nReview Section 5.6",
    "crumbs": [
      "Lesson 3",
      "Harmonic Seasonal Variables - Part 2"
    ]
  },
  {
    "objectID": "chapter_5_lesson_3.html#class-activity-monthly-average-high-temperature-in-rexburg-20-min",
    "href": "chapter_5_lesson_3.html#class-activity-monthly-average-high-temperature-in-rexburg-20-min",
    "title": "Harmonic Seasonal Variables - Part 2",
    "section": "Class Activity: Monthly Average High Temperature in Rexburg (20 min)",
    "text": "Class Activity: Monthly Average High Temperature in Rexburg (20 min)\n\nVisualization of the Time Series\nConsider the mean monthly high temperature in Rexburg.\n\n\nShow the code\nweather_df &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/rexburg_weather_monthly.csv\") |&gt;\n  mutate(dates = my(date_text)) |&gt;\n  filter(dates &gt;= my(\"1/2008\") & dates &lt;= my(\"12/2023\")) |&gt;\n  rename(x = avg_daily_high_temp) |&gt;\n  mutate(TIME = 1:n()) |&gt;\n  mutate(\n    cos1 = cos(2 * pi * 1 * TIME/12),\n    cos2 = cos(2 * pi * 2 * TIME/12),\n    cos3 = cos(2 * pi * 3 * TIME/12),\n    cos4 = cos(2 * pi * 4 * TIME/12),\n    cos5 = cos(2 * pi * 5 * TIME/12),\n    cos6 = cos(2 * pi * 6 * TIME/12),\n    sin1 = sin(2 * pi * 1 * TIME/12),\n    sin2 = sin(2 * pi * 2 * TIME/12),\n    sin3 = sin(2 * pi * 3 * TIME/12),\n    sin4 = sin(2 * pi * 4 * TIME/12),\n    sin5 = sin(2 * pi * 5 * TIME/12),\n    sin6 = sin(2 * pi * 6 * TIME/12)) |&gt;\n  as_tsibble(index = TIME)\n\nweather_df |&gt;\n  as_tsibble(index = dates) |&gt;\n  autoplot(.vars = x) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#F0E442\") +\n    labs(\n      x = \"Month\",\n      y = \"Mean Daily High Temperature (Fahrenheit)\",\n      title = \"Time Plot of Mean Daily Rexburg High Temperature by Month\",\n      subtitle = paste0(\"(\", format(weather_df$dates %&gt;% head(1), \"%b %Y\"), endash, format(weather_df$dates %&gt;% tail(1), \"%b %Y\"), \")\")\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5),\n      plot.subtitle = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\n\n\nModel Selection\n\n\n\n\n\n\nStandardizing the Time Variable\n\n\n\n\n\n\n\nStandardizing the Time Variable\nTo avoid serious floating point errors, we standardize the time variable. First, compute the sine and cosine terms using the original time variable, then transform the time variable by subtracting its mean and dividing by its standard deviation. (In other words, compute a \\(z\\)-score.)\nThe model is adjusted accordingly after fitting.\n\n\n\n\n\n\nWarning\n\n\n\nWhen the independent variable (the measure of time) is large, floating point errors in the computation of the regression coefficents can be substantial.\n\n\nWe will demonstrate standardizing the time variable below.\n\n\nComputing the standardized time variable\nOur time variable was a simple incremented value counting the months ranging from 1 (representing 1/2008) to 192 (representing 12/2023).\nWe standardize this variable by the transformation:\n\\[\nzTIME\n  = \\frac{t - \\bar t}{s_t}\n  = \\frac{t - 96.5}{55.57}\n\\]\nwhere the mean of the variable TIME is \\(\\bar t = 96.5\\), and the standard deviation is \\(s_t = 55.57\\).\n\nweather_df &lt;- weather_df |&gt;\n  mutate(zTIME = (TIME - mean(TIME)) / sd(TIME))\n\nNow, we fit the trend components of the models using zTIME instead of TIME. We will start by modeling a cubic trend term.\n\n\n\n\n\n\n\n\n\n\n\n\nCubic Trends\n\n\n\n\n\n\n\nCubic Trend: Full Model\nVisually, we can identify a positive linear trend in the data. It is possible that there are higher-order properties of the trend. We will include a quadratic and cubic term in our search for a model.\nIn addition to modeling the trend, we need to include terms for the seasonal component. We start with a full model that includes all six of the the sine and cosine terms from the summation above.\n\n\nShow the code\n# Cubic model with standardized time variable\n\nfull_cubic_lm &lt;- weather_df |&gt;\n  model(full_cubic = TSLM(x ~ zTIME + I(zTIME^2) + I(zTIME^3) +\n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n    + sin4 + cos4 + sin5 + cos5 + cos6 ))\n\nfull_cubic_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 15 × 7\n   .model     term        estimate std.error statistic   p.value sig  \n   &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 full_cubic (Intercept)  57.2        0.405   141.    7.11e-184 TRUE \n 2 full_cubic zTIME        -0.359      0.678    -0.529 5.98e-  1 FALSE\n 3 full_cubic I(zTIME^2)   -0.733      0.303    -2.42  1.67e-  2 TRUE \n 4 full_cubic I(zTIME^3)    0.586      0.348     1.68  9.41e-  2 FALSE\n 5 full_cubic sin1        -16.0        0.383   -41.8   1.23e- 93 TRUE \n 6 full_cubic cos1        -23.7        0.382   -62.1   4.30e-122 TRUE \n 7 full_cubic sin2          1.29       0.382     3.37  9.34e-  4 TRUE \n 8 full_cubic cos2         -2.87       0.382    -7.51  2.74e- 12 TRUE \n 9 full_cubic sin3         -0.819      0.382    -2.15  3.33e-  2 TRUE \n10 full_cubic cos3          0.608      0.382     1.59  1.13e-  1 FALSE\n11 full_cubic sin4          0.377      0.382     0.987 3.25e-  1 FALSE\n12 full_cubic cos4         -0.214      0.382    -0.561 5.76e-  1 FALSE\n13 full_cubic sin5         -0.0810     0.382    -0.212 8.32e-  1 FALSE\n14 full_cubic cos5         -0.0433     0.382    -0.113 9.10e-  1 FALSE\n15 full_cubic cos6         -0.567      0.270    -2.10  3.70e-  2 TRUE \n\n\nShow the code\nforecast_df &lt;- full_cubic_lm |&gt; forecast(weather_df, ) |&gt; as_tibble() |&gt; dplyr::select(zTIME, .mean) |&gt; rename(pred = .mean)\n\nweather_df |&gt;\n  left_join(forecast_df, by = \"zTIME\") |&gt;\n  as_tsibble(index = dates) |&gt;\n  autoplot(.vars = x) +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"#F0E442\") +\n  geom_line(aes(y = pred), color = \"#56B4E9\", alpha = 0.75) +\n    labs(\n      x = \"Month\",\n      y = \"Mean Daily High Temperature (Fahrenheit)\",\n      title = \"Time Plot of Mean Daily Rexburg High Temperature by Month\",\n      subtitle = paste0(\"(\", format(weather_df$dates %&gt;% head(1), \"%b %Y\"), endash, format(weather_df$dates %&gt;% tail(1), \"%b %Y\"), \")\")\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5),\n      plot.subtitle = element_text(hjust = 0.5)\n    )\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nThe cubic term in the trend is not significant, but the quadratic term is. We will fit a quadratic model.\n\n\n\n\n\n\nCaution\n\n\n\nIf you choose a different range of dates, you may get a different result. The regression model is fitted to the data, not to the physical situation.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nQuadratic Trends\n\n\n\n\n\n\n\nQuadratic Trend: Full Model\nWe now fit a quadratic model that includes all of the seasonal terms.\n\n\nShow the code\nfull_quadratic_lm &lt;- weather_df |&gt;\n  model(full_quadratic = TSLM(x ~ zTIME + I(zTIME^2) +\n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n    + sin4 + cos4 + sin5 + cos5 + cos6 ))\n\nfull_quadratic_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 14 × 7\n   .model         term        estimate std.error statistic   p.value sig  \n   &lt;chr&gt;          &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 full_quadratic (Intercept)  57.2        0.407  141.     2.70e-184 TRUE \n 2 full_quadratic zTIME         0.688      0.273    2.52   1.25e-  2 TRUE \n 3 full_quadratic I(zTIME^2)   -0.733      0.305   -2.40   1.73e-  2 TRUE \n 4 full_quadratic sin1        -16.1        0.384  -41.8    5.46e- 94 TRUE \n 5 full_quadratic cos1        -23.7        0.384  -61.8    3.68e-122 TRUE \n 6 full_quadratic sin2          1.26       0.384    3.29   1.19e-  3 TRUE \n 7 full_quadratic cos2         -2.86       0.384   -7.44   4.03e- 12 TRUE \n 8 full_quadratic sin3         -0.832      0.384   -2.17   3.15e-  2 TRUE \n 9 full_quadratic cos3          0.620      0.384    1.62   1.08e-  1 FALSE\n10 full_quadratic sin4          0.370      0.384    0.963  3.37e-  1 FALSE\n11 full_quadratic cos4         -0.202      0.384   -0.525  6.00e-  1 FALSE\n12 full_quadratic sin5         -0.0844     0.384   -0.220  8.26e-  1 FALSE\n13 full_quadratic cos5         -0.0307     0.384   -0.0799 9.36e-  1 FALSE\n14 full_quadratic cos6         -0.561      0.271   -2.07   4.01e-  2 TRUE \n\n\nThe coefficient on the quadratic trend term is very small and negative. This suggests the data may indicate a decrease in the rate of global warming.\n\n\nQuadratic Trend: Reduced Model 1\nEliminating the \\(i=4\\) and \\(i=5\\) terms, we get the model:\n\n\nShow the code\nreduced1_quadratic_lm &lt;- weather_df |&gt;\n  model(reduced_quadratic_1  = TSLM(x ~ zTIME + I(zTIME^2) + sin1 + cos1 + sin2 + cos2 + sin3 + cos3 + cos6))\n\nreduced1_quadratic_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 10 × 7\n   .model              term        estimate std.error statistic   p.value sig  \n   &lt;chr&gt;               &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 reduced_quadratic_1 (Intercept)   57.2       0.404    142.   4.02e-188 TRUE \n 2 reduced_quadratic_1 zTIME          0.684     0.271      2.53 1.23e-  2 TRUE \n 3 reduced_quadratic_1 I(zTIME^2)    -0.733     0.303     -2.42 1.64e-  2 TRUE \n 4 reduced_quadratic_1 sin1         -16.1       0.381    -42.1  8.23e- 96 TRUE \n 5 reduced_quadratic_1 cos1         -23.7       0.381    -62.2  1.34e-124 TRUE \n 6 reduced_quadratic_1 sin2           1.26      0.381      3.32 1.09e-  3 TRUE \n 7 reduced_quadratic_1 cos2          -2.86      0.381     -7.50 2.71e- 12 TRUE \n 8 reduced_quadratic_1 sin3          -0.832     0.381     -2.18 3.02e-  2 TRUE \n 9 reduced_quadratic_1 cos3           0.621     0.381      1.63 1.05e-  1 FALSE\n10 reduced_quadratic_1 cos6          -0.561     0.269     -2.08 3.86e-  2 TRUE \n\n\n\n\nQuadratic Trend: Reduced Model 2\nSequentially removing the cosine term for \\(i = 3\\), we get:\n\n\nShow the code\nreduced2_quadratic_lm &lt;- weather_df |&gt;\n  model(reduced_quadratic_2  = TSLM(x ~ zTIME + I(zTIME^2) + sin1 + cos1 + sin2 + cos2 + sin3 + cos6))\n\nreduced2_quadratic_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 9 × 7\n  .model              term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;               &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_quadratic_2 (Intercept)   57.2       0.406    141.   1.41e-188 TRUE \n2 reduced_quadratic_2 zTIME          0.689     0.272      2.54 1.20e-  2 TRUE \n3 reduced_quadratic_2 I(zTIME^2)    -0.733     0.304     -2.41 1.69e-  2 TRUE \n4 reduced_quadratic_2 sin1         -16.1       0.383    -41.9  8.35e- 96 TRUE \n5 reduced_quadratic_2 cos1         -23.7       0.383    -62.0  9.99e-125 TRUE \n6 reduced_quadratic_2 sin2           1.26      0.383      3.30 1.14e-  3 TRUE \n7 reduced_quadratic_2 cos2          -2.86      0.383     -7.47 3.23e- 12 TRUE \n8 reduced_quadratic_2 sin3          -0.832     0.383     -2.17 3.10e-  2 TRUE \n9 reduced_quadratic_2 cos6          -0.561     0.271     -2.07 3.95e-  2 TRUE \n\n\n\n\nQuadratic Trend: Reduced Model 3\nThis is the quadratic model obtained by eliminating the \\(i=6\\) term from the previous model.\n\n\nShow the code\nreduced3_quadratic_lm &lt;- weather_df |&gt;\n  model(reduced_quadratic_3  = TSLM(x ~ zTIME + I(zTIME^2) + sin1 + cos1 + sin2 + cos2 + sin3))\n\nreduced3_quadratic_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 8 × 7\n  .model              term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;               &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_quadratic_3 (Intercept)   57.2       0.409    140.   1.12e-188 TRUE \n2 reduced_quadratic_3 zTIME          0.684     0.274      2.50 1.34e-  2 TRUE \n3 reduced_quadratic_3 I(zTIME^2)    -0.733     0.307     -2.39 1.79e-  2 TRUE \n4 reduced_quadratic_3 sin1         -16.1       0.386    -41.6  1.76e- 95 TRUE \n5 reduced_quadratic_3 cos1         -23.7       0.386    -61.4  1.63e-124 TRUE \n6 reduced_quadratic_3 sin2           1.26      0.386      3.28 1.26e-  3 TRUE \n7 reduced_quadratic_3 cos2          -2.86      0.386     -7.40 4.67e- 12 TRUE \n8 reduced_quadratic_3 sin3          -0.832     0.386     -2.16 3.25e-  2 TRUE \n\n\n\n\nQuadratic Trend: Reduced Model 4\nThis is similar to the previous model, except both the sine and cosine terms are included for \\(i=3\\).\n\n\nShow the code\nreduced4_quadratic_lm &lt;- weather_df |&gt;\n  model(reduced_quadratic_4  = TSLM(x ~ zTIME + I(zTIME^2) + sin1 + cos1 + sin2 + cos2 + sin3 + cos3))\n\nreduced4_quadratic_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 9 × 7\n  .model              term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;               &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_quadratic_4 (Intercept)   57.2       0.408    140.   3.22e-188 TRUE \n2 reduced_quadratic_4 zTIME          0.679     0.273      2.49 1.38e-  2 TRUE \n3 reduced_quadratic_4 I(zTIME^2)    -0.733     0.305     -2.40 1.74e-  2 TRUE \n4 reduced_quadratic_4 sin1         -16.1       0.385    -41.8  1.76e- 95 TRUE \n5 reduced_quadratic_4 cos1         -23.7       0.384    -61.7  2.21e-124 TRUE \n6 reduced_quadratic_4 sin2           1.26      0.384      3.29 1.21e-  3 TRUE \n7 reduced_quadratic_4 cos2          -2.86      0.384     -7.43 3.95e- 12 TRUE \n8 reduced_quadratic_4 sin3          -0.832     0.384     -2.16 3.17e-  2 TRUE \n9 reduced_quadratic_4 cos3           0.621     0.384      1.61 1.08e-  1 FALSE\n\n\n\n\nQuadratic Trend: Reduced Model 5\nThis quadratic model only includes the \\(i=1\\) and \\(i=2\\) terms.\n\n\nShow the code\nreduced5_quadratic_lm &lt;- weather_df |&gt;\n  model(reduced_quadratic_5  = TSLM(x ~ zTIME + I(zTIME^2) + sin1 + cos1 + sin2 + cos2))\n\nreduced5_quadratic_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 7 × 7\n  .model              term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;               &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_quadratic_5 (Intercept)   57.2       0.413    138.   1.06e-188 TRUE \n2 reduced_quadratic_5 zTIME          0.692     0.277      2.50 1.33e-  2 TRUE \n3 reduced_quadratic_5 I(zTIME^2)    -0.733     0.310     -2.37 1.90e-  2 TRUE \n4 reduced_quadratic_5 sin1         -16.1       0.390    -41.2  4.40e- 95 TRUE \n5 reduced_quadratic_5 cos1         -23.7       0.390    -60.8  3.16e-124 TRUE \n6 reduced_quadratic_5 sin2           1.26      0.390      3.24 1.40e-  3 TRUE \n7 reduced_quadratic_5 cos2          -2.86      0.390     -7.33 6.96e- 12 TRUE \n\n\n\n\nQuadratic Trend: Reduced Model 6\nThis model includes a quadratic effect for time, but only includes the sine and cosine terms corresponding to \\(i=1\\).\n\n\nShow the code\nreduced6_quadratic_lm &lt;- weather_df |&gt;\n  model(reduced_quadratic_6  = TSLM(x ~ zTIME + I(zTIME^2) + sin1 + cos1))\n\nreduced6_quadratic_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 5 × 7\n  .model              term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;               &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_quadratic_6 (Intercept)   57.2       0.477    120.   9.32e-179 TRUE \n2 reduced_quadratic_6 zTIME          0.646     0.319      2.02 4.45e-  2 TRUE \n3 reduced_quadratic_6 I(zTIME^2)    -0.734     0.358     -2.05 4.14e-  2 TRUE \n4 reduced_quadratic_6 sin1         -16.1       0.451    -35.7  2.32e- 85 TRUE \n5 reduced_quadratic_6 cos1         -23.7       0.450    -52.7  4.14e-114 TRUE \n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLinear Trends\n\n\n\n\n\n\n\nLinear Trend: Full Model\nThis is the full model with a linear time component. All of the period functions are included from \\(i=1\\) to \\(i=6\\).\n\n\nShow the code\nfull_linear_lm &lt;- weather_df |&gt;\n  model(full_linear = TSLM(x ~ zTIME +\n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n    + sin4 + cos4 + sin5 + cos5 + cos6 ))\n\nfull_linear_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 13 × 7\n   .model      term        estimate std.error statistic   p.value sig  \n   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 full_linear (Intercept)  56.5        0.275  205.     2.02e-214 TRUE \n 2 full_linear zTIME         0.688      0.276    2.49   1.37e-  2 TRUE \n 3 full_linear sin1        -16.1        0.389  -41.3    2.19e- 93 TRUE \n 4 full_linear cos1        -23.7        0.389  -61.0    1.16e-121 TRUE \n 5 full_linear sin2          1.26       0.389    3.25   1.38e-  3 TRUE \n 6 full_linear cos2         -2.86       0.389   -7.35   6.86e- 12 TRUE \n 7 full_linear sin3         -0.832      0.389   -2.14   3.37e-  2 TRUE \n 8 full_linear cos3          0.620      0.389    1.59   1.13e-  1 FALSE\n 9 full_linear sin4          0.370      0.389    0.950  3.43e-  1 FALSE\n10 full_linear cos4         -0.202      0.389   -0.518  6.05e-  1 FALSE\n11 full_linear sin5         -0.0845     0.389   -0.217  8.28e-  1 FALSE\n12 full_linear cos5         -0.0307     0.389   -0.0789 9.37e-  1 FALSE\n13 full_linear cos6         -0.561      0.275   -2.04   4.28e-  2 TRUE \n\n\n\n\nLinear Trend: Reduced Model 1\nThis linear model excludes the terms corresponding to \\(i=4\\) and \\(i=5\\).\n\n\nShow the code\nreduced1_linear_lm &lt;- weather_df |&gt;\n  model(reduced_linear_1  = TSLM(x ~ zTIME + sin1 + cos1 + sin2 + cos2 + sin3 + cos3 + cos6))\n\nreduced1_linear_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 9 × 7\n  .model           term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;            &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_linear_1 (Intercept)   56.5       0.273    207.   6.64e-219 TRUE \n2 reduced_linear_1 zTIME          0.684     0.274      2.49 1.35e-  2 TRUE \n3 reduced_linear_1 sin1         -16.1       0.386    -41.6  3.44e- 95 TRUE \n4 reduced_linear_1 cos1         -23.7       0.386    -61.4  4.39e-124 TRUE \n5 reduced_linear_1 sin2           1.26      0.386      3.27 1.26e-  3 TRUE \n6 reduced_linear_1 cos2          -2.86      0.386     -7.40 4.65e- 12 TRUE \n7 reduced_linear_1 sin3          -0.832     0.386     -2.16 3.24e-  2 TRUE \n8 reduced_linear_1 cos3           0.620     0.386      1.61 1.10e-  1 FALSE\n9 reduced_linear_1 cos6          -0.561     0.273     -2.06 4.12e-  2 TRUE \n\n\n\n\nLinear Trend: Reduced Model 2\nThis represents the reduction of the previous model by eliminating the cosine term for \\(i=3\\).\n\n\nShow the code\nreduced2_linear_lm &lt;- weather_df |&gt;\n  model(reduced_linear_2  = TSLM(x ~ zTIME + sin1 + cos1 + sin2 + cos2 + sin3 + cos6))\n\nreduced2_linear_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 8 × 7\n  .model           term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;            &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_linear_2 (Intercept)   56.5       0.274    206.   1.56e-219 TRUE \n2 reduced_linear_2 zTIME          0.689     0.275      2.50 1.31e-  2 TRUE \n3 reduced_linear_2 sin1         -16.1       0.388    -41.4  3.41e- 95 TRUE \n4 reduced_linear_2 cos1         -23.7       0.388    -61.2  3.22e-124 TRUE \n5 reduced_linear_2 sin2           1.26      0.388      3.26 1.32e-  3 TRUE \n6 reduced_linear_2 cos2          -2.86      0.388     -7.37 5.48e- 12 TRUE \n7 reduced_linear_2 sin3          -0.832     0.388     -2.15 3.31e-  2 TRUE \n8 reduced_linear_2 cos6          -0.561     0.274     -2.05 4.20e-  2 TRUE \n\n\n\n\nLinear Trend: Reduced Model 3\nThis model is similar to the previous one, but the cosine term associated with \\(i=6\\) has been excluded.\n\n\nShow the code\nreduced3_linear_lm &lt;- weather_df |&gt;\n  model(reduced_linear_3  = TSLM(x ~ zTIME + sin1 + cos1 + sin2 + cos2 + sin3))\n\nreduced3_linear_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 7 × 7\n  .model           term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;            &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_linear_3 (Intercept)   56.5       0.276    204.   8.13e-220 TRUE \n2 reduced_linear_3 zTIME          0.684     0.278      2.47 1.46e-  2 TRUE \n3 reduced_linear_3 sin1         -16.1       0.391    -41.1  6.93e- 95 TRUE \n4 reduced_linear_3 cos1         -23.7       0.391    -60.6  5.06e-124 TRUE \n5 reduced_linear_3 sin2           1.26      0.391      3.23 1.45e-  3 TRUE \n6 reduced_linear_3 cos2          -2.86      0.391     -7.31 7.77e- 12 TRUE \n7 reduced_linear_3 sin3          -0.832     0.391     -2.13 3.46e-  2 TRUE \n\n\n\n\nLinear Trend: Reduced Model 4\nThis model consists of a linear trend with all the periodic terms associated with \\(i=1\\) through \\(i=3\\).\n\n\nShow the code\nreduced4_linear_lm &lt;- weather_df |&gt;\n  model(reduced_linear_4  = TSLM(x ~ zTIME + sin1 + cos1 + sin2 + cos2 + sin3 + cos3))\n\nreduced4_linear_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 8 × 7\n  .model           term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;            &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_linear_4 (Intercept)   56.5       0.275    205.   3.50e-219 TRUE \n2 reduced_linear_4 zTIME          0.679     0.276      2.45 1.50e-  2 TRUE \n3 reduced_linear_4 sin1         -16.1       0.390    -41.2  7.05e- 95 TRUE \n4 reduced_linear_4 cos1         -23.7       0.389    -60.9  6.98e-124 TRUE \n5 reduced_linear_4 sin2           1.26      0.389      3.25 1.39e-  3 TRUE \n6 reduced_linear_4 cos2          -2.86      0.389     -7.34 6.63e- 12 TRUE \n7 reduced_linear_4 sin3          -0.832     0.389     -2.14 3.38e-  2 TRUE \n8 reduced_linear_4 cos3           0.620     0.389      1.59 1.13e-  1 FALSE\n\n\n\n\nLinear Trend: Reduced Model 5\nThis model has a linear trend and only includes the periodic terms corresponding to \\(i=1\\) and \\(i=2\\).\n\n\nShow the code\nreduced5_linear_lm &lt;- weather_df |&gt;\n  model(reduced_linear_5  = TSLM(x ~ zTIME + sin1 + cos1 + sin2 + cos2))\n\nreduced5_linear_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 6 × 7\n  .model           term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;            &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_linear_5 (Intercept)   56.5       0.279    202.   5.06e-220 TRUE \n2 reduced_linear_5 zTIME          0.692     0.280      2.47 1.44e-  2 TRUE \n3 reduced_linear_5 sin1         -16.1       0.395    -40.7  1.66e- 94 TRUE \n4 reduced_linear_5 cos1         -23.7       0.395    -60.1  9.40e-124 TRUE \n5 reduced_linear_5 sin2           1.26      0.395      3.20 1.60e-  3 TRUE \n6 reduced_linear_5 cos2          -2.86      0.395     -7.24 1.13e- 11 TRUE \n\n\n\n\nLinear Trend: Reduced Model 6\nThis simple model includes a linear trend and period terms associated with \\(i=1\\).\n\n\nShow the code\nreduced6_linear_lm &lt;- weather_df |&gt;\n  model(reduced_linear_6  = TSLM(x ~ zTIME + sin1 + cos1))\n\nreduced6_linear_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 4 × 7\n  .model           term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;            &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_linear_6 (Intercept)   56.5       0.321    176.   1.43e-210 TRUE \n2 reduced_linear_6 zTIME          0.646     0.322      2.01 4.63e-  2 TRUE \n3 reduced_linear_6 sin1         -16.1       0.454    -35.4  5.09e- 85 TRUE \n4 reduced_linear_6 cos1         -23.7       0.454    -52.2  7.20e-114 TRUE \n\n\n\n\n\n\n\n\n\n\n\n\n\nModel Comparison\n\n\n\n\n\n\n\nModel Comparison\nNow, we compare all the models side-by-side.\n\n\nShow the code\nmodel_combined &lt;- weather_df |&gt;\n  model(\n    full_cubic = TSLM(x ~ TIME + I(TIME^2) + I(TIME^3) +\n      sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n      + sin4 + cos4 + sin5 + cos5 + cos6),\n    full_quadratic = TSLM(x ~ TIME + I(TIME^2) +\n      sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n      + sin4 + cos4 + sin5 + cos5 + cos6),\n    full_linear = TSLM(x ~ TIME  +\n      sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n      + sin4 + cos4 + sin5 + cos5 + cos6 ),\n    reduced_quadratic_1  = TSLM(x ~ TIME + I(TIME^2) + sin1 + cos1 + sin2 + cos2 + sin3 + cos3 + cos6),\n    reduced_quadratic_2  = TSLM(x ~ TIME + I(TIME^2) + sin1 + cos1 + sin2 + cos2 + sin3 + cos6),\n    reduced_quadratic_3  = TSLM(x ~ TIME + I(TIME^2) + sin1 + cos1 + sin2 + cos2 + sin3),\n    reduced_quadratic_4  = TSLM(x ~ TIME + I(TIME^2) + sin1 + cos1 + sin2 + cos2 + sin3 + cos3),\n    reduced_quadratic_5  = TSLM(x ~ TIME + I(TIME^2) + sin1 + cos1 + sin2 + cos2),\n    reduced_quadratic_6  = TSLM(x ~ TIME + I(TIME^2) + sin1 + cos1),\n    reduced_linear_1  = TSLM(x ~ TIME + sin1 + cos1 + sin2 + cos2 + sin3 + cos3 + cos6),\n    reduced_linear_2  = TSLM(x ~ TIME + sin1 + cos1 + sin2 + cos2 + sin3 + cos6),\n    reduced_linear_3  = TSLM(x ~ TIME + sin1 + cos1 + sin2 + cos2 + sin3),\n    reduced_linear_4  = TSLM(x ~ TIME + sin1 + cos1 + sin2 + cos2 + sin3 + cos3),\n    reduced_linear_5  = TSLM(x ~ TIME + sin1 + cos1 + sin2 + cos2),\n    reduced_linear_6  = TSLM(x ~ TIME + sin1 + cos1)\n  )\n\nglance(model_combined) |&gt;\n  select(.model, AIC, AICc, BIC)\n\n\n\n\n\n\n\nModel\nAIC\nAICc\nBIC\n\n\n\n\nfull_cubic\n523\n526.1\n575.1\n\n\nfull_quadratic\n524.1\n526.8\n572.9\n\n\nfull_linear\n528.2\n530.6\n573.8\n\n\nreduced_quadratic_1\n**517.4**\n**518.9**\n553.3\n\n\nreduced_quadratic_2\n518.2\n519.4\n550.8\n\n\nreduced_quadratic_3\n520.7\n521.7\n550\n\n\nreduced_quadratic_4\n520\n521.2\n552.5\n\n\nreduced_quadratic_5\n523.5\n524.2\n**549.5**\n\n\nreduced_quadratic_6\n576.7\n577.1\n596.2\n\n\nreduced_linear_1\n521.5\n522.7\n554.1\n\n\nreduced_linear_2\n522.2\n523.2\n551.5\n\n\nreduced_linear_3\n524.5\n525.3\n550.6\n\n\nreduced_linear_4\n523.9\n524.9\n553.2\n\n\nreduced_linear_5\n527.2\n527.8\n550\n\n\nreduced_linear_6\n579\n579.3\n595.2\n\n\n\n\n\n\n\nWe look for the smallest value of the AIC, AICc, and BIC criteria. These methods do not have to agree with each other, and they provide different perspectives based on their various algorithms. Notice that the AIC and AICc criteria both suggest the model we titled “Reduced Quadratic 1:”\n\\[\\begin{align*}\n  x_t &= \\beta_0 + \\beta_1 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right) + \\beta_2 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right)^2\n                        + \\beta_3 \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n                        + \\beta_4 \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + \\beta_5 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n                        + \\beta_6 \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + \\beta_7 \\sin \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right)  \n                        + \\beta_8 \\cos \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + \\beta_9 \\cos \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right)\n               \\phantom{+ \\beta_9 \\sin \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right)}\n      + z_t\n\\end{align*}\\]\nThe BIC criteria points to the “Reduced Quadratic 5” model:\n\\[\\begin{align*}\n  x_t &= \\beta_0 + \\beta_1 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right) + \\beta_2 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right)^2\n            + \\beta_3 \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + \\beta_4 \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + \\beta_5 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + \\beta_6 \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right) + z_t\n\\end{align*}\\]\nIf there are two competing models that are both satisfactory, it is usually preferable to choose the more parsimonious (or simpler) model.\nSometimes there will be a model that does not attain the lowest value of these measures, yet you may still want to use it if the AIC, AICc, and BIC values are not too much larger than the smallest values. Why might you do this? If the model is particularly interpretable or makes logical sense in the context of the physical situation, it is better than a model with a lower AIC that is not readily interpretable.\nYou may even choose to include terms that are not statistically significant, if you determine they are practically important. For example, if a quadratic term is significant, but the linear term is not, it is a good practice to include the linear term anyway.\nNotice that the linear models corresponding to the “Reduced Quadratic 1” and “Reduced Quadratic 5” models have AIC/AICc/BIC values that are not much larger than the minimum values. Given other external evidence related to global warming, it is unlikely that the second derivative of the function representing the Earth’s mean temperature is negative. In other words, it does not seem like the rate at which the earth is warming is decreasing. Even if there is a quadratic component to the trend, it is not visible to the eye in the time plot.\nFor these reasons, we will apply the “Reduced Linear 5” model. This model implies a linear trend in the mean temperature of the Earth. The BIC for this model is not much bigger than the BIC for the “Reduced Quadratic 5” model. This model is simpler than the “Reduced Quadratic 1,” “Reduced Quadratic 5,” and “Reduced Linear 1” models.\n\\[\\begin{align*}\n  x_t &= \\beta_0 + \\beta_1 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right)  \n            + \\beta_2 \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + \\beta_3 \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ + \\beta_4 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + \\beta_5 \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right) + z_t\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\n\n\nModel Fitting\n\n\n\n\n\n\n\nModel Fitting\nThe model we have chosen is the “Reduced Linear 5” model. For convenience, we reprint the coefficients here.\n\nReduced Linear 5\n\n\nShow the code\nreduced5_linear_lm &lt;- weather_df |&gt;\n  model(reduced_linear_5  = TSLM(x ~ zTIME + sin1 + cos1 + sin2 + cos2))\n\nreduced5_linear_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 6 × 7\n  .model           term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;            &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_linear_5 (Intercept)   56.5       0.279    202.   5.06e-220 TRUE \n2 reduced_linear_5 zTIME          0.692     0.280      2.47 1.44e-  2 TRUE \n3 reduced_linear_5 sin1         -16.1       0.395    -40.7  1.66e- 94 TRUE \n4 reduced_linear_5 cos1         -23.7       0.395    -60.1  9.40e-124 TRUE \n5 reduced_linear_5 sin2           1.26      0.395      3.20 1.60e-  3 TRUE \n6 reduced_linear_5 cos2          -2.86      0.395     -7.24 1.13e- 11 TRUE \n\n\nShow the code\nr5lin_coef_unrounded &lt;- reduced5_linear_lm |&gt;\n  tidy() |&gt;\n  select(term, estimate, std.error)\n\nr5lin_coef &lt;- r5lin_coef_unrounded |&gt;\n  round_df(3)\n\n\nThe fitted model is therefore:\n\\[\\begin{align*}\n  x_t &= \\hat \\beta_0 + \\hat \\beta_1 \\left( zTIME \\right)  \\\\\n      & ~~~~~~~~~~ + \\hat \\beta_2 \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + \\hat \\beta_3 \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~ + \\hat \\beta_4 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + \\hat \\beta_5 \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n      \\\\\n      &= \\hat \\beta_0 + \\hat \\beta_1 \\left( \\frac{t - \\bar t}{s_t} \\right)  \\\\\n      & ~~~~~~~~~~ + \\hat \\beta_2 \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + \\hat \\beta_3 \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~ + \\hat \\beta_4 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + \\hat \\beta_5 \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n      \\\\\n      &= 56.467\n            + 0.692 \\left( \\frac{t - 96.5}{55.57} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~ + (-16.068) \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + (-23.708) \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~ + 1.264 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + (-2.858) \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n      \\\\\n\\end{align*}\\]\nThe linear trend term can be simplified, if desired, but it is not necessary.\n\\[\\begin{align*}\n  x_t\n      &= 56.467\n            + 0.692 \\left( \\frac{t - 96.5}{55.57} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~ + (-16.068) \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + (-23.708) \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~ + 1.264 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + (-2.858) \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n      \\\\\n      &= 56.467\n            + 0.692 \\left( \\frac{t}{55.57} \\right)\n            - 0.692 \\left( \\frac{96.5}{55.57} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~ + (-16.068) \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + (-23.708) \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~ + 1.264 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + (-2.858) \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)  \n      \\\\\n      &= 56.467\n            + 0.012 t\n            - 1.201 \\\\\n      & ~~~~~~~~~~~~~~~~~ + (-16.068) \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + (-23.708) \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~ + 1.264 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + (-2.858) \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n      \\\\\n      &= 55.265\n            + 0.012 t  \\\\\n      & ~~~~~~~~~~~~~~~~~ + (-16.068) \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + (-23.708) \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~ + 1.264 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + (-2.858) \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n      \\\\\n\\end{align*}\\]\n\n\n–&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;\n\n\nReduced Quadratic 5\nTo illustrate how to incorporate the quadratic term, we also fit the model called “Reduced Quadratic 5”.\n\n\nShow the code\nreduced5_quadratic_lm &lt;- weather_df |&gt;\n  model(reduced_quadratic_5  = TSLM(x ~ zTIME + I(zTIME^2) + sin1 + cos1 + sin2 + cos2))\n\nreduced5_quadratic_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 7 × 7\n  .model              term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;               &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_quadratic_5 (Intercept)   57.2       0.413    138.   1.06e-188 TRUE \n2 reduced_quadratic_5 zTIME          0.692     0.277      2.50 1.33e-  2 TRUE \n3 reduced_quadratic_5 I(zTIME^2)    -0.733     0.310     -2.37 1.90e-  2 TRUE \n4 reduced_quadratic_5 sin1         -16.1       0.390    -41.2  4.40e- 95 TRUE \n5 reduced_quadratic_5 cos1         -23.7       0.390    -60.8  3.16e-124 TRUE \n6 reduced_quadratic_5 sin2           1.26      0.390      3.24 1.40e-  3 TRUE \n7 reduced_quadratic_5 cos2          -2.86      0.390     -7.33 6.96e- 12 TRUE \n\n\nShow the code\nr5quad_coef &lt;- reduced5_quadratic_lm |&gt;\n  tidy() |&gt;\n  select(term, estimate, std.error) |&gt;\n  round_df(3)\n\n\nThe fitted model is:\n\\[\\begin{align*}\n  x_t &= \\hat \\beta_0 + \\hat \\beta_1 \\left( zTIME \\right) + \\hat \\beta_2 \\left( zTIME \\right)^2  \\\\\n      & ~~~~~~~~~~ + \\hat \\beta_3 \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + \\hat \\beta_4 \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~ + \\hat \\beta_5 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + \\hat \\beta_6 \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n      \\\\\n      &= \\hat \\beta_0 + \\hat \\beta_1 \\left( \\frac{t - \\bar t}{s_t} \\right) + \\hat \\beta_2 \\left( \\frac{t - \\bar t}{s_t} \\right)^2 \\\\\n      & ~~~~~~~~~~ + \\hat \\beta_3 \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + \\hat \\beta_4 \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~ + \\hat \\beta_5 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + \\hat \\beta_6 \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n      \\\\\n      &= 57.196\n            + 0.692 \\left( \\frac{t - 96.5}{55.57} \\right)\n            + (-0.733) \\left( \\frac{t - 96.5}{55.57} \\right)^2 \\\\\n      & ~~~~~~~~~~~~~~~~~ + (-16.067) \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + (-23.705) \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~ + 1.265 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + (-2.857) \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n      \\\\\n\\end{align*}\\]\nIf we want, we could rewrite this by expanding out the polynomial in the first three terms, but it is not necessary.\n\n\n\n\n\n\n\n\n\n\n\n\nComparison of Fitted Values\n\n\n\n\n\n\n\nComparison of Fitted Values\nThis time plot shows the original temperature data superimposed with the fitted curves based on three models.\n\n\nShow the code\nnum_months &lt;- weather_df |&gt; \n  as_tibble() |&gt; \n  dplyr::select(TIME) |&gt; \n  tail(1) |&gt; \n  pull()\n\ndf &lt;- tibble( TIME = seq(1, num_months, 0.01) ) |&gt;\n  mutate(\n    cos1 = cos(2 * pi * 1 * TIME/12),\n    cos2 = cos(2 * pi * 2 * TIME/12),\n    cos3 = cos(2 * pi * 3 * TIME/12),\n    cos4 = cos(2 * pi * 4 * TIME/12),\n    cos5 = cos(2 * pi * 5 * TIME/12),\n    cos6 = cos(2 * pi * 6 * TIME/12),\n    sin1 = sin(2 * pi * 1 * TIME/12),\n    sin2 = sin(2 * pi * 2 * TIME/12),\n    sin3 = sin(2 * pi * 3 * TIME/12),\n    sin4 = sin(2 * pi * 4 * TIME/12),\n    sin5 = sin(2 * pi * 5 * TIME/12),\n    sin6 = sin(2 * pi * 6 * TIME/12)) |&gt;\n  mutate(zTIME = (TIME - mean(TIME)) / sd(TIME)) |&gt;\n  as_tsibble(index = TIME)\n\nquad1_ts &lt;- reduced1_quadratic_lm |&gt;\n  forecast(df) |&gt;\n  as_tibble() |&gt;\n  dplyr::select(TIME, .mean) |&gt;\n  rename(value = .mean) |&gt;\n  mutate(Model = \"Quadratic 1\")\n\nquad5_ts &lt;- reduced5_quadratic_lm |&gt;\n  forecast(df) |&gt;\n  as_tibble() |&gt;\n  dplyr::select(TIME, .mean) |&gt;\n  rename(value = .mean) |&gt;\n  mutate(Model = \"Quadratic 5\")\n\nlinear5_ts &lt;- reduced5_linear_lm |&gt;\n  forecast(df) |&gt;\n  as_tibble() |&gt;\n  dplyr::select(TIME, .mean) |&gt;\n  rename(value = .mean) |&gt;\n  mutate(Model = \"Linear 5\")\n\ndata_ts &lt;- weather_df |&gt; \n  as_tibble() |&gt;\n  rename(value = x) |&gt;\n  mutate(Model = \"Data\") |&gt;\n  dplyr::select(TIME, value, Model)\n\ncombined_ts &lt;- bind_rows(data_ts, quad1_ts, quad5_ts, linear5_ts) \npoint_ts &lt;- combined_ts |&gt; filter(TIME == floor(TIME))\n\nokabe_ito_colors &lt;- c(\"#000000\", \"#E69F00\", \"#56B4E9\", \"#009E73\", \"#F0E442\", \"#0072B2\", \"#D55E00\", \"#CC79A7\")\n\ncombined_ts |&gt;\n  ggplot(aes(x = TIME, y = value, color = Model)) +\n  geom_line() +\n  geom_point(data = point_ts) +\n  labs(\n      x = \"Month Number\",\n      y = \"Temperature (Fahrenheit)\",\n      title = \"Monthly Average of Daily High Temperatures in Rexburg\",\n      subtitle = paste0(\"(\", format(weather_df$dates %&gt;% head(1), \"%b %Y\"), endash, format(weather_df$dates %&gt;% tail(1), \"%b %Y\"), \")\")\n  ) +    \n  scale_color_manual(\n    values = okabe_ito_colors[1:nrow(combined_ts |&gt; as_tibble() |&gt; select(Model) |&gt; unique())], \n    name = \"\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5),\n    legend.position = \"top\", # Position the legend at the top\n    legend.direction = \"horizontal\" # Set the legend direction to horizontal\n  )",
    "crumbs": [
      "Lesson 3",
      "Harmonic Seasonal Variables - Part 2"
    ]
  },
  {
    "objectID": "chapter_5_lesson_3.html#small-group-activity-fall-river-flow-rate-35-min",
    "href": "chapter_5_lesson_3.html#small-group-activity-fall-river-flow-rate-35-min",
    "title": "Harmonic Seasonal Variables - Part 2",
    "section": "Small-Group Activity: Fall River Flow Rate (35 min)",
    "text": "Small-Group Activity: Fall River Flow Rate (35 min)\nThe Fall River is a tributary of the Henrys Fork of the Snake River northeast of Rexburg, Idaho. The United States Geological Survey (USGS) provides data every fifteen minutes on the flow rate (in cubic feet per second, cfs) of this river.\nThis map shows the location of the monitoring station. On the left, you can see where Highway 20 intersects with Main St. in Ashton, Idaho.\n Here is a glimpse of the raw data:\n\n\n\n\n\nagency_cd\nsite_no\ndatetime\nflow\ncode\n\n\n\n\nUSGS\n13046995\n1998-10-01 00:00 MDT\n662\nA:[91]\n\n\nUSGS\n13046995\n1998-10-01 00:15 MDT\n655\nA:[91]\n\n\nUSGS\n13046995\n1998-10-01 00:30 MDT\n655\nA:[91]\n\n\nUSGS\n13046995\n1998-10-01 00:45 MDT\n662\nA:[91]\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\nUSGS\n13046995\n2023-09-30 23:00 MDT\n547\nA\n\n\nUSGS\n13046995\n2023-09-30 23:15 MDT\n547\nA\n\n\nUSGS\n13046995\n2023-09-30 23:30 MDT\n547\nA\n\n\nUSGS\n13046995\n2023-09-30 23:45 MDT\n552\nA\n\n\n\n\n\n\n\nThe 2023 water year goes from October 1, 2022 to September 30, 2023. The data starts with the 1999 water year and goes through the 2023 water year.\nWe will average the flow rates across the first and last half of each month, so there are 24 values in a year. For various reasons, there are gaps in the data, even after averaging across the first and last half of the month. We impute the missing values by linear interpolation using the zoo package. This just means that when we encounter NAs, we fit a line between the most recent observed value and the next observed value. Then, we fill all the NAs in with the value given by the liner relationship between the two points.\nHere are a few of the values, after averaging:\n\n\nShow the code\n# load necessary packages\npacman::p_load(zoo) # for linear interpolation of missing values\n\n# create the tsibble\nfallriver_ts0 &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/fallriver.parquet\") |&gt;\n  mutate(date = ymd_hm(datetime)) |&gt;\n  dplyr::select(datetime, date, flow) |&gt;\n  mutate(dates = ymd(paste0(year(date), \"/\", month(date), \"/\", day(date)))) |&gt;\n  group_by(dates) |&gt;\n  summarize(flow = mean(flow), .groups = \"keep\") |&gt;\n  ungroup() |&gt; \n  as_tsibble(index = dates) |&gt;\n  fill_gaps() |&gt; \n  read.zoo() %&gt;% \n  # impute missing values by interpolation\n  na.approx(xout = seq(start(.), end(.), \"day\")) %&gt;% \n  fortify.zoo() |&gt;\n  rename(\n    flow = \".\", \n    dates = Index\n  ) |&gt;\n  mutate(round_day = ifelse(day(dates) &lt;= 15, 1, 16)) |&gt;\n  mutate(dates2 = ymd(paste0(year(dates), \"/\", month(dates), \"/\", round_day))) |&gt;\n  group_by(dates2) |&gt;\n  summarize(flow = mean(flow)) |&gt;\n  ungroup() |&gt; \n  rename(dates = dates2) |&gt;\n  as_tsibble(index = dates, regular = FALSE)\n\n\n\n\n\n\n\ndates\nflow\n\n\n\n\n1998-10-01\n634.682\n\n\n1998-10-16\n596.352\n\n\n1998-11-01\n578.583\n\n\n1998-11-16\n590.703\n\n\n⋮\n⋮\n\n\n2023-08-01\n751.708\n\n\n2023-08-16\n634.236\n\n\n2023-09-01\n625.772\n\n\n2023-09-16\n584.168\n\n\n\n\n\n\n\n\n\nShow the code\nfallriver_ts &lt;- fallriver_ts0 |&gt;\n  # compute additional variables needed for the regression\n  mutate(TIME = 1:n()) |&gt;\n  mutate(\n    cos1 = cos(2 * pi * 1 * TIME / 24),\n    cos2 = cos(2 * pi * 2 * TIME / 24),\n    cos3 = cos(2 * pi * 3 * TIME / 24),\n    cos4 = cos(2 * pi * 4 * TIME / 24),\n    cos5 = cos(2 * pi * 5 * TIME / 24),\n    cos6 = cos(2 * pi * 6 * TIME / 24),\n    cos7 = cos(2 * pi * 7 * TIME / 24),\n    cos8 = cos(2 * pi * 8 * TIME / 24),\n    cos9 = cos(2 * pi * 9 * TIME / 24),\n    cos10 = cos(2 * pi * 10 * TIME / 24),\n    cos11 = cos(2 * pi * 11 * TIME / 24),\n    cos12 = cos(2 * pi * 12 * TIME / 24),\n    sin1 = sin(2 * pi * 1 * TIME / 24),\n    sin2 = sin(2 * pi * 2 * TIME / 24),\n    sin3 = sin(2 * pi * 3 * TIME / 24),\n    sin4 = sin(2 * pi * 4 * TIME / 24),\n    sin5 = sin(2 * pi * 5 * TIME / 24),\n    sin6 = sin(2 * pi * 6 * TIME / 24),\n    sin7 = sin(2 * pi * 7 * TIME / 24),\n    sin8 = sin(2 * pi * 8 * TIME / 24),\n    sin9 = sin(2 * pi * 9 * TIME / 24),\n    sin10 = sin(2 * pi * 10 * TIME / 24),\n    sin11 = sin(2 * pi * 11 * TIME / 24),\n    # sin12 = sin(2 * pi * 12 * TIME / 24) # zero for all integer values of t\n  ) \n\n# plot the time series\nfallriver_ts |&gt;\n  autoplot(.vars = flow) +\n  labs(\n      x = \"Date\",\n      y = \"Flow (cubic feet per second, cfs)\",\n      title = \"Fall River Flow Rate\",\n      subtitle = \"Above the Yellowstone Canal near Squirrel, Idaho\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5),\n    plot.subtitle = element_text(hjust = 0.5)\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUse the Fall River flow data to do the following.\n\nExplore various linear models for the flow of the Fall River over time.\nChoose a model to represent the data, and justify your decision.",
    "crumbs": [
      "Lesson 3",
      "Harmonic Seasonal Variables - Part 2"
    ]
  },
  {
    "objectID": "chapter_5_lesson_3.html#homework-preview-5-min",
    "href": "chapter_5_lesson_3.html#homework-preview-5-min",
    "title": "Harmonic Seasonal Variables - Part 2",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_5_3.qmd \n\n\nSmall-Group Activity\n\nHere are the coefficients for two possible models that could be fitted to the Fall River flow data.\n\n\nShow the code\nriver_full_lm &lt;- fallriver_ts |&gt;\n  model(river_full_lm  = TSLM(flow ~ TIME \n                              + sin1 + cos1 + sin2 + cos2 \n                              + sin3 + cos3 + sin4 + cos4 \n                              + sin5 + cos5 + sin6 + cos6 \n                              + sin7 + cos7 + sin8 + cos8\n                              + sin9 + cos9 + sin10 + cos10\n                              + sin11 + cos11       + cos12\n                              )\n        )\n\nriver_full_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 25 × 7\n   .model        term         estimate std.error statistic   p.value sig  \n   &lt;chr&gt;         &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 river_full_lm (Intercept)  841.       24.8       33.9   1.82e-139 TRUE \n 2 river_full_lm TIME          -0.0267    0.0715    -0.373 7.09e-  1 FALSE\n 3 river_full_lm sin1        -581.       17.5      -33.2   6.56e-136 TRUE \n 4 river_full_lm cos1        -130.       17.5       -7.46  3.30e- 13 TRUE \n 5 river_full_lm sin2         308.       17.5       17.6   6.27e- 56 TRUE \n 6 river_full_lm cos2        -307.       17.5      -17.5   1.92e- 55 TRUE \n 7 river_full_lm sin3          90.8      17.5        5.19  2.94e-  7 TRUE \n 8 river_full_lm cos3         250.       17.5       14.3   7.36e- 40 TRUE \n 9 river_full_lm sin4        -146.       17.5       -8.35  5.29e- 16 TRUE \n10 river_full_lm cos4         -25.6      17.5       -1.46  1.44e-  1 FALSE\n# ℹ 15 more rows\n\n\n\n\nShow the code\nriver_reduced1_lm &lt;- fallriver_ts |&gt;\n  model(river_reduced1_lm  = TSLM(flow ~ TIME \n                              + sin1 + cos1 + sin2 + cos2 \n                              + sin3 + cos3 + sin4 + cos4 \n                              + sin5 + cos5\n                              )\n        )\n\nriver_reduced1_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 12 × 7\n   .model            term         estimate std.error statistic   p.value sig  \n   &lt;chr&gt;             &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 river_reduced1_lm (Intercept)  841.       24.6       34.2   5.32e-142 TRUE \n 2 river_reduced1_lm TIME          -0.0264    0.0709    -0.373 7.09e-  1 FALSE\n 3 river_reduced1_lm sin1        -581.       17.4      -33.5   2.24e-138 TRUE \n 4 river_reduced1_lm cos1        -130.       17.4       -7.51  2.14e- 13 TRUE \n 5 river_reduced1_lm sin2         308.       17.4       17.8   7.47e- 57 TRUE \n 6 river_reduced1_lm cos2        -307.       17.4      -17.7   2.34e- 56 TRUE \n 7 river_reduced1_lm sin3          90.8      17.4        5.23  2.36e-  7 TRUE \n 8 river_reduced1_lm cos3         250.       17.4       14.4   1.70e- 40 TRUE \n 9 river_reduced1_lm sin4        -146.       17.4       -8.41  3.09e- 16 TRUE \n10 river_reduced1_lm cos4         -25.6      17.4       -1.47  1.41e-  1 FALSE\n11 river_reduced1_lm sin5          40.0      17.4        2.31  2.15e-  2 TRUE \n12 river_reduced1_lm cos5         -66.8      17.4       -3.85  1.33e-  4 TRUE",
    "crumbs": [
      "Lesson 3",
      "Harmonic Seasonal Variables - Part 2"
    ]
  },
  {
    "objectID": "chapter_5_lesson_5.html",
    "href": "chapter_5_lesson_5.html",
    "title": "Forecasting, Inverse Transformation, and Bias Correction",
    "section": "",
    "text": "Apply logarithmic transformations to time series\n\n\nApply a log-transformation to a multiplicative time series\n\n\n\n\nApply the bias correction factor for inverse transformations\n\n\nState the bias correction procedure for log-transform estimates \nExplain when to use the bias correction factor\nUse the bias correction factor for a log-transform model\nForecast using the inverse-transform and bias correction of a log-transformed model",
    "crumbs": [
      "Lesson 5",
      "Forecasting, Inverse Transformation, and Bias Correction"
    ]
  },
  {
    "objectID": "chapter_5_lesson_5.html#learning-outcomes",
    "href": "chapter_5_lesson_5.html#learning-outcomes",
    "title": "Forecasting, Inverse Transformation, and Bias Correction",
    "section": "",
    "text": "Apply logarithmic transformations to time series\n\n\nApply a log-transformation to a multiplicative time series\n\n\n\n\nApply the bias correction factor for inverse transformations\n\n\nState the bias correction procedure for log-transform estimates \nExplain when to use the bias correction factor\nUse the bias correction factor for a log-transform model\nForecast using the inverse-transform and bias correction of a log-transformed model",
    "crumbs": [
      "Lesson 5",
      "Forecasting, Inverse Transformation, and Bias Correction"
    ]
  },
  {
    "objectID": "chapter_5_lesson_5.html#preparation",
    "href": "chapter_5_lesson_5.html#preparation",
    "title": "Forecasting, Inverse Transformation, and Bias Correction",
    "section": "Preparation",
    "text": "Preparation\n\nRead Sections 5.9-5.11",
    "crumbs": [
      "Lesson 5",
      "Forecasting, Inverse Transformation, and Bias Correction"
    ]
  },
  {
    "objectID": "chapter_5_lesson_5.html#learning-journal-exchange-10-min",
    "href": "chapter_5_lesson_5.html#learning-journal-exchange-10-min",
    "title": "Forecasting, Inverse Transformation, and Bias Correction",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading another student’s?\nWhat would you recommend the other student add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 5",
      "Forecasting, Inverse Transformation, and Bias Correction"
    ]
  },
  {
    "objectID": "chapter_5_lesson_5.html#class-activity-anti-log-transformation-and-bias-correction-on-simulated-data-10-min",
    "href": "chapter_5_lesson_5.html#class-activity-anti-log-transformation-and-bias-correction-on-simulated-data-10-min",
    "title": "Forecasting, Inverse Transformation, and Bias Correction",
    "section": "Class Activity: Anti-Log Transformation and Bias Correction on Simulated Data (10 min)",
    "text": "Class Activity: Anti-Log Transformation and Bias Correction on Simulated Data (10 min)\n\nForecasts for a Simulated Time Series\nIn the previous lesson, we simulated a time series with an exponential trend. In this lesson, we will forecast future values based on our model.\nFigure 1 shows the simulated time series and the time series after the natural logarithm is applied.\n\n\nShow the code\nset.seed(12345)\n\nn_years &lt;- 9 # Number of years to simulate\nn_months &lt;- n_years * 12 # Number of months\nsigma &lt;- .05 # Standard deviation of random term\n\nz_t &lt;- rnorm(n = n_months, mean = 0, sd = sigma)\n\ndates_seq &lt;- seq(floor_date(now(), unit = \"year\"), length.out=n_months + 1, by=\"-1 month\") |&gt;\n  floor_date(unit = \"month\") |&gt; sort() |&gt; head(n_months)\n\nsim_ts &lt;- tibble(\n  t = 1:n_months,\n  dates = dates_seq,\n  random = arima.sim(model=list(ar=c(.5,0.2)), n = n_months, sd = 0.02),\n  x_t = exp(2 + 0.015 * t +\n              0.03 * sin(2 * pi * 1 * t / 12) + 0.04 * cos(2 * pi * 1 * t / 12) + \n              0.05 * sin(2 * pi * 2 * t / 12) + 0.03 * cos(2 * pi * 2 * t / 12) +\n              0.01 * sin(2 * pi * 3 * t / 12) + 0.005 * cos(2 * pi * 3 * t / 12) +\n              random\n  )\n) |&gt;\n  mutate(\n    cos1 = cos(2 * pi * 1 * t / 12),\n    cos2 = cos(2 * pi * 2 * t / 12),\n    cos3 = cos(2 * pi * 3 * t / 12),\n    cos4 = cos(2 * pi * 4 * t / 12),\n    cos5 = cos(2 * pi * 5 * t / 12),\n    cos6 = cos(2 * pi * 6 * t / 12),\n    sin1 = sin(2 * pi * 1 * t / 12),\n    sin2 = sin(2 * pi * 2 * t / 12),\n    sin3 = sin(2 * pi * 3 * t / 12),\n    sin4 = sin(2 * pi * 4 * t / 12),\n    sin5 = sin(2 * pi * 5 * t / 12),\n    sin6 = sin(2 * pi * 6 * t / 12)) |&gt;\n  mutate(std_t = (t - mean(t)) / sd(t)) |&gt;\n  as_tsibble(index = dates)\n\nsim_plot_raw &lt;- sim_ts |&gt;\n  autoplot(.vars = x_t) +\n  labs(\n    x = \"Month\",\n    y = \"x_t\",\n    title = \"Simulated Time Series\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n\nsim_plot_log &lt;- sim_ts |&gt;\n  autoplot(.vars = log(x_t)) +\n  labs(\n    x = \"Month\",\n    y = \"log(x_t)\",\n    title = \"Logarithm of Simulated Time Series\"\n  ) +\n  theme_minimal() +\n  theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n\nsim_plot_raw | sim_plot_log\n\n\n\n\n\n\n\n\nFigure 1: Time plot of the time series (left) and the natural logarithm of the time series (right)\n\n\n\n\n\nWe can use the forecast() function to predict future values of this time series. Table 1 displays the output of the forecast() command. Note that the column labeled x_t (i.e. \\(x_t\\)), representing the time series is populated with information tied to a normal distribution. The mean and standard deviation specified are the estimated parameters for the distribution of the predicted values of \\(\\log(x_t)\\). If you raise \\(e\\) to the power of the mean, you get the values in the .mean column.\n\n\nShow the code\n# Fit model (OLS)\nsim_reduced_linear_lm1 &lt;- sim_ts |&gt;\n  model(sim_reduced_linear1 = TSLM(log(x_t) ~ std_t + \n        sin1 + cos1 + sin2 + cos2 + sin3 + cos3))\n\n# Compute forecast\nn_years_forecast &lt;- 5\nn_months_forecast &lt;- 12 * n_years_forecast\n\nnew_dat &lt;- tibble(t = n_months:(n_months + n_months_forecast )) |&gt;\n  mutate(\n    dates = seq(max(dates_seq), length.out=n_months_forecast + 1, by=\"1 month\") \n  ) |&gt;\n  mutate(\n    std_t = (t - mean(pull(sim_ts, t))) / sd(pull(sim_ts, t)),\n    sin1 = sin(2 * pi * 1 * t / 12),\n    cos1 = cos(2 * pi * 1 * t / 12),\n    sin2 = sin(2 * pi * 2 * t / 12),\n    cos2 = cos(2 * pi * 2 * t / 12),\n    sin3 = sin(2 * pi * 3 * t / 12),\n    cos3 = cos(2 * pi * 3 * t / 12),\n    sin4 = sin(2 * pi * 4 * t / 12),\n    cos4 = cos(2 * pi * 4 * t / 12),\n    sin5 = sin(2 * pi * 5 * t / 12),\n    cos5 = cos(2 * pi * 5 * t / 12),\n    cos6 = cos(2 * pi * 6 * t / 12)\n  ) |&gt;\n  as_tsibble(index = dates)\n\nsim_reduced_linear_lm1 |&gt; \n  forecast(new_data = new_dat)\n\n\n\n\n\n\nTable 1: Output of the forecast() command for the simulated time series\n\n\n\n\n\n\n.model\ndates\nx_t\n.mean\nt\nstd_t\nsin1\ncos1\nsin2\ncos2\nsin3\ncos3\n\n\n\n\n\nsim_reduced_linear1\n2024-12-01\nt(N(3.7, 0.00055))\n40.714\n108\n1.708\n0\n1\n0\n1\n0\n1\n...\n\n\nsim_reduced_linear1\n2025-01-01\nt(N(3.8, 0.00056))\n43.133\n109\n1.74\n0.5\n0.866\n0.866\n0.5\n1\n0\n...\n\n\nsim_reduced_linear1\n2025-02-01\nt(N(3.7, 0.00056))\n41.625\n110\n1.772\n0.866\n0.5\n0.866\n-0.5\n0\n-1\n...\n\n\nsim_reduced_linear1\n2025-03-01\nt(N(3.7, 0.00056))\n39.159\n111\n1.804\n1\n0\n0\n-1\n-1\n0\n...\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\nsim_reduced_linear1\n2029-11-01\nt(N(4.5, 6e-04))\n90.307\n167\n3.592\n-0.5\n0.866\n-0.866\n0.5\n-1\n0\n...\n\n\nsim_reduced_linear1\n2029-12-01\nt(N(4.6, 6e-04))\n101.132\n168\n3.624\n0\n1\n0\n1\n0\n1\n...\n\n\n\n\n\n\n\n\n\n\nFigure 2 illustrates the forecasted values for the time series.\n\n\nShow the code\nsim_forecast_plot_regular &lt;- sim_reduced_linear_lm1 |&gt; \n  forecast(new_data = new_dat) |&gt;\n  autoplot(sim_ts, level = 95) +\n  labs(\n    x = \"Month\",\n    y = \"x_t\",\n    title = \"Simulated Time Series\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"inset\") +\n  theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n\nsim_forecast_plot_logged &lt;- sim_reduced_linear_lm1 |&gt; \n  forecast(new_data = new_dat) |&gt;\n  autoplot(sim_ts, level = 95) +\n  scale_y_continuous(trans = \"log\", labels = trans_format(\"log\")) +\n  labs(\n    x = \"Month\",\n    y = \"log(x_t)\",\n    title = \"Logarithm of Simulated Time Series\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"inset\")  +\n  theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n\nsim_forecast_plot_regular\n\n\n\n\n\n\n\n\nFigure 2: Forecasted values of the time series with 95% confidence bands\n\n\n\n\n\n\n\nBias Correction\nThe forecasts presented above were computed by raising \\(e\\) to the power of the predicted log-values. Unfortunately, this introduces bias in the forecasted means. This bias tends to be large if the regression model does not fit the data closely.\nThe textbook points out that the bias correction should only be applied for means, not for simulated values. This means that if you are simulating transformed values, and you apply the inverse of your original transformation, the resulting values are appropriate.\nWhen we apply the inverse transform to the residual series, we introduce a bias.  We can account for this bias applying one of two adjustments to our mean values. The theory behind this transformations is alluded to in the textbook, but is not essential.\nThere are two common patterns observed in the residual series: (1) Gaussian white noise or (2) Negatively-skewed values. Note that negatively-skewed values are the same as left-skewed values.\nWe can use the skewness statistic to assess the shape of the residual series. When the skewness is less than -1 or greater than 1, we say that the distribution is highly skewed. For skewness values between -1 and -0.5 or between 0.5 and 1, we say there is moderate skewness. If skewness lies between -0.5 and 0.5, the distribution is considered roughly symmetric.\n\n\n\n\nLog-Normal Correction\n\nNormally-Distributed Residual Series\nIf the residual series follows a normal distribution, we multiply the means of the forecasted values \\(\\hat x_t\\) by the factor \\(e^{\\frac{1}{2} \\sigma^2}\\):\n\\[\n  \\hat x_t' = e^{\\frac{1}{2} \\sigma^2} \\cdot \\hat x_t\n\\]\nwhere \\(\\left\\{ \\hat x_t: t = 1, \\ldots, n \\right\\}\\) gives the values of the forecasted series, and \\(\\left\\{ \\hat x_t': t = 1, \\ldots, n \\right\\}\\) is the adjusted forecasted values.\n\n\n\n\n\n\nEmperical Correction\n\nNegatively-Skewed Residual Series\nIf the residual series demonstrates negative skewness, then we can adjust the forecasts \\(\\left\\{ \\hat x_t \\right\\}\\) as follows:\n\\[\n  \\hat x_t' = e^{\\widehat{\\log x_t}} \\sum_{t=1}^{n} \\frac{e^{z_t}}{n}\n\\]\nwhere \\(\\left\\{ \\widehat{\\log x_t}: t = 1, \\ldots, n \\right\\}\\) is the forecasted series obtained by fitting the log-regression model.\n\\(\\left\\{ z_t \\right\\}\\) is the residual series from this fitted model in the log-transformed units.\n\n\n\n\n\nThe code given below can be used to compute the corrected mean values for the simulated data.\n\n\nShow the code\nsim_model_values &lt;- sim_reduced_linear_lm1 |&gt;\n  glance()\n\nsim_model_check &lt;- sim_model_values |&gt;\n  mutate(\n    sigma = sqrt(sigma2),\n    lognorm_cf = exp((1/2) * sigma2),\n    empirical_cf = sim_reduced_linear_lm1 |&gt;\n      residuals() |&gt;\n      pull(.resid) |&gt;\n      exp() |&gt;\n      mean()) |&gt;\n  select(.model, r_squared, sigma2, sigma, lognorm_cf, empirical_cf)\n\nsim_pred &lt;- sim_reduced_linear_lm1 |&gt; \n  forecast(new_data = new_dat) |&gt;\n  mutate(.mean_correction = .mean * sim_model_check$empirical_cf) |&gt;\n  select(t, x_t, .mean, .mean_correction)\n\nsim_model_check\n\n\n# A tibble: 1 × 6\n  .model              r_squared   sigma2  sigma lognorm_cf empirical_cf\n  &lt;chr&gt;                   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n1 sim_reduced_linear1     0.998 0.000507 0.0225       1.00         1.00\n\n\nFrom this, we observe that for the simulated data, \\(R^2 = 0.998\\). This indicates that the model explains a high proportion of the variation in the data. The log-normal adjustment is \\(1.00025\\), and the emperical adjustment is \\(1.00023\\). Both of these values are extremely close to 1, so they will have a negligible impact on the predicted values.\nThis result does not generalize. In other situations, there can be a substantial effect of this bias on the predicted means.\n\n\nHistogram of residuals\nFigure 3 gives a histogram of the residuals and compute the skewness of the residual series.\n\n\nShow the code\nsim_resid_df &lt;- sim_reduced_linear_lm1 |&gt; \n  residuals() |&gt; \n  as_tibble() |&gt; \n  dplyr::select(.resid) |&gt;\n  rename(x = .resid) \n  \nsim_resid_df |&gt;\n  mutate(density = dnorm(x, mean(sim_resid_df$x), sd(sim_resid_df$x))) |&gt;\n  ggplot(aes(x = x)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"#56B4E9\", binwidth = 0.01) +\n    geom_line(aes(x = x, y = density)) +\n    theme_bw() +\n    labs(\n      x = \"Values\",\n      y = \"Frequency\",\n      title = \"Histogram of Residuals from the Reduced Linear Model\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\nFigure 3: Histogram of the values in the residual series based on the model with a linear trend and seasonal Fourier terms where i≤3\n\n\n\n\n\nWe can use the command skewness(sim_resid_df$x) to compute the skewness of these residuals: -0.135. This number is close to zero (specifically between -0.5 and 0.5,) so we conclude that the residual series is approximately normally distributed. We can apply the log-normal correction to our mean forecast values.",
    "crumbs": [
      "Lesson 5",
      "Forecasting, Inverse Transformation, and Bias Correction"
    ]
  },
  {
    "objectID": "chapter_5_lesson_5.html#class-activity-apple-revenue-10-min",
    "href": "chapter_5_lesson_5.html#class-activity-apple-revenue-10-min",
    "title": "Forecasting, Inverse Transformation, and Bias Correction",
    "section": "Class Activity: Apple Revenue (10 min)",
    "text": "Class Activity: Apple Revenue (10 min)\nWe take another look at the quarterly revenue reported by Apple Inc. from Q1 of 2005 through Q1 of 2012\n\nVisualizing the Time Series\nFigure 4 gives the time plot illustrating the quarterly revenue reported by Apple from the first quarter of 2005 through the first quarter of 2012.\n\n\nShow the code\napple_raw &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/apple_revenue.csv\") |&gt;\n  mutate(dates = round_date(mdy(date), unit = \"quarter\")) |&gt;\n  arrange(dates)\n\napple_ts &lt;- apple_raw |&gt;\n  filter(dates &lt;= my(\"Jan 2012\")) |&gt;\n  dplyr::select(dates, revenue_billions) |&gt;\n  mutate(t = 1:n()) |&gt;\n  mutate(std_t = (t - mean(t)) / sd(t)) |&gt;\n  mutate(\n    sin1 = sin(2 * pi * 1 * t / 4),\n    cos1 = cos(2 * pi * 1 * t / 4),\n    cos2 = cos(2 * pi * 2 * t / 4)\n  ) |&gt;\n  as_tsibble(index = dates)\n\napple_plot_regular &lt;- apple_ts |&gt;\n  autoplot(.vars = revenue_billions) +\n  stat_smooth(method = \"lm\", \n              formula = y ~ x, \n              geom = \"smooth\",\n              se = FALSE,\n              color = \"#E69F00\",\n              linetype = \"dotted\") +\n    labs(\n      x = \"Quarter\",\n      y = \"Revenue (Billions USD)\",\n      title = \"Apple Revenue (in Billions USD)\"\n    ) +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5))\n\napple_plot_transformed &lt;- apple_ts |&gt;\n  autoplot(.vars = log(revenue_billions)) +\n  stat_smooth(method = \"lm\", \n              formula = y ~ x, \n              geom = \"smooth\",\n              se = FALSE,\n              color = \"#E69F00\",\n              linetype = \"dotted\") +\n    labs(\n      x = \"Quarter\",\n      y = \"Logarithm of Revenue\",\n      title = \"Logarithm of Apple Revenue\"\n    ) +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5))\n\napple_plot_regular | apple_plot_transformed\n\n\n\n\n\n\n\n\nFigure 4: Apple quarterly revenue figures (in billions of U.S. dollars) from Q1 of 2005 to Q1 of 2012; the figure on the left presents the revenue in dollars and the figure on the right gives the logarithm of the quarterly revenue; a simple linear regression line is given for reference\n\n\n\n\n\n\n\nFinding a Suitable Model\nWe start by fitting a cubic trend to the logarithm of the quarterly revenues. The full model is fitted here:\n\n\nShow the code\n# Cubic model with standardized time variable\n\napple_full_cubic_lm &lt;- apple_ts |&gt;\n  model(apple_full_cubic = TSLM(log(revenue_billions) ~ std_t + I(std_t^2) + I(std_t^3) +\n    sin1 + cos1 + cos2 )) # Note sin2 is omitted\napple_full_cubic_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 7 × 7\n  .model           term        estimate std.error statistic  p.value sig  \n  &lt;chr&gt;            &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;\n1 apple_full_cubic (Intercept)   1.83      0.0572    32.1   5.67e-20 TRUE \n2 apple_full_cubic std_t         1.01      0.0972    10.4   6.41e-10 TRUE \n3 apple_full_cubic I(std_t^2)   -0.0158    0.0445    -0.355 7.26e- 1 FALSE\n4 apple_full_cubic I(std_t^3)    0.0866    0.0516     1.68  1.07e- 1 FALSE\n5 apple_full_cubic sin1          0.217     0.0533     4.08  4.96e- 4 TRUE \n6 apple_full_cubic cos1          0.0839    0.0552     1.52  1.43e- 1 FALSE\n7 apple_full_cubic cos2         -0.0981    0.0382    -2.57  1.74e- 2 TRUE \n\n\nThe quadratic and cubic trend terms are not statistically signficant. We now eliminate the cubic term and fit a full model with a quadratic trend.\n\n\nShow the code\napple_full_quad_lm &lt;- apple_ts |&gt;\n  model(apple_full_quad = TSLM(log(revenue_billions) ~ std_t + I(std_t^2) + \n    sin1 + cos1 + cos2 )) # Note sin2 is omitted\napple_full_quad_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 6 × 7\n  .model          term        estimate std.error statistic  p.value sig  \n  &lt;chr&gt;           &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;\n1 apple_full_quad (Intercept)   1.83      0.0594    30.9   3.12e-20 TRUE \n2 apple_full_quad std_t         1.16      0.0403    28.7   1.64e-19 TRUE \n3 apple_full_quad I(std_t^2)   -0.0158    0.0462    -0.341 7.36e- 1 FALSE\n4 apple_full_quad sin1          0.217     0.0554     3.93  6.73e- 4 TRUE \n5 apple_full_quad cos1          0.0934    0.0570     1.64  1.15e- 1 FALSE\n6 apple_full_quad cos2         -0.0981    0.0396    -2.48  2.11e- 2 TRUE \n\n\nThe quadratic trend term is not statistically significant. Nevertheless, we will still fit a reduced model with a quadratic trend but we will omit the non-signficant seasonal Fourier term, cos1.\n\n\nShow the code\n# Quadratic trend with standardized time variable\n\napple_reduced_quad_lm1 &lt;- apple_ts |&gt;\n  model(apple_reduced_quad1 = TSLM(log(revenue_billions) ~ std_t + I(std_t^2) +\n    sin1 + cos2 )) # Note sin2 is omitted\napple_reduced_quad_lm1 |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 5 × 7\n  .model              term        estimate std.error statistic  p.value sig  \n  &lt;chr&gt;               &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;\n1 apple_reduced_quad1 (Intercept)   1.83      0.0614    29.9   1.71e-20 TRUE \n2 apple_reduced_quad1 std_t         1.16      0.0415    28.0   7.94e-20 TRUE \n3 apple_reduced_quad1 I(std_t^2)   -0.0158    0.0478    -0.330 7.44e- 1 FALSE\n4 apple_reduced_quad1 sin1          0.217     0.0573     3.80  8.79e- 4 TRUE \n5 apple_reduced_quad1 cos2         -0.0981    0.0410    -2.39  2.49e- 2 TRUE \n\n\nThe quadratic trend term is still not statistically significant. We will fit a full model with a linear trend.\n\n\nShow the code\n# Linear trend with standardized time variable\n\napple_full_linear_lm &lt;- apple_ts |&gt;\n  model(apple_full_linear = TSLM(log(revenue_billions) ~ std_t + \n    sin1 + cos1 + cos2 )) # Note sin2 is omitted\napple_full_linear_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 5 × 7\n  .model            term        estimate std.error statistic  p.value sig  \n  &lt;chr&gt;             &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;\n1 apple_full_linear (Intercept)   1.82      0.0388     46.9  4.04e-25 TRUE \n2 apple_full_linear std_t         1.16      0.0396     29.2  2.81e-20 TRUE \n3 apple_full_linear sin1          0.215     0.0540      3.99 5.42e- 4 TRUE \n4 apple_full_linear cos1          0.0934    0.0559      1.67 1.08e- 1 FALSE\n5 apple_full_linear cos2         -0.0972    0.0388     -2.50 1.95e- 2 TRUE \n\n\nThe coefficient on the cos1 seasonal Fourier term is not statistically significant. We now fit a reduced model that only contains the significant terms from the full model with a linear trend.\n\n\nShow the code\n# Linear trend with standardized time variable\n\napple_reduced_linear_lm1 &lt;- apple_ts |&gt;\n  model(apple_reduced_linear1 = TSLM(log(revenue_billions) ~ std_t + \n    sin1 + cos2 )) # Note sin2 is omitted\napple_reduced_linear_lm1 |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 4 × 7\n  .model                term        estimate std.error statistic  p.value sig  \n  &lt;chr&gt;                 &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;\n1 apple_reduced_linear1 (Intercept)   1.82      0.0402     45.3  1.60e-25 TRUE \n2 apple_reduced_linear1 std_t         1.16      0.0408     28.5  1.42e-20 TRUE \n3 apple_reduced_linear1 sin1          0.215     0.0559      3.85 7.21e- 4 TRUE \n4 apple_reduced_linear1 cos2         -0.0972    0.0402     -2.42 2.32e- 2 TRUE \n\n\nAll the terms are statistically significant in this model. We now compare the models we have fitted using the AIC, AICc, and BIC criterion.\n\n\nShow the code\nmodel_combined &lt;- apple_ts |&gt;\n  model(\n    apple_full_cubic = TSLM(log(revenue_billions) ~ std_t + I(std_t^2) + I(std_t^3) +\n                    sin1 + cos1 + cos2),\n    apple_full_quad = TSLM(log(revenue_billions) ~ std_t + I(std_t^2) + \n                    sin1 + cos1 + cos2),\n    apple_reduced_quad1 = TSLM(log(revenue_billions) ~ std_t + I(std_t^2) + \n                    sin1 + cos2),\n    apple_full_linear = TSLM(log(revenue_billions) ~ std_t +\n                    sin1 + cos1 + cos2),\n    apple_reduced_linear1 = TSLM(log(revenue_billions) ~ std_t + \n                    sin1 + cos2 )\n  )\n\nglance(model_combined) |&gt;\n  select(.model, AIC, AICc, BIC)\n\n\n\n\n\n\nTable 2: Comparison of the AIC, AICc, and BIC values for the models fitted to the logarithm of the simulated time series.\n\n\n\n\n\n\nModel\nAIC\nAICc\nBIC\n\n\n\n\napple_full_cubic\n-84\n-76.8\n-73.1\n\n\napple_full_quad\n-82.5\n-77.2\n-73\n\n\napple_reduced_quad1\n-81.3\n-77.5\n-73.1\n\n\napple_full_linear\n**-84.4**\n-80.6\n-76.2\n\n\napple_reduced_linear1\n-83.2\n**-80.6**\n**-76.4**\n\n\n\n\n\n\n\n\n\n\nWe will apply the apple_reduced_linear1 model.\n\n\nUsing the Residuals to Determine the Appropriate Correction\nThe residuals of this model are illustrated in Figure 5.\n\n\nShow the code\napple_resid_df &lt;- model_combined |&gt; \n  dplyr::select(apple_reduced_linear1) |&gt;\n  residuals() |&gt; \n  as_tibble() |&gt; \n  dplyr::select(.resid) |&gt;\n  rename(x = .resid) \n  \napple_resid_df |&gt;\n  mutate(density = dnorm(x, mean(apple_resid_df$x), sd(apple_resid_df$x))) |&gt;\n  ggplot(aes(x = x)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"#56B4E9\", binwidth = 0.05) +\n    geom_line(aes(x = x, y = density)) +\n    theme_bw() +\n    labs(\n      x = \"Values\",\n      y = \"Frequency\",\n      title = \"Histogram of Residuals from the Reduced Model with a Linear Trend\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\nFigure 5: Histogram of the residuals from the reduced model with a linear trend component\n\n\n\n\n\nUsing the command skewness(apple_resid_df$x), we compute the skewness of these residuals as: -0.799. This number is not close to zero (it is between -1 and -0.5) indicating moderate negative skewness. We would therefore apply the empirical correction to our mean forecast values.\n\n\nApplying the Correction Factor\nTable 3 summarizes some of the corrected mean values. Note that in this particular case, the corrected values are very close to the uncorrected values.\n\n\nShow the code\napple_model_values &lt;- model_combined |&gt; \n  dplyr::select(apple_reduced_linear1) |&gt;\n  glance()\n\napple_model_check &lt;- apple_model_values |&gt;\n  mutate(\n    sigma = sqrt(sigma2),\n    lognorm_cf = exp((1/2) * sigma2),\n    empirical_cf = apple_reduced_linear_lm1 |&gt;\n      residuals() |&gt;\n      pull(.resid) |&gt;\n      exp() |&gt;\n      mean()) |&gt;\n  select(.model, r_squared, sigma2, sigma, lognorm_cf, empirical_cf)\n\napple_pred &lt;- model_combined |&gt; \n  dplyr::select(apple_reduced_linear1) |&gt;\n  forecast(new_data = apple_ts) |&gt;\n  mutate(.mean_correction = .mean * apple_model_check$empirical_cf) |&gt;\n  select(t, revenue_billions, .mean, .mean_correction)\n\napple_model_check\n\n\n# A tibble: 1 × 6\n  .model                r_squared sigma2 sigma lognorm_cf empirical_cf\n  &lt;chr&gt;                     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n1 apple_reduced_linear1     0.971 0.0466 0.216       1.02         1.02\n\n\n\n\nShow the code\napple_pred &lt;- model_combined |&gt; \n  dplyr::select(apple_reduced_linear1) |&gt;\n  forecast(new_data = apple_ts) |&gt;\n  mutate(.mean_correction = .mean * apple_model_check$empirical_cf) |&gt;\n  select(t, revenue_billions, .mean, .mean_correction)\n\n\n\n\n\n\nTable 3: Fitted values for the model representing Apple’s quarterly revenue\n\n\n\n\n\n\nt\nrevenue_billions\n.mean\n.mean_correction\ndates\n\n\n\n\n1\nt(N(0.22, 0.057))\n1.284\n1.309\n2005-01-01\n\n\n2\nt(N(-0.051, 0.054))\n0.975\n0.994\n2005-04-01\n\n\n3\nt(N(0.064, 0.057))\n1.096\n1.118\n2005-07-01\n\n\n4\nt(N(0.22, 0.053))\n1.281\n1.306\n2005-10-01\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n28\nt(N(3.5, 0.054))\n33.885\n34.542\n2011-10-01\n\n\n29\nt(N(4, 0.057))\n58.588\n59.725\n2012-01-01\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting the Fitted Values\nThese fitted values are illustrated in Figure 6.\n\n\nShow the code\napple_ts |&gt;\n  autoplot(.vars = revenue_billions) +\n  geom_line(data = apple_pred, aes(x = dates, y = .mean_correction), color = \"#56B4E9\") +\n    labs(\n      x = \"Quarter\",\n      y = \"Revenue (Billions USD)\",\n      title = \"Apple Revenue in Billions of U.S. Dollars\"\n    ) +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFigure 6: Apple Inc.’s quarterly revenue in billions of U.S. dollars through first quarter of 2012 (in black) and the fitted regression model (in blue)\n\n\n\n\n\nThis time series was used as an example. We are obviously not interested in forecasting future values using this model. However, this is an excellent example of real-world exponential growth in a time series with a seasonal component. Limiting factors prevent exponential growth from being sustainable in the long run. After 2012, the Apple quarterly revenues follow a different, but very impressive, model. This is illustrated in Figure 7.\n\n\nShow the code\napple_raw |&gt;\n  dplyr::select(dates, revenue_billions) |&gt;\n  as_tsibble(index = dates) |&gt;\n  autoplot(.vars = revenue_billions) +\n  geom_line(\n    data = apple_raw |&gt; filter(dates &gt;= my(\"Jan 2012\")), \n    aes(x = dates, y = revenue_billions), \n    color = \"#D55E00\"\n  ) +\n  labs(\n    x = \"Quarter\",\n    y = \"Revenue (Billions USD)\",\n    title = \"Apple Revenue (in Billions USD)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFigure 7: Apple Inc.’s quarterly revenue in billions of U.S. dollars; values beginning with the first quarter of 2012 are shown in orange",
    "crumbs": [
      "Lesson 5",
      "Forecasting, Inverse Transformation, and Bias Correction"
    ]
  },
  {
    "objectID": "chapter_5_lesson_5.html#choose-one-of-the-following-small-group-activities-25-min",
    "href": "chapter_5_lesson_5.html#choose-one-of-the-following-small-group-activities-25-min",
    "title": "Forecasting, Inverse Transformation, and Bias Correction",
    "section": "Choose One of the Following Small-Group Activities (25 min)",
    "text": "Choose One of the Following Small-Group Activities (25 min)\n\nSmall-Group Activity: Retail Sales (All Other General Merchandise Stores)\nThe code below downloads and gives the time plot for the total monthly sales in the United States for retail stores with the NAICS category 45299, “All Other General Merchandise Stores.” The time plot is given in Figure Figure 8.\n\n\nShow the code\n# Read in retail sales data for \"all other general merchandise stores\"\nretail_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/retail_by_business_type.parquet\") |&gt;\n  filter(naics == 45299) |&gt;\n  filter(as_date(month) &gt;= my(\"Jan 1998\")) |&gt;\n  mutate(t = 1:n()) |&gt;\n  mutate(std_t = (t - mean(t)) / sd(t)) |&gt;\n  mutate(\n    cos1 = cos(2 * pi * 1 * t / 12),\n    cos2 = cos(2 * pi * 2 * t / 12),\n    cos3 = cos(2 * pi * 3 * t / 12),\n    cos4 = cos(2 * pi * 4 * t / 12),\n    cos5 = cos(2 * pi * 5 * t / 12),\n    cos6 = cos(2 * pi * 6 * t / 12),\n    sin1 = sin(2 * pi * 1 * t / 12),\n    sin2 = sin(2 * pi * 2 * t / 12),\n    sin3 = sin(2 * pi * 3 * t / 12),\n    sin4 = sin(2 * pi * 4 * t / 12),\n    sin5 = sin(2 * pi * 5 * t / 12)\n  ) |&gt;\n  as_tsibble(index = month)\n\nretail_ts |&gt;\n  autoplot(.vars = sales_millions) +\n  stat_smooth(method = \"lm\", \n              formula = y ~ x, \n              geom = \"smooth\",\n              se = FALSE,\n              color = \"#E69F00\",\n              linetype = \"dotted\") +\n    labs(\n      x = \"Month\",\n      y = \"Sales (Millions of U.S. Dollars)\",\n      title = paste0(retail_ts$business[1], \" (\", retail_ts$naics[1], \")\")\n    ) +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFigure 8: Time plot of the total monthly retail sales for all other general merchandise stores (45299)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUse the retail sales data to do the following.\n\nSelect an appropriate fitted model using the AIC, AICc, or BIC critera.\nUse the residuals to determine the appropriate correction for the data.\nForecast the data for the next 5 years.\nApply the appropriate correction to the forecasted values.\nPlot the fitted (forecasted) values along with the time series.\n\n\n\n\n\nSmall-Group Activity: Industrial Electricity Consumption in Texas\nThese data represent the amount of electricity used each month for industrial applications in Texas.\n\n\nShow the code\nelec_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/electricity_tx.csv\") |&gt;\n  dplyr::select(-comments) |&gt;\n  mutate(month = my(month)) |&gt;\n  mutate(\n    t = 1:n(),\n    std_t = (t - mean(t)) / sd(t)\n  ) |&gt;\n  mutate(\n    cos1 = cos(2 * pi * 1 * t / 12),\n    cos2 = cos(2 * pi * 2 * t / 12),\n    cos3 = cos(2 * pi * 3 * t / 12),\n    cos4 = cos(2 * pi * 4 * t / 12),\n    cos5 = cos(2 * pi * 5 * t / 12),\n    cos6 = cos(2 * pi * 6 * t / 12),\n    sin1 = sin(2 * pi * 1 * t / 12),\n    sin2 = sin(2 * pi * 2 * t / 12),\n    sin3 = sin(2 * pi * 3 * t / 12),\n    sin4 = sin(2 * pi * 4 * t / 12),\n    sin5 = sin(2 * pi * 5 * t / 12)\n  ) |&gt;\n  as_tsibble(index = month)\n\nelec_plot_raw &lt;- elec_ts |&gt;\n    autoplot(.vars = megawatthours) +\n    labs(\n      x = \"Month\",\n      y = \"Megawatt-hours\",\n      title = \"Texas' Industrial Electricity Use\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\nelec_plot_log &lt;- elec_ts |&gt;\n    autoplot(.vars = log(megawatthours)) +\n    labs(\n      x = \"Month\",\n      y = \"log(Megwatt-hours)\",\n      title = \"Log of Texas' Industrial Electricity Use\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\nelec_plot_raw | elec_plot_log\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUse the Texas industrial electricity consumption data to do the following.\n\nSelect an appropriate fitted model using the AIC, AICc, or BIC critera.\nUse the residuals to determine the appropriate correction for the data.\nForecast the data for the next 5 years.\nApply the appropriate correction to the forecasted values.\nPlot the fitted (forecasted) values along with the time series.",
    "crumbs": [
      "Lesson 5",
      "Forecasting, Inverse Transformation, and Bias Correction"
    ]
  },
  {
    "objectID": "chapter_5_lesson_5.html#homework-preview-5-min",
    "href": "chapter_5_lesson_5.html#homework-preview-5-min",
    "title": "Forecasting, Inverse Transformation, and Bias Correction",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_5_5.qmd \n\n\nSmall-Group Activity: Retail Sales\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUse the retail sales data to do the following.\n\nSelect an appropriate fitted model using the AIC, AICc, or BIC critera.\nUse the residuals to determine the appropriate correction for the data.\nForecast the data for the next 5 years.\nApply the appropriate correction to the forecasted values.\nPlot the fitted (forecasted) values along with the time series.\n\n\n\n\n\nShow the code\nretail_full_quad_lm &lt;- retail_ts |&gt;\n  model(retail_full_quad = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + \n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n    + sin4 + cos4 + sin5 + cos5 + cos6 )) # Note sin6 is omitted\nretail_full_quad_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 14 × 7\n   .model           term        estimate std.error statistic   p.value sig  \n   &lt;chr&gt;            &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 retail_full_quad (Intercept)   8.37     0.00352   2378.   0         TRUE \n 2 retail_full_quad std_t         0.369    0.00235    157.   2.37e-279 TRUE \n 3 retail_full_quad I(std_t^2)    0.0130   0.00263      4.94 1.30e-  6 TRUE \n 4 retail_full_quad sin1         -0.0231   0.00332     -6.96 2.31e- 11 TRUE \n 5 retail_full_quad cos1          0.0353   0.00332     10.7  1.54e- 22 TRUE \n 6 retail_full_quad sin2         -0.0692   0.00332    -20.9  2.28e- 59 TRUE \n 7 retail_full_quad cos2          0.0785   0.00332     23.7  2.33e- 69 TRUE \n 8 retail_full_quad sin3         -0.0434   0.00332    -13.1  5.77e- 31 TRUE \n 9 retail_full_quad cos3          0.0699   0.00332     21.1  3.96e- 60 TRUE \n10 retail_full_quad sin4         -0.0288   0.00332     -8.69 2.83e- 16 TRUE \n11 retail_full_quad cos4          0.0627   0.00332     18.9  2.84e- 52 TRUE \n12 retail_full_quad sin5          0.0118   0.00332      3.57 4.25e-  4 TRUE \n13 retail_full_quad cos5          0.0636   0.00332     19.2  2.81e- 53 TRUE \n14 retail_full_quad cos6          0.0251   0.00235     10.7  1.04e- 22 TRUE \n\n\nShow the code\nretail_resid_df &lt;- retail_full_quad_lm |&gt; \n  residuals() |&gt; \n  as_tibble() |&gt; \n  dplyr::select(.resid) |&gt;\n  rename(x = .resid) \n  \nretail_resid_df |&gt;\n  mutate(density = dnorm(x, mean(retail_resid_df$x), sd(retail_resid_df$x))) |&gt;\n  ggplot(aes(x = x)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"#56B4E9\", binwidth = 0.02) +\n    geom_line(aes(x = x, y = density)) +\n    theme_bw() +\n    labs(\n      x = \"Values\",\n      y = \"Frequency\",\n      title = \"Histogram of Residuals from the Full Quadratic Model\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nShow the code\nskewness(retail_resid_df$x)\n\n\n[1] 0.2205956\n\n\nThere is little skewness. We will use the log-normal correction factor.\n\nSmall-Group Activity: Texas Industrial Electricity Usage\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUse the Texas industrial electricity consumption data to do the following.\n\nSelect an appropriate fitted model using the AIC, AICc, or BIC critera.\nUse the residuals to determine the appropriate correction for the data.\nForecast the data for the next 5 years.\nApply the appropriate correction to the forecasted values.\nPlot the fitted (forecasted) values along with the time series.\n\n\n\n\n\nShow the code\n# Cubic model with standardized time variable\n\nelec_full_cubic_lm &lt;- elec_ts |&gt;\n  model(elec_full_cubic = TSLM(log(megawatthours) ~ std_t + I(std_t^2) + I(std_t^3) +\n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 + sin4 + cos4 + sin5 + cos5 + cos6)) # Note sin6 is omitted\nelec_full_cubic_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 15 × 7\n   .model          term          estimate std.error statistic  p.value sig  \n   &lt;chr&gt;           &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;\n 1 elec_full_cubic (Intercept) 16.1         0.00684 2349.     0        TRUE \n 2 elec_full_cubic std_t        0.124       0.0115    10.8    1.19e-20 TRUE \n 3 elec_full_cubic I(std_t^2)   0.0313      0.00513    6.10   8.50e- 9 TRUE \n 4 elec_full_cubic I(std_t^3)   0.0105      0.00589    1.79   7.55e- 2 FALSE\n 5 elec_full_cubic sin1        -0.0541      0.00648   -8.36   3.68e-14 TRUE \n 6 elec_full_cubic cos1        -0.0468      0.00645   -7.25   1.89e-11 TRUE \n 7 elec_full_cubic sin2        -0.0000864   0.00646   -0.0134 9.89e- 1 FALSE\n 8 elec_full_cubic cos2        -0.000831    0.00645   -0.129  8.98e- 1 FALSE\n 9 elec_full_cubic sin3         0.00103     0.00645    0.160  8.73e- 1 FALSE\n10 elec_full_cubic cos3         0.0131      0.00645    2.03   4.46e- 2 TRUE \n11 elec_full_cubic sin4         0.0121      0.00645    1.87   6.29e- 2 FALSE\n12 elec_full_cubic cos4         0.00272     0.00645    0.421  6.74e- 1 FALSE\n13 elec_full_cubic sin5         0.0220      0.00645    3.41   8.36e- 4 TRUE \n14 elec_full_cubic cos5        -0.000505    0.00645   -0.0783 9.38e- 1 FALSE\n15 elec_full_cubic cos6        -0.00172     0.00456   -0.378  7.06e- 1 FALSE\n\n\nShow the code\n# Quadratic model with standardized time variable\n\nelec_full_quadratic_lm &lt;- elec_ts |&gt;\n  model(elec_full_cubic = TSLM(log(megawatthours) ~ std_t + I(std_t^2) + \n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 + sin4 + cos4 + sin5 + cos5 + cos6)) # Note sin6 is omitted\nelec_full_quadratic_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 14 × 7\n   .model          term         estimate std.error statistic  p.value sig  \n   &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;\n 1 elec_full_cubic (Intercept) 16.1        0.00689 2333.     0        TRUE \n 2 elec_full_cubic std_t        0.143      0.00462   31.0    5.06e-68 TRUE \n 3 elec_full_cubic I(std_t^2)   0.0313     0.00517    6.05   1.04e- 8 TRUE \n 4 elec_full_cubic sin1        -0.0551     0.00650   -8.47   1.79e-14 TRUE \n 5 elec_full_cubic cos1        -0.0465     0.00650   -7.16   3.03e-11 TRUE \n 6 elec_full_cubic sin2        -0.000536   0.00650   -0.0825 9.34e- 1 FALSE\n 7 elec_full_cubic cos2        -0.000571   0.00650   -0.0880 9.30e- 1 FALSE\n 8 elec_full_cubic sin3         0.000775   0.00650    0.119  9.05e- 1 FALSE\n 9 elec_full_cubic cos3         0.0133     0.00650    2.05   4.19e- 2 TRUE \n10 elec_full_cubic sin4         0.0119     0.00649    1.84   6.80e- 2 FALSE\n11 elec_full_cubic cos4         0.00298    0.00650    0.458  6.48e- 1 FALSE\n12 elec_full_cubic sin5         0.0219     0.00649    3.37   9.39e- 4 TRUE \n13 elec_full_cubic cos5        -0.000245   0.00650   -0.0378 9.70e- 1 FALSE\n14 elec_full_cubic cos6        -0.00159    0.00459   -0.347  7.29e- 1 FALSE\n\n\nShow the code\nelec_resid_df &lt;- elec_full_quadratic_lm |&gt; \n  residuals() |&gt; \n  as_tibble() |&gt; \n  dplyr::select(.resid) |&gt;\n  rename(x = .resid) \n  \nelec_resid_df |&gt;\n  mutate(density = dnorm(x, mean(elec_resid_df$x), sd(elec_resid_df$x))) |&gt;\n  ggplot(aes(x = x)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"#56B4E9\", binwidth = 0.02) +\n    geom_line(aes(x = x, y = density)) +\n    theme_bw() +\n    labs(\n      x = \"Values\",\n      y = \"Frequency\",\n      title = \"Histogram of Residuals from the Full Quadratic Model\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nShow the code\nskewness(elec_resid_df$x)\n\n\n[1] -0.9556189\n\n\nThere is moderate negative skewness. We will use the emperical correction factor.",
    "crumbs": [
      "Lesson 5",
      "Forecasting, Inverse Transformation, and Bias Correction"
    ]
  },
  {
    "objectID": "chapter_6_lesson_1.html",
    "href": "chapter_6_lesson_1.html",
    "title": "Moving Average (MA) Models",
    "section": "",
    "text": "Characterize the properties of moving average (MA) models\n\n\nDefine a moving average (MA) process\nWrite an MA(q) model in terms of the backward shift operator\nState the mean and variance of an MA(q) process\nExplain the autocorrelation function of an MA(q) process\nDefine an invertible MA process\n\n\n\n\nFit time series models to data and interpret fitted parameters\n\n\nDetermine an appropriate MA(q) model to fit to a time series based on the ACF plot\nFit an MA(q) model to data in R using the arima() function\nAssess model fit by examining residual diagnostic plots\nInterpret the fitted MA coefficients",
    "crumbs": [
      "Lesson 1",
      "Moving Average (MA) Models"
    ]
  },
  {
    "objectID": "chapter_6_lesson_1.html#learning-outcomes",
    "href": "chapter_6_lesson_1.html#learning-outcomes",
    "title": "Moving Average (MA) Models",
    "section": "",
    "text": "Characterize the properties of moving average (MA) models\n\n\nDefine a moving average (MA) process\nWrite an MA(q) model in terms of the backward shift operator\nState the mean and variance of an MA(q) process\nExplain the autocorrelation function of an MA(q) process\nDefine an invertible MA process\n\n\n\n\nFit time series models to data and interpret fitted parameters\n\n\nDetermine an appropriate MA(q) model to fit to a time series based on the ACF plot\nFit an MA(q) model to data in R using the arima() function\nAssess model fit by examining residual diagnostic plots\nInterpret the fitted MA coefficients",
    "crumbs": [
      "Lesson 1",
      "Moving Average (MA) Models"
    ]
  },
  {
    "objectID": "chapter_6_lesson_1.html#preparation",
    "href": "chapter_6_lesson_1.html#preparation",
    "title": "Moving Average (MA) Models",
    "section": "Preparation",
    "text": "Preparation\n\nRead Sections 6.1-6.4",
    "crumbs": [
      "Lesson 1",
      "Moving Average (MA) Models"
    ]
  },
  {
    "objectID": "chapter_6_lesson_1.html#learning-journal-exchange-10-min",
    "href": "chapter_6_lesson_1.html#learning-journal-exchange-10-min",
    "title": "Moving Average (MA) Models",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading another student’s?\nWhat would you recommend the other student add to their learning journal?\nSign the Learning Journal review sheet for your peer",
    "crumbs": [
      "Lesson 1",
      "Moving Average (MA) Models"
    ]
  },
  {
    "objectID": "chapter_6_lesson_1.html#class-activity-introduction-to-moving-average-ma-models-15-min",
    "href": "chapter_6_lesson_1.html#class-activity-introduction-to-moving-average-ma-models-15-min",
    "title": "Moving Average (MA) Models",
    "section": "Class Activity: Introduction to Moving Average (MA) Models (15 min)",
    "text": "Class Activity: Introduction to Moving Average (MA) Models (15 min)\n\nStationary Processes\nIn previous chapters, we have explored how to identify and remove the trend and seasonal components of a time series. After the trend and seasonal component have been properly removed, the residual should be stationary. However, these residual components may still contain autocorrelation.\nIn this chapter, we will explore stationary models that are appropriate when there are no obvious trends or seasonal elements. Combining the fitted stationary model with the estimated trend and seasonal components can improve our ability to make forecasts. We will build on the autoregressive (AR) models we learned in Chapter 4.\n\n\nStrictly Stationary Series\nFirst, we define a strictly stationary series.\n\n\n\n\n\n\nDefinition of Strict Stationarity\n\n\n\nA time series model \\(\\{x_t\\}\\) is said to be strictly stationary if the joint distribution of the random variables \\(x_{t_1}, x_{t_2}, \\ldots, x_{t_n}\\) is the same as the joint distribution of \\(x_{t_1+m}, x_{t_2+m}, \\ldots, x_{t_n+m}\\) for all \\(t_1, t_2, \\ldots, t_n\\) and \\(m\\), so that the distribution of the values in the time series is the same after an arbitrary time shift.\n\n\nIf a time series is strictly stationary, then its mean and variance are constant in time. Hence, the autocovariance \\(cov(x_t, x_s)\\) depends only on the lag, \\(k = | t - s |\\). We can therefore denote the covariance function as \\(\\gamma(k) = cov(x_t, x_{t+k})\\).\nNote: It is possible that a series could have a constant mean and variance in time and the autocovariance depends only on the lag, but the series is not strictly stationary. This is called second-order stationary.\nWe will focus on the second-order properties of the time series, even though all the series we will explore in this chapter are strictly stationary.\nNote: if a white noise process is Gaussian, the stochastic process is completely determined by the mean and covariance structure. This is similar to how a (univariate or multivariate) normal distribution is completely specified by the mean and variance-covariance matrix.\nThe concept of stationarity is a property of time series models. When we use certain models, we are assuming stationarity. Before we apply these models, it is important to check for stationarity in the time series. In other words, we check to see if there is evidence of a trend or seasonality and if so, we remove these components. We can use methods such as decomposition, Holt-Winters, or regression to remove the trend and seasonality. Hence, it is typically reasonable to consider the residual series as a stationary series. Typically the models in this chapter are applied to the residual series from a regression or similar analysis.\n\n\nMoving Average (MA) Models\nRecall in Chapter 4, Lesson 3, we learned the definition of an AR model:\n\n\n\n\n\n\nDefinition of an Autoregressive (AR) Model\n\n\n\nThe time series \\(\\{x_t\\}\\) is an autoregressive process of order \\(p\\), denoted as \\(AR(p)\\), if \\[\n  x_t = \\alpha_1 x_{t-1} + \\alpha_2 x_{t-2} + \\alpha_3 x_{t-3} + \\cdots + \\alpha_{p-1} x_{t-(p-1)} + \\alpha_p x_{t-p} + w_t ~~~~~~~~~~~~~~~~~~~~~~~ (4.15)\n\\]\nwhere \\(\\{w_t\\}\\) is white noise and the \\(\\alpha_i\\) are the model parameters with \\(\\alpha_p \\ne 0\\).\n\n\nThe \\(AR(p)\\) model can be expressed in terms of the backward shift operator: \\[\n   \\left( 1 - \\alpha_1 \\mathbf{B} - \\alpha_2 \\mathbf{B}^2 - \\cdots - \\alpha_p \\mathbf{B}^p \\right) x_t = w_t\n\\]\nNow, we consider a different, but related model, the moving average (MA) model\n\n\n\n\n\n\nDefinition of a Moving Average (MA) Model\n\n\n\nWe say that a time series \\(\\{x_t\\}\\) is a moving average process of order \\(q\\), denoted as \\(MA(q)\\), if each term in the time series is a linear combination of the current white noise term and the \\(q\\) most recent past white noise terms.\nIt is given as: \\[\n  x_t = w_t + \\beta_1 w_{t-1} + \\beta_2 w_{t-2} + \\beta_3 w_{t-3} + \\cdots + \\beta_{q-1} w_{t-(q-1)} + \\beta_q w_{t-q} ~~~~~~~~~~~~~~~~~~~~~~~ (6.1)\n\\]\nwhere \\(\\{w_t\\}\\) is white noise with zero mean and variance \\(\\sigma_w^2\\), and the \\(\\beta_i\\) are the model parameters with \\(\\beta_q \\ne 0\\).\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWrite Equation (6.1) in terms of the backward shift operator. Your answer will be of the form:\n\n\\[\n  x_t\n    = (\\text{some}~q^{th}~\\text{degree polynomial in}~\\mathbf{B}) w_t\n    = \\phi_q(\\mathbf{B}) w_t\n\\]\n\n\n\n\n\n\n\n\nNote\n\n\n\nAn \\(MA(q)\\) process is comprised of a finite summation of stationary white noise terms. Hence, an \\(MA(q)\\) process will be stationary with a time-invariante mean and autocovariance.\nThe mean and variance of \\(\\{x_t\\}\\) are easily derived. The mean must be zero, because each term is a sum of scaled white noise terms with mean zero.\nThe variance of an \\(MA(q)\\) process is \\({ \\sigma_w^2 \\left( 1 + \\beta_1^2 + \\beta_2^2 + \\beta_3^2 + \\cdots + \\beta_{q-1}^2 + \\beta_q^2 \\right) }\\). This can be seen, because the white noise terms are independent with the same variance.\nSo, the autocorrelation function is\n\\[\n  \\rho(k) =\n  cor(x_t, x_{t+k}) =\n    \\begin{cases}\n      1, & k=0 \\\\\n      ~\\\\\n      \\dfrac{ \\sum\\limits_{i=0}^{q-k} \\beta_i \\beta_{i+k} }{ \\sum\\limits_{i=0}^q \\beta_i^2 }, & k = 1, 2, \\ldots, q \\\\\n      ~\\\\\n      0, & k &gt; q\n    \\end{cases}\n\\] where \\(\\beta_0 = 1\\).\nNote that the autocorrelation function is zero if \\(k&gt;q\\), because \\(x_t\\) and \\(x_{t+k}\\) would be independent weighted summations of white noise processes and hence the covariance between them would be zero.\n\n\nWe now define an invertible \\(MA\\) process.\n\n\n\n\n\n\nDefinition of an Invertible \\(MA\\) Process\n\n\n\nAn \\(MA\\) process is said to be invertible if it can be expressed as a stationary autoregressive process (of infinite order) with no error term.\n\n\n\nExample of an Invertible MA Process\nRecall that \\[\n  (1-x)(1 + x + x^2 + x^3 + \\cdots) = 1\n\\]\nor,\n\\[\n  (1-x)^{-1} = (1 + x + x^2 + x^3 + \\cdots)\n\\] if \\(|x|&lt;1\\).\nNow, note that the \\(MA\\) process\n\\[\n  x_t = \\left( 1 - \\beta \\mathbf{B} \\right) w_t\n\\]\ncan be written as:\n\\[\\begin{align*}\n  \\left( 1 - \\beta \\mathbf{B} \\right)^{-1} x_t &= w_t \\\\\n  \\left( 1 + \\beta \\mathbf{B} + \\beta^2 \\mathbf{B}^2 + \\beta^3 \\mathbf{B}^3 + \\cdots \\right) x_t &= w_t \\\\\n  x_t + \\beta x_{t-1} + \\beta^2 x_{t-2} + \\beta^3 x_{t-3} + \\cdots &= w_t \\\\\n  x_t &= \\left( -\\beta x_{t-1} - \\beta^2 x_{t-2} - \\beta^3 x_{t-3} - \\cdots \\right) + w_t\n\\end{align*}\\]\nassuming that \\(|\\beta|&lt;1\\). Note that this series will not converge unless \\(|\\beta|&lt;1\\).\nWe have just shown that the \\(MA\\) process \\[\n  x_t = \\left( 1 - \\beta \\mathbf{B} \\right) w_t\n\\] is invertible.\n\n\n\n\n\n\nTheorem: Invertibility of an \\(MA(q)\\) Process\n\n\n\nThe \\(MA(q)\\) process \\[\n  x_t = \\phi_q(\\mathbf{B}) w_t\n\\] will be invertible if the solutions to the equation \\[\n  \\phi_q(\\mathbf{B}) = 0\n\\] are all greater than 1 in absolute value.\n\n\nDoes this remind you of the test for the stationarity of an \\(AR(p)\\) model?\nNote that the autocovariance function (acvf) will identify a unique \\(MA(q)\\) process only if the process is invertible. Fortunately, the algorithm R uses to estimate an \\(MA(q)\\) process always leads to an invertible model.",
    "crumbs": [
      "Lesson 1",
      "Moving Average (MA) Models"
    ]
  },
  {
    "objectID": "chapter_6_lesson_1.html#class-activity-simulating-an-maq-model-5-min",
    "href": "chapter_6_lesson_1.html#class-activity-simulating-an-maq-model-5-min",
    "title": "Moving Average (MA) Models",
    "section": "Class Activity: Simulating an \\(MA(q)\\) Model (5 min)",
    "text": "Class Activity: Simulating an \\(MA(q)\\) Model (5 min)\nThe textbook gives a simulation of an \\(MA(3)\\) process:\n\\[\n  x_t = w_t + \\beta_1 w_{t-1} + \\beta_2 w_{t-2} + \\beta_3 w_{t-3}\n\\]\nwhere \\(\\beta_1 = 0.7\\), \\(\\beta_1 = 0.5\\), and \\(\\beta_3 = 0.2\\). This shiny app allows you to simulate from this process.\n \n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUse the simulation above to do the following:\n\nGenerate the theoretical acf plot for the \\(MA(3)\\) model \\[\n  x_t = w_t - 0.7 w_{t-1} + 0.5 w_{t-2} - 0.2 w_{t-3}\n\\]\nHow does the value of the \\(\\beta\\)’s affect the acf?\nSimulate 1000 observations from this \\(MA(3)\\) process.\n\nGive the time plot of the simulated data\nPlot the acf of the simulated data.\n\nCompare the acf from the simulated data with the theoretical acf.",
    "crumbs": [
      "Lesson 1",
      "Moving Average (MA) Models"
    ]
  },
  {
    "objectID": "chapter_6_lesson_1.html#class-activity-identifying-ar-and-ma-models-from-the-acf-and-pacf-5-min",
    "href": "chapter_6_lesson_1.html#class-activity-identifying-ar-and-ma-models-from-the-acf-and-pacf-5-min",
    "title": "Moving Average (MA) Models",
    "section": "Class Activity: Identifying AR and MA Models from the ACF and PACF (5 min)",
    "text": "Class Activity: Identifying AR and MA Models from the ACF and PACF (5 min)\n\nAR Process\nRecall that on page 81, the textbook states that in general, the partial autocorrelation at lag \\(k\\) is the \\(k^{th}\\) coefficient of a fitted \\(AR(k)\\) model. This implies that if the underlying process is \\(AR(p)\\), then all the coefficients \\(\\alpha_k\\) will equal 0 whenever \\(k&gt;p\\). So, an \\(AR(p)\\) process will result in partial correlations that are zero after lag \\(p\\). So, we can look at the correlogram of partial autocorrelations to determine the order of an appropriate \\(AR\\) process to model a time series.\n\n\nMA Process\nSimilarly, for an \\(MA(q)\\) process, the coefficients \\(\\beta_k\\) will equal 0 whenever \\(k &gt; q\\). Hence, an \\(MA(q)\\) process will demonstrate autocorrelations that are 0 after lag \\(q\\). So, considering the correlogram of autocorrelations, we can assess if an \\(MA(q)\\) model would be appropriate.\nBless their hearts, the textbook authors give a bad example in Section 6.4.2. They even state that it is “not a realistic realisation.” MA processes naturally arise in ratios of observed data. Multi-period asset returns (i.e. ratios of some previous term’s value) tend to follow an MA process.\nFor example, if there are 252 trading days in a year, then the daily series of year-over-year returns (this year’s value divided by last year’s value) follows an \\(MA(252-1)\\) process. If we are comparing values observed to those from one week ago, we would have an \\(MA(7-1)\\) process.\n\n\nComparison\n\n\n\n\n\n\nACF and PACF of an \\(AR(p)\\) Process\n\n\n\nWe can use the pacf and acf plots to assess if an \\(AR(p)\\) or \\(MA(q)\\) model is appropriate. For an \\(AR(p)\\) or \\(MA(q)\\) process, we observe the following:\n\n\n\n\n\nAR(p)\nMA(q)\n\n\n\n\nACF\nTails off\nCuts off after lag \\(q\\)\n\n\nPACF\nCuts off after lag \\(p\\)\nTails off",
    "crumbs": [
      "Lesson 1",
      "Moving Average (MA) Models"
    ]
  },
  {
    "objectID": "chapter_6_lesson_1.html#class-activity-fitting-an-maq-model-to-gdp-year-over-year-ratios-5-min",
    "href": "chapter_6_lesson_1.html#class-activity-fitting-an-maq-model-to-gdp-year-over-year-ratios-5-min",
    "title": "Moving Average (MA) Models",
    "section": "Class Activity: Fitting an \\(MA(q)\\) Model to GDP Year-Over-Year Ratios (5 min)",
    "text": "Class Activity: Fitting an \\(MA(q)\\) Model to GDP Year-Over-Year Ratios (5 min)\nTo fit an \\(MA(q)\\) model, we look at the acf to determine if it cuts off after \\(q\\) lags.\n\n\nShow the code\n# gdp_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/gdp_fred.csv\") |&gt;\ngdp_ts &lt;- rio::import(\"data/gdp_fred.csv\") |&gt;\n  mutate(year_over_year = gdp_millions / lag(gdp_millions, 4)) |&gt;\n  mutate(quarter = yearquarter(mdy(quarter))) |&gt;\n  filter(quarter &gt;= yearquarter(my(\"Jan 1990\")) & quarter &lt; yearquarter(my(\"Jan 2025\"))) |&gt;\n  na.omit() |&gt;\n  mutate(t = 1:n()) |&gt;\n  mutate(std_t = (t - mean(t)) / sd(t)) |&gt;\n  as_tsibble(index = quarter)\n\ngdp_ts |&gt;\n  autoplot(.vars = gdp_millions) +\n    labs(\n      x = \"Quarter\",\n      y = \"GDP (Millions of $US)\",\n      title = \"U.S. Gross Domestic Product (GDP) in Millions of Dollars\"\n    ) +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nShow the code\ngdp_ts |&gt;\n  autoplot(.vars = year_over_year) +\n  stat_smooth(method = \"lm\", \n              formula = y ~ x, \n              geom = \"smooth\",\n              se = FALSE,\n              color = \"#E69F00\",\n              linetype = \"dotted\") +\n    labs(\n      x = \"Quarter\",\n      y = \"Ratio\",\n      title = \"Year-Over-Year Change in U.S. GDP\"\n    ) +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nShow the code\ngdp_ts |&gt;\n  select(year_over_year) |&gt;\n  acf()\n\n\n\n\n\n\n\n\n\nShow the code\ngdp_ts |&gt;\n  select(year_over_year) |&gt;\n  pacf()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat process would you use to model the year-over-year GDP ratios?\nModify the code below to implement your model.\n\n\ngdp_ma &lt;- gdp_ts |&gt;\n  model(arima = ARIMA(year_over_year ~ 1 + pdq(0,0,1) + PDQ(0, 0, 0)))\n\ntidy(gdp_ma)\n\ngdp_ma |&gt;\n  residuals() |&gt;\n  ACF() |&gt;\n  autoplot()\n\n\nWhat are the values of the model coefficients?\nBased on the acf of the residuals, is the MA model you identified a good fit to the data?",
    "crumbs": [
      "Lesson 1",
      "Moving Average (MA) Models"
    ]
  },
  {
    "objectID": "chapter_6_lesson_1.html#small-group-activity-fitting-an-maq-model-to-the-trade-data-15-min",
    "href": "chapter_6_lesson_1.html#small-group-activity-fitting-an-maq-model-to-the-trade-data-15-min",
    "title": "Moving Average (MA) Models",
    "section": "Small-Group Activity: Fitting an \\(MA(q)\\) Model to the Trade Data (15 min)",
    "text": "Small-Group Activity: Fitting an \\(MA(q)\\) Model to the Trade Data (15 min)\n\nVessels Cleared in Foreign Trade for United States\nIn the homework for Chapter 1 Lesson 5, you explored data on the thousands of net tons cleared in foreign trade for the United States each month from January 1902 to December 1940. The code below computes the year-over-year change in the amount of cargo cleared for trade. This is stored in the variable ratio.\n\n\nShow the code\nvessels_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/Vessels_Trade_US.csv\") |&gt;\n  # filter(-comments) |&gt;\n  mutate(\n    date = yearmonth(dmy(date)),\n    ratio = vessels / lag(vessels, 12)\n  ) |&gt;\n  na.omit() |&gt;\n  as_tsibble(index = date)\n\nvessels_ts |&gt;\n  autoplot(.vars = ratio) +\n    labs(\n      x = \"Month\",\n      y = \"Ratio\",\n      title = \"Year-Over-Year Change in Net Tons on Vessels Cleared for Trade\"\n    ) +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nPractice applying an \\(MA(q)\\) model using the year-over-year amounts.\n\nDetermine which MA model is most appropriate for these data.\nFit the model you deem most appropriate.\nAssess the appropriateness of applying your model to the data.",
    "crumbs": [
      "Lesson 1",
      "Moving Average (MA) Models"
    ]
  },
  {
    "objectID": "chapter_6_lesson_1.html#homework-preview-5-min",
    "href": "chapter_6_lesson_1.html#homework-preview-5-min",
    "title": "Moving Average (MA) Models",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_6_1.qmd \n\n\nMA(q) process in terms of the backward shift operator\n\n\\[\\begin{align*}\n  x_t\n    &= w_t + \\beta_1 w_{t-1} + \\beta_2 w_{t-2} + \\beta_3 w_{t-3} + \\cdots + \\beta_{q-1} w_{t-(q-1)} + \\beta_q w_{t-q} \\\\\n    &= w_t + \\beta_1 \\mathbf{B} w_t + \\beta_2 \\mathbf{B}^2 w_t + \\beta_3 \\mathbf{B}^3 w_t + \\cdots + \\beta_{q-1} \\mathbf{B}^{q-1} w_t + \\beta_q \\mathbf{B}^{q} w_t \\\\\n    &= \\left( 1 + \\beta_1 \\mathbf{B} + \\beta_2 \\mathbf{B}^2  + \\beta_3 \\mathbf{B}^3  + \\cdots + \\beta_{q-1} \\mathbf{B}^{q-1}  + \\beta_q \\mathbf{B}^{q} \\right) w_t \\\\\n    &= \\phi_q(\\mathbf{B}) w_t\n\\end{align*}\\]\n\nSimulating an MA(3) process\n\nSet the values of the parameters \\(n\\), \\(\\beta_1\\), \\(\\beta_2\\), and \\(\\beta_3\\) in the simulation to the following: \\[\nn = 1000, ~~~~~~~~~~~\n\\beta_1 = -0.7, ~~~~~~~~~~~\n\\beta_2 = 0.5, ~~~~~~~~~~~\n\\beta_3 = -0.2\n\\]\n\nClass Activity: Fitting an MA(q) Model to GDP Year-Over-Year Ratios\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat process would you use to model the year-over-year GDP ratios?\n\nBased on the acf, an \\(MA(3)\\) model seems most appropriate.\n\n\nShow the code\ngdp_ts |&gt;\n  select(year_over_year) |&gt;\n  acf()\n\n\n\n\n\n\n\n\n\nShow the code\ngdp_ts |&gt;\n  select(year_over_year) |&gt;\n  pacf()\n\n\n\n\n\n\n\n\n\n\nModify the code below to implement your model.\n\n\n\nShow the code\ngdp_ma &lt;- gdp_ts |&gt;\n  # Changed 1 to a 3 on the next line\n  model(arima = ARIMA(year_over_year ~ 1 + pdq(0,0,3) + PDQ(0, 0, 0))) \n\n\n\nWhat are the values of the model coefficients?\n\nThe values of the coefficients are given in the table below:\n\n\nShow the code\ntidy(gdp_ma)\n\n\n# A tibble: 4 × 6\n  .model term     estimate std.error statistic   p.value\n  &lt;chr&gt;  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n1 arima  ma1         0.978   0.0341       28.7 1.32e- 59\n2 arima  ma2         1.01    0.0413       24.5 8.75e- 52\n3 arima  ma3         0.965   0.0389       24.8 2.32e- 52\n4 arima  constant    1.05    0.00435     241.  7.17e-181\n\n\n\nBased on the acf of the residuals, is the MA model you identified a good fit to the data?\n\n\n\nShow the code\ngdp_ma |&gt;\n  residuals() |&gt;\n  ACF(var = .resid) |&gt;\n  autoplot()\n\n\nWarning: The `...` argument of `PACF()` is deprecated as of feasts 0.2.2.\nℹ ACF variables should be passed to the `y` argument. If multiple variables are\n  to be used, specify them using `vars(...)`.\n\n\n\n\n\n\n\n\n\nNone of the acf values are significant. The \\(MA(3)\\) model seems appropriate.\nWe can write the estimated model as: \\[\n  x_t = 1.0478741 + 0.9783408 \\mathbf{B} + 1.0127334 \\mathbf{B}^2 + 0.9651468 \\mathbf{B}^3\n\\]\nThe absolute values of the roots of the right-hand side of this equation are:\n\n\nShow the code\ncoeffs &lt;- tidy(gdp_ma) |&gt;\n  # filter(term != \"constant\") |&gt;\n  select(estimate) |&gt;\n  pull()\n\nabs(polyroot(c(coeffs |&gt; tail(1), coeffs |&gt; head(-1))))\n\n\n[1] 1.012232 1.059633 1.012232\n\n\nThe model is invertible. (As mentioned previously, the process by which these are constructed guarantees they will be invertible.)\n\n\n\nSmall-Group Activity: Fitting an MA(q) Model to the Trade Data\n\n\n\nShow the code\nvessels_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/Vessels_Trade_US.csv\") |&gt;\n  # filter(-comments) |&gt;\n  mutate(\n    date = yearmonth(dmy(date)),\n    ratio = vessels / lag(vessels, 12)\n  ) |&gt;\n  na.omit() |&gt;\n  as_tsibble(index = date)\n\nvessels_ts |&gt;\n  autoplot(.vars = ratio) +\n    labs(\n      x = \"Month\",\n      y = \"Ratio\",\n      title = \"Year-Over-Year Change in Net Tons on Vessels Cleared for Trade\"\n    ) +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\nShow the code\nvessels_ts |&gt;\n  select(ratio) |&gt;\n  acf()\n\n\n\n\n\n\n\n\n\nShow the code\nvessels_ts |&gt;\n  select(ratio) |&gt;\n  pacf()\n\n\n\n\n\n\n\n\n\nShow the code\ngdp_ma &lt;- vessels_ts |&gt;\n  model(arima = ARIMA(ratio ~ 1 + pdq(0,0,12) + PDQ(0, 0, 0)))\n\ntidy(gdp_ma)\n\n\n# A tibble: 13 × 6\n   .model term     estimate std.error statistic   p.value\n   &lt;chr&gt;  &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;\n 1 arima  ma1         0.572    0.0475     12.0  2.70e- 29\n 2 arima  ma2         0.558    0.0475     11.8  4.09e- 28\n 3 arima  ma3         0.601    0.0589     10.2  3.55e- 22\n 4 arima  ma4         0.488    0.0582      8.38 6.21e- 16\n 5 arima  ma5         0.430    0.0585      7.35 8.83e- 13\n 6 arima  ma6         0.457    0.0659      6.94 1.27e- 11\n 7 arima  ma7         0.323    0.0600      5.37 1.23e-  7\n 8 arima  ma8         0.358    0.0558      6.42 3.36e- 10\n 9 arima  ma9         0.479    0.0607      7.90 1.98e- 14\n10 arima  ma10        0.411    0.0514      7.99 1.04e- 14\n11 arima  ma11        0.517    0.0471     11.0  4.69e- 25\n12 arima  ma12       -0.306    0.0586     -5.22 2.70e-  7\n13 arima  constant    1.02     0.0171     59.8  2.45e-221\n\n\nShow the code\ngdp_ma |&gt;\n  residuals() |&gt;\n  ACF() |&gt;\n  autoplot()\n\n\nResponse variable not specified, automatically selected `var = .resid`",
    "crumbs": [
      "Lesson 1",
      "Moving Average (MA) Models"
    ]
  },
  {
    "objectID": "chapter_7.html",
    "href": "chapter_7.html",
    "title": "Lessons & Homework",
    "section": "",
    "text": "Chapter 7\n\n\n\n\n\n\nLessons\n\n\n\n\nLesson 1 - Introduction to Non-stationary Models and Differencing\nLesson 2 - Seasonal ARIMA Models\nLesson 3 - ARCH and GARCH Models\n\n\n\n\n\n\n\n\n\nDownload Homework Assignments\n\n\n\n\n homework_7_1.qmd \n homework_7_2.qmd \n homework_7_3.qmd",
    "crumbs": [
      "Overview",
      "Lessons & Homework"
    ]
  },
  {
    "objectID": "chapter_7_lesson_2.html",
    "href": "chapter_7_lesson_2.html",
    "title": "Seasonal ARIMA Models",
    "section": "",
    "text": "Identify seasonal ARIMA models\n\n\nDefine seasonal ARIMA models and their notation (p, d, q)(P, D, Q)[m]\nIdentify the need for seasonal ARIMA models in time series with seasonal patterns\n\n\n\n\nApply the fitting procedure for seasonal ARIMA models\n\n\nDescribe the steps involved in fitting seasonal ARIMA models\nDetermine the appropriate order of differencing (d and D) based on ACF/PACF plots\nSelect the order of AR and MA terms (p, q, P, Q) using ACF/PACF plots and model selection criteria\n\n\n\n\nFit seasonal ARIMA models to time series data using R\n\n\nUse R to fit seasonal ARIMA models\nInterpret the output for the ARIMA models, including coefficients and model diagnostics\nForecast future values using the fitted seasonal ARIMA model",
    "crumbs": [
      "Lesson 2",
      "Seasonal ARIMA Models"
    ]
  },
  {
    "objectID": "chapter_7_lesson_2.html#learning-outcomes",
    "href": "chapter_7_lesson_2.html#learning-outcomes",
    "title": "Seasonal ARIMA Models",
    "section": "",
    "text": "Identify seasonal ARIMA models\n\n\nDefine seasonal ARIMA models and their notation (p, d, q)(P, D, Q)[m]\nIdentify the need for seasonal ARIMA models in time series with seasonal patterns\n\n\n\n\nApply the fitting procedure for seasonal ARIMA models\n\n\nDescribe the steps involved in fitting seasonal ARIMA models\nDetermine the appropriate order of differencing (d and D) based on ACF/PACF plots\nSelect the order of AR and MA terms (p, q, P, Q) using ACF/PACF plots and model selection criteria\n\n\n\n\nFit seasonal ARIMA models to time series data using R\n\n\nUse R to fit seasonal ARIMA models\nInterpret the output for the ARIMA models, including coefficients and model diagnostics\nForecast future values using the fitted seasonal ARIMA model",
    "crumbs": [
      "Lesson 2",
      "Seasonal ARIMA Models"
    ]
  },
  {
    "objectID": "chapter_7_lesson_2.html#preparation",
    "href": "chapter_7_lesson_2.html#preparation",
    "title": "Seasonal ARIMA Models",
    "section": "Preparation",
    "text": "Preparation\n\nRead Section 7.3",
    "crumbs": [
      "Lesson 2",
      "Seasonal ARIMA Models"
    ]
  },
  {
    "objectID": "chapter_7_lesson_2.html#learning-journal-exchange-10-min",
    "href": "chapter_7_lesson_2.html#learning-journal-exchange-10-min",
    "title": "Seasonal ARIMA Models",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading another student’s?\nWhat would you recommend the other student add to their learning journal?\nSign the Learning Journal review sheet for your peer\n\n\nDefinition of Seasonal ARIMA (SARIMA) Models\nWe can add a seasonal component to an ARIMA model. We call this model a Seasonal ARIMA model, or a SARIMA model. We use differencing at a lag equal to the number of seasons, \\(s\\). This allows us to remove additional seasonal effects that carry over from one cycle to another.\nRecall that if we use a difference of lag 1 to remove a linear trend, we introduce a moving average term. The same thing happens when we introduce a difference with lag \\(s\\). For the lag \\(s\\) values, we can apply an autoregressive (AR) component at lag \\(s\\) with \\(P\\) parameters, an integrated (I) component with parameter \\(D\\), and a moving average (MA) component with \\(Q\\) terms. This yields the full SARIMA model.\n\n\n\n\n\n\nDefinition of SARIMA Models\n\n\n\nA Seasonal ARIMA model, or SARIMA model with \\(s\\) seasons can be expressed as:\n\\[\n  \\Theta_P \\left( \\mathbf{B}^s \\right) \\theta_p \\left( \\mathbf{B} \\right) \\left( 1 - \\mathbf{B}^s \\right)^D \\left( 1 - \\mathbf{B} \\right)^d x_t = \\Phi_Q \\left( \\mathbf{B}^s \\right) \\phi_q \\left( \\mathbf{B} \\right) w_t\n\\]\nwhere\n\n\\(p\\), \\(d\\), and \\(q\\) are the parameters for the \\(ARIMA(p, d, q)\\) model applied to the time series and the differences are taken with a lag of 1,\n\\(s\\) is the number of seasons, and\n\\(P\\), \\(D\\), and \\(Q\\) are the parameters for the \\(ARIMA(P, D, Q)\\) model applied to the time series where the differences are taken across a lag of \\(s\\).\n\n\\(\\Theta_P \\left( \\mathbf{B}^s \\right)\\), \\(\\theta_p \\left( \\mathbf{B} \\right)\\), \\(\\Phi_Q \\left( \\mathbf{B}^s \\right)\\), and \\(\\phi_q \\left( \\mathbf{B} \\right)\\) are polynomials of degree \\(P\\), \\(p\\), \\(Q\\), and \\(q\\), respectively.\nWe denote this model as \\(ARIMA(p,d,q)(P,D,Q)_s\\) or \\(ARIMA(p,d,q)(P,D,Q)[s]\\).\n\n\nLooking closely at this model, we can see it is the combination of the ARIMA model \\[\n  \\theta_p \\left( \\mathbf{B} \\right) \\left( 1 - \\mathbf{B} \\right)^d x_t = \\phi_q \\left( \\mathbf{B} \\right) w_t\n\\] and the same model, after applying a difference across \\(s\\) seasons \\[\n  \\Theta_P \\left( \\mathbf{B}^s \\right) \\left( 1 - \\mathbf{B}^s \\right)^D x_t = \\Phi_Q \\left( \\mathbf{B}^s \\right) w_t\n\\]\n\n\nSpecial Cases of a Seasonal ARIMA Model\n\nStationary SARIMA Models\n\n\n\n\n\n\nStationarity of SARIMA Models\n\n\n\nThe SARIMA model is, in general, non-stationary. However, there is a special case in which it is stationary. An \\(ARIMA(p,d,q)(P,D,Q)_s\\) will be stationary if the following conditions are satisfied:\n\nIf \\(D = d = 0\\), and\nThe roots of the characteristic equation \\[\n  \\Theta_P \\left( \\mathbf{B}^s \\right) \\theta_p \\left( \\mathbf{B} \\right) = 0\n\\] are all greter than 1 in absolute value.\n\n\n\n\n\nA Simple AR Model with a Seasonal Period of \\(s\\) Units\n\\(ARIMA(0,0,0)(1,0,0)_{s}\\) is given as\n\\[\n  x_t = \\alpha x_{t-s} + w_t\n\\] This model would be appropriate for data where there are \\(s\\) seasons and the value in the season exactly one cycle prior affects the current value. This model is stationary when \\(\\left| \\alpha^{1/s} \\right| &gt; 1\\).\n\n\nStochastic Trends with Seasonal Influences\nMany time series with stochastic trends also have seasonal influences. We can extend the previous model as:\n\\[\n  x_t = x_{t-1} + \\alpha x_{t-s} - \\alpha x_{t-(s+1)} + w_t\n\\]\nThis is equivalent to \\[\n  \\underbrace{\\left( 1 - \\alpha \\mathbf{B}^{12} \\right)}_{\\Theta_1 \\left( \\mathbf{B}^{12} \\right)} \\left( 1 - \\mathbf{B} \\right) x_t = w_t\n\\]\nwhich is \\(ARIMA(0,1,0)(1,0,0)_s\\).\nNote that we could have written this model as: \\[\n  \\nabla x_t = \\alpha \\nabla x_{t-s} + w_t\n\\]\nThis makes it clear that under this model, the change at time \\(t\\) depends on the change at the corresponding time in the previous cycle.\nNote that \\(\\left( 1 - \\mathbf{B} \\right)\\) is a factor in the characteristic polynomial, so this model will be non-stationary.\n\n\nSimple Quarterly Seasonal Moving Average Model (with no Trend)\nWe can write a simple quarterly seasonal moving average model as\n\\[\n  x_t = \\underbrace{w_t - \\beta w_{t-4}}_{\\left( 1 - \\beta \\mathbf{B}^{4} \\right) w_t}\n\\]\nThis is an \\(ARIMA(0,0,0)(0,0,1)_4\\) model.\n\n\nQuarterly Seasonal Moving Average Model with a Stocastic Trend\nWe can add a stochastic trend to the previous model by including first-order differences:\n\\[\n  x_t = x_{t-1} + w_t - \\beta w_{t-4}\n\\] This is an \\(ARIMA(0,1,0)(0,0,1)_4\\) model.\n\n\nQuarterly Seasonal Moving Average Model where the Seasonal Terms Contain a Stocastic Trend\nWe can allow the seasonal components to include a stochastic trend. We apply differencing at the seasonal period. This yields the model\n\\[\n  x_t = x_{t-4} + w_t - \\beta w_{t-4}\n\\]\nThis is an \\(ARIMA(0,0,0)(0,1,1)_4\\) process.\n\n\n\n\n\n\nEffects of Differencing\n\n\n\nRecall that lag \\(s\\) differencing will remove a linear trend, however if there is a linear trend, differencing at lag 1 will introduce an AR process in the residuals. If a linear model is appropriate in a, say, quarterly series with additive seasonals, then the model could be \\[\n  x_t = a + bt + s_{[t]} + w_t\n\\]\nwhere \\([t]\\) is the modulus operator, or \\([t]\\) is the remainder when \\(t\\) is divided by \\(4\\). Another way to view this is to note that for a quarterly time series, \\([t] = [t-4]\\).\nIf we apply first-order differencing at lag 4, we get \\[\\begin{align*}\n  \\left( 1 - \\mathbf{B}^{4} \\right) x_t\n    &= x_t - x_{t-4} \\\\\n    &= \\left(a + bt + s_{[t]} + w_t \\right) - \\left(a + b(t-4) + s_{[t-4]} + w_{t-4} \\right) \\\\\n    &= \\left(a + bt + s_{[t]} + w_t \\right) - \\left(a + b(t-4) + s_{[t]} + w_{t-4} \\right) \\\\\n    &= 4b + w_t - w_{t-4}\n\\end{align*}\\]\nThis is an \\(ARIMA(0,0,0)(0,1,1)_4\\) process with constant term of \\(4b\\).\nIf we apply first-order differencing at lag 1 and then do the differencing at lag 4, we get the following process: \\[\\begin{align*}\n  \\left( 1 - \\mathbf{B}^{4} \\right) \\left( 1 - \\mathbf{B} \\right) x_t\n    &= \\left( 1 - \\mathbf{B}^{4} \\right) \\left( x_t  - \\mathbf{B}x_t \\right) \\\\\n    &= \\left( 1 - \\mathbf{B}^{4} \\right) \\left[ \\left( a + bt + s_{[t]} + w_t \\right)  - \\mathbf{B} \\left( a + bt + s_{[t]} + w_t \\right) \\right] \\\\\n    &= \\left( 1 - \\mathbf{B}^{4} \\right) \\left[ \\left( a + bt + s_{[t]} + w_t \\right)  - \\left( a + b(t-1) + s_{[t-1]} + w_{t-1} \\right) \\right] \\\\\n    &= \\left( 1 - \\mathbf{B}^{4} \\right) \\left[ \\left( s_{[t]} + w_t \\right)  - \\left( -b + s_{[t-1]} + w_{t-1} \\right) \\right] \\\\\n    &= \\left( 1 - \\mathbf{B}^{4} \\right) \\left( b + s_{[t]} - s_{[t-1]} + w_t - w_{t-1} \\right) \\\\\n    &= \\left( b + s_{[t]} - s_{[t-1]} + w_t - w_{t-1} \\right) - \\mathbf{B}^{4} \\left( b + s_{[t]} - s_{[t-1]} + w_t - w_{t-1} \\right)  \\\\\n    &= \\left( b + s_{[t]} - s_{[t-1]} + w_t - w_{t-1} \\right) - \\left( b + s_{[t-4]} - s_{[t-5]} + w_{t-4} - w_{t-5} \\right)  \\\\\n    &= \\left( s_{[t]} - s_{[t-1]} + w_t - w_{t-1} \\right) - \\left( s_{[t]} - s_{[t-1]} + w_{t-4} - w_{t-5} \\right)  \\\\\n    &= w_t - w_{t-1} - w_{t-4} + w_{t-5}\n\\end{align*}\\]\nThis represents an \\(ARIMA(0,1,1)(0,1,1)_4\\) process without a constant term.",
    "crumbs": [
      "Lesson 2",
      "Seasonal ARIMA Models"
    ]
  },
  {
    "objectID": "chapter_7_lesson_2.html#class-activity-seasonal-arima-models---retail-general-merchandise-stores-10-min",
    "href": "chapter_7_lesson_2.html#class-activity-seasonal-arima-models---retail-general-merchandise-stores-10-min",
    "title": "Seasonal ARIMA Models",
    "section": "Class Activity: Seasonal ARIMA Models - Retail: General Merchandise Stores (10 min)",
    "text": "Class Activity: Seasonal ARIMA Models - Retail: General Merchandise Stores (10 min)\n\n\nShow the code\n# Read in retail sales data for \"Full-Service Restaurants\"\nretail_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/retail_by_business_type.parquet\") |&gt;\n  filter(naics == 452) |&gt;\n  mutate( month = yearmonth(as_date(month)) ) |&gt;\n  na.omit() |&gt;\n  as_tsibble(index = month)\n\nretail_plot_raw &lt;- retail_ts |&gt;\n    autoplot(.vars = sales_millions) +\n    labs(\n      x = \"Month\",\n      y = \"sales_millions\",\n      title = \"Sales (in millions)\",\n      subtitle = \"General Merchandise Stores\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5),\n      plot.subtitle = element_text(hjust = 0.5)\n    )\n\nretail_plot_log &lt;- retail_ts |&gt;\n    autoplot(.vars = log(sales_millions)) +\n    labs(\n      x = \"Month\",\n      y = \"log(sales_millions)\",\n      title = \"Log of Sales (in millions)\",\n      subtitle = \"General Merchandise Stores\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5),\n      plot.subtitle = element_text(hjust = 0.5)\n    )\n\nretail_plot_raw | retail_plot_log\n\n\n\n\n\n\n\n\nFigure 1: Time plots of the retail sales in General Merchandise stores; time plot of the timt series (left) and the natural logarithm of the time series (right)\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nBased on the figure above, should the logarithm be applied to the time series? Justify your answer.\n\n\n\nHere are a few models we could fit to this time series.\n\n\nShow the code\nretail_ts |&gt;\n  model(\n    ar_model = ARIMA(sales_millions ~ 1 +\n      pdq(1, 1, 0) +\n      PDQ(1, 0, 0), approximation = TRUE),\n    ma_model = ARIMA(sales_millions ~ 1 + \n      pdq(0, 1, 1) +\n      PDQ(0, 0, 1), approximation = TRUE),\n    arima_102012 = ARIMA(sales_millions ~ 1 + \n      pdq(1, 0, 2) +\n      PDQ(0, 1, 2), approximation = TRUE),\n    arima_102102 = ARIMA(sales_millions ~ 1 + \n      pdq(1, 0, 2) +\n      PDQ(1, 0, 2), approximation = TRUE),\n    arima_112112 = ARIMA(sales_millions ~ 1 + \n      pdq(1, 1, 2) +\n      PDQ(1, 1, 2), approximation = TRUE),\n    arima_012012 = ARIMA(sales_millions ~ 1 + \n      pdq(0, 1, 2) +\n      PDQ(0, 1, 2), approximation = TRUE),\n    arima_111111 = ARIMA(sales_millions ~ 1 + \n      pdq(1, 1, 1) +\n      PDQ(1, 1, 1), approximation = TRUE),\n    ) |&gt;\n  glance()\n\n\n\n\n\n\n\n.model\nsigma2\nlog_lik\nAIC\nAICc\nBIC\n\n\n\n\nar_model\n1887684\n-3227.047\n6462.094\n6462.204\n6477.759\n\n\nma_model\n15542235\n-3605.992\n7219.985\n7220.094\n7235.650\n\n\narima_102012\n1264061\n-3038.772\n6091.544\n6091.862\n6118.747\n\n\narima_112112\n1272651\n-3030.991\n6077.982\n6078.393\n6109.048\n\n\narima_012012\n1266582\n-3031.167\n6074.333\n6074.572\n6097.633\n\n\narima_111111\n1270495\n-3031.753\n6075.506\n6075.745\n6098.806\n\n\n\n\n\n\n\nHere is the model automatically selected by R.\n\n\nShow the code\nbest_fit_retail &lt;- retail_ts |&gt;\n  model(\n    ar_model = ARIMA(sales_millions ~ 1 +\n      pdq(0:2, 0:2, 0:2) +\n      PDQ(0:2, 0:2, 0:2), approximation = TRUE))\n\nbest_fit_retail\n\n\n# A mable: 1 x 1\n                            ar_model\n                             &lt;model&gt;\n1 &lt;ARIMA(1,0,2)(0,1,2)[12] w/ drift&gt;\n\n\nThis is the acf and pacf of the residuals from the automatically selected model.\n\n\nShow the code\nacf_plot &lt;- best_fit_retail |&gt;\n  residuals() |&gt;\n  feasts::ACF() |&gt;\n  autoplot()\n\npacf_plot &lt;- best_fit_retail |&gt;\n  residuals() |&gt;\n  feasts::PACF() |&gt;\n  autoplot()\n\nacf_plot | pacf_plot\n\n\n\n\n\n\n\n\nFigure 2: Correlogram (left) and partial correlogram (right) of the residuals from the best-fitting model\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nWhat do you notice in the acf and pacf plots?\n\n\n\nHere are the coefficients from the automatically-selected model.\n\n\nShow the code\ncoefficients(best_fit_retail)\n\n\n\n\n\n\n\n.model\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\nar_model\nar1\n0.9783222\n0.0159629\n61.287161\n0.0000000\n\n\nar_model\nma1\n-0.8326530\n0.0542793\n-15.340162\n0.0000000\n\n\nar_model\nma2\n0.1720797\n0.0561780\n3.063116\n0.0023554\n\n\nar_model\nsma1\n-0.4200558\n0.0535544\n-7.843536\n0.0000000\n\n\nar_model\nsma2\n-0.0980853\n0.0515993\n-1.900905\n0.0581129\n\n\nar_model\nconstant\n39.3324809\n9.4755984\n4.150923\n0.0000414\n\n\n\n\n\n\n\nWe can forecast future values of this time series using our model.\n\n\nShow the code\nbest_fit_retail |&gt;\n  forecast(h = \"60 months\") |&gt;\n  autoplot(retail_ts) +\n    labs(\n      x = \"Month\",\n      y = \"Total U.S. Sales in Millions\",\n      title = \"Forecasted Sales (in millions)\",\n      subtitle = \"General Merchandise Stores\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5),\n      plot.subtitle = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\nFigure 3: Five-year forecast of the time series based on the best-fitting model\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nHow well does the forecast generated by this model appear to work?",
    "crumbs": [
      "Lesson 2",
      "Seasonal ARIMA Models"
    ]
  },
  {
    "objectID": "chapter_7_lesson_2.html#small-group-activity-seasonal-arima-models---retail-shoe-stores-20-min",
    "href": "chapter_7_lesson_2.html#small-group-activity-seasonal-arima-models---retail-shoe-stores-20-min",
    "title": "Seasonal ARIMA Models",
    "section": "Small-Group Activity: Seasonal ARIMA Models - Retail: Shoe Stores (20 min)",
    "text": "Small-Group Activity: Seasonal ARIMA Models - Retail: Shoe Stores (20 min)\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nRepeat the analysis above for the retail sales for shoe stores (NAICS code 4482).\n\nCreate time plots of the data and the logarithm of the data.\nIs it appropriate to take the logarithm of the time series? (Use the appropriate time series for the following.)\nFind the best-fitting SARIMA model for the time series.\nDetermine the model coefficients.\nUse the acf and pacf plots of the residuals to assess whether this model adequately models the time series.\nUse your model to forecast the value of the time series over the next five years.\nHow well does the forecast generated by this model appear to work?\nDid the downward COVID spike seriously affect the applicability of this model?",
    "crumbs": [
      "Lesson 2",
      "Seasonal ARIMA Models"
    ]
  },
  {
    "objectID": "chapter_7_lesson_2.html#homework-preview-5-min",
    "href": "chapter_7_lesson_2.html#homework-preview-5-min",
    "title": "Seasonal ARIMA Models",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_7_2.qmd \n\n\nSmall-Group Activity: Seasonal ARIMA Models - Retail: Shoe Stores\n\n\n\nShow the code\n# Read in retail sales data for \"Full-Service Restaurants\"\nretail_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/retail_by_business_type.parquet\") |&gt;\n  filter(naics == 4482) |&gt;\n  mutate( month = yearmonth(as_date(month)) ) |&gt;\n  na.omit() |&gt;\n  as_tsibble(index = month)\n\nretail_plot_raw &lt;- retail_ts |&gt;\n    autoplot(.vars = sales_millions) +\n    labs(\n      x = \"Month\",\n      y = \"sales_millions\",\n      title = \"Sales (in millions)\",\n      subtitle = \"Shoe Stores\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5),\n      plot.subtitle = element_text(hjust = 0.5)\n    )\n\nretail_plot_log &lt;- retail_ts |&gt;\n    autoplot(.vars = log(sales_millions)) +\n    labs(\n      x = \"Month\",\n      y = \"log(sales_millions)\",\n      title = \"Log of Sales (in millions)\",\n      subtitle = \"Shoe Stores\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5),\n      plot.subtitle = element_text(hjust = 0.5)\n    )\n\nretail_plot_raw | retail_plot_log\n\n\n\n\n\n\n\n\n\nShow the code\nbest_fit_retail &lt;- retail_ts |&gt;\n  model(\n    ar_model = ARIMA(sales_millions ~ 1 +\n      pdq(0:2, 0:2, 0:2) +\n      PDQ(0:2, 0:2, 0:2), approximation = TRUE))\n\nbest_fit_retail\n\n\n# A mable: 1 x 1\n                            ar_model\n                             &lt;model&gt;\n1 &lt;ARIMA(1,0,2)(0,1,2)[12] w/ drift&gt;\n\n\nShow the code\nacf_plot &lt;- best_fit_retail |&gt;\n  residuals() |&gt;\n  feasts::ACF() |&gt;\n  autoplot()\n\npacf_plot &lt;- best_fit_retail |&gt;\n  residuals() |&gt;\n  feasts::PACF() |&gt;\n  autoplot()\n\nacf_plot | pacf_plot\n\n\n\n\n\n\n\n\n\nShow the code\nbest_fit_retail |&gt;\n  glance()\n\n\n# A tibble: 1 × 8\n  .model   sigma2 log_lik   AIC  AICc   BIC ar_roots  ma_roots  \n  &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt;    &lt;list&gt;    \n1 ar_model 36049.  -2401. 4817. 4817. 4844. &lt;cpl [1]&gt; &lt;cpl [26]&gt;\n\n\nShow the code\ncoefficients(best_fit_retail)\n\n\n# A tibble: 6 × 6\n  .model   term     estimate std.error statistic  p.value\n  &lt;chr&gt;    &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 ar_model ar1         0.907    0.0423     21.4  2.50e-66\n2 ar_model ma1        -0.234    0.0689     -3.39 7.73e- 4\n3 ar_model ma2        -0.362    0.0564     -6.42 4.41e-10\n4 ar_model sma1       -0.846    0.0650    -13.0  4.95e-32\n5 ar_model sma2        0.176    0.0646      2.72 6.84e- 3\n6 ar_model constant    5.09     1.37        3.73 2.24e- 4\n\n\nShow the code\nbest_fit_retail |&gt;\n  forecast(h = \"60 months\") |&gt;\n  autoplot(retail_ts) +\n    labs(\n      x = \"Month\",\n      y = \"Total U.S. Sales in Millions\",\n      title = \"Forecasted Sales (in millions)\",\n      subtitle = \"Shoe Stores\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5),\n      plot.subtitle = element_text(hjust = 0.5)\n    )",
    "crumbs": [
      "Lesson 2",
      "Seasonal ARIMA Models"
    ]
  },
  {
    "objectID": "chapter_xxx_lesson_yyy.html",
    "href": "chapter_xxx_lesson_yyy.html",
    "title": "TitleGoesHere",
    "section": "",
    "text": "MainOutcome\n\n\nSuboutcome\nSuboutcome\nSuboutcome\n\n\n\n\nMainOutcome\n\n\nSuboutcome\nSuboutcome\nSuboutcome"
  },
  {
    "objectID": "chapter_xxx_lesson_yyy.html#learning-outcomes",
    "href": "chapter_xxx_lesson_yyy.html#learning-outcomes",
    "title": "TitleGoesHere",
    "section": "",
    "text": "MainOutcome\n\n\nSuboutcome\nSuboutcome\nSuboutcome\n\n\n\n\nMainOutcome\n\n\nSuboutcome\nSuboutcome\nSuboutcome"
  },
  {
    "objectID": "chapter_xxx_lesson_yyy.html#preparation",
    "href": "chapter_xxx_lesson_yyy.html#preparation",
    "title": "TitleGoesHere",
    "section": "Preparation",
    "text": "Preparation\n\nRead Sections xxx.yyy-xxx.zzz"
  },
  {
    "objectID": "chapter_xxx_lesson_yyy.html#learning-journal-exchange-10-min",
    "href": "chapter_xxx_lesson_yyy.html#learning-journal-exchange-10-min",
    "title": "TitleGoesHere",
    "section": "Learning Journal Exchange (10 min)",
    "text": "Learning Journal Exchange (10 min)\n\nReview another student’s journal\nWhat would you add to your learning journal after reading another student’s?\nWhat would you recommend the other student add to their learning journal?\nSign the Learning Journal review sheet for your peer"
  },
  {
    "objectID": "chapter_xxx_lesson_yyy.html#small-group-activity-sectiontitle-xxx-min",
    "href": "chapter_xxx_lesson_yyy.html#small-group-activity-sectiontitle-xxx-min",
    "title": "TitleGoesHere",
    "section": "Small Group Activity: SectionTitle (xxx min)",
    "text": "Small Group Activity: SectionTitle (xxx min)"
  },
  {
    "objectID": "chapter_xxx_lesson_yyy.html#class-activity-sectiontitle-xxx-min",
    "href": "chapter_xxx_lesson_yyy.html#class-activity-sectiontitle-xxx-min",
    "title": "TitleGoesHere",
    "section": "Class Activity: SectionTitle (xxx min)",
    "text": "Class Activity: SectionTitle (xxx min)\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\n\nQuestion1\nQuestion2"
  },
  {
    "objectID": "chapter_xxx_lesson_yyy.html#homework-preview-5-min",
    "href": "chapter_xxx_lesson_yyy.html#homework-preview-5-min",
    "title": "TitleGoesHere",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_xxx_yyy.qmd \n\n\nClass Activity\n\n\n\nClass Activity\n\n\n\nClass Activity"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_1_class_activity.html",
    "href": "class_activities/chapter_2_lesson_1_class_activity.html",
    "title": "Chapter 2: Correlation",
    "section": "",
    "text": "Caution\n\n\n\nPlease change the colors to be the okabeito scheme…\npalette(“okabeito”)\nokabeito_colors_list &lt;- c( orange = “#E69F00”, light blue = “#56B4E9”, green = “#009E73”, yellow = “#F0E442”, blue = “#0072B2”, red = “#D55E00”, purple = “#CC79A7”, grey = “#999999”, black = “#000000”, sky blue = “#56B4E9”, bluish green = “#009E73”, vermillion = “#D55E00”, reddish purple = “#CC79A7”, dark yellow = “#F5C710”, amber = “#F5C710” )"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_1_class_activity.html#objectives",
    "href": "class_activities/chapter_2_lesson_1_class_activity.html#objectives",
    "title": "Chapter 2: Correlation",
    "section": "Objectives:",
    "text": "Objectives:\n\nDefine covariance, correlation\nCompute covariance and correlation"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_1_class_activity.html#agenda",
    "href": "class_activities/chapter_2_lesson_1_class_activity.html#agenda",
    "title": "Chapter 2: Correlation",
    "section": "Agenda:",
    "text": "Agenda:\n\nIntroduction (5 mins)\nMotivate covariance and correlation"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_1_class_activity.html#learning-journal-exchange-10-mins",
    "href": "class_activities/chapter_2_lesson_1_class_activity.html#learning-journal-exchange-10-mins",
    "title": "Chapter 2: Correlation",
    "section": "Learning Journal Exchange (10 mins)",
    "text": "Learning Journal Exchange (10 mins)"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_1_class_activity.html#class-activity-variance-and-standard-deviation-15-mins",
    "href": "class_activities/chapter_2_lesson_1_class_activity.html#class-activity-variance-and-standard-deviation-15-mins",
    "title": "Chapter 2: Correlation",
    "section": "Class Activity: Variance and Standard Deviation (15 mins)",
    "text": "Class Activity: Variance and Standard Deviation (15 mins)\nWe will explore the variance and standard deviation in this section.\n\nWhat do the standard deviation and the variance measure?\n\nThe following code simulates observations of a random variable. We will use these data to explore the variance and standard deviation.\n\n# Set random seed\nset.seed(2412)\n\n# Specify means and standard deviation\nn &lt;- 5        # number of points\nmu &lt;- 10      # mean\nsigma &lt;- 3    # standard deviation\n\n# Simulate normal data\nsim_data &lt;- data.frame(x = round(rnorm(n, mu, sigma), 1)) %&gt;% \n  arrange(x)\n\nThe data simulated by this process are:\n\n6.9, 7.7, 8.1, 10.8, 13.5\n\n\nFind the mean of these numbers. \nWhat are some ways to interpret the mean?\n\nThe variance and standard deviation are individual numbers that summarize how far the data are from the mean. We first compute the deviations from the mean, \\(x - \\bar x\\). This is the directed distance from the mean to each data point.\n\n\nWarning in geom_text(aes(x = upper, y = -1, label = \"x\"), size = 4, hjust = -1, : All aesthetics have length 1, but the data has 5 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in geom_segment(aes(x = mean_x, xend = mean_x, y = -0.75, yend = n + : All aesthetics have length 1, but the data has 5 rows.\nℹ Please consider using `annotate()` or provide this layer with data containing\n  a single row.\n\n\nWarning in is.na(x): is.na() applied to non-(list or vector) of type\n'expression'\n\n\n\n\n\n\n\n\n\nWe can summarize this information in a table:\n\n\n\n\n\nx\nx - mean(x)\nWorkspace for your use\n\n\n\n\n6.9\n-2.5\n\n\n\n7.7\n-1.7\n\n\n\n8.1\n-1.3\n\n\n\n10.8\n1.4\n\n\n\n13.5\n4.1\n\n\n\n\n\n\n\n\nHow can we obtain one number that summarizes how spread out the data are from the mean? We may try averaging the deviations from the mean.\n\nWhat is the average deviation from the mean?\nWill we get the same value with other data sets, or is this just a coincidence?\nWhat could you do to prevent this from happening?\nApply your idea. Compute the resulting value that summarizes the spread. What do you get?\nWhat is the relationship between the sample variance and the sample standard deviation?\nUse the table above to verify that the sample variance is 7.4.\nShow that the sample standard deviation is 2.7203."
  },
  {
    "objectID": "class_activities/chapter_2_lesson_1_class_activity.html#class-activity-covariance-and-correlation-15-mins",
    "href": "class_activities/chapter_2_lesson_1_class_activity.html#class-activity-covariance-and-correlation-15-mins",
    "title": "Chapter 2: Correlation",
    "section": "Class Activity: Covariance and Correlation (15 mins)",
    "text": "Class Activity: Covariance and Correlation (15 mins)\n\n\n\n\n\n\nCaution\n\n\n\nCould we make this into a shiny app? Students could use a slider or something to choose the values of the following:\n\n# n &lt;- 50             # number of points\n# mu &lt;- c(1.25, 2.5)  # mean vector (mu_x, mu_y)\n# sigma_x &lt;- 1        # standard deviation x\n# sigma_y &lt;- 3        # standard deviation y\n# rho &lt;- 0.8          # correlation coefficient\n\nThe table and scatterplot would update to match the values input into the app.\n\n\n\nNote (use shiny live)\n \n#| standalone: true\n#| viewerHeight: 600\nlibrary(shiny)\nlibrary(bslib)\n\n# Define UI for app that draws a histogram ----\nui &lt;- page_sidebar(\n  sidebar = sidebar(open = \"open\",\n    numericInput(\"n\", \"Sample count\", 100),\n    checkboxInput(\"pause\", \"Pause\", FALSE),\n  ),\n  plotOutput(\"plot\", width=1100)\n)\n\nserver &lt;- function(input, output, session) {\n  data &lt;- reactive({\n    input$resample\n    if (!isTRUE(input$pause)) {\n      invalidateLater(1000)\n    }\n    rnorm(input$n)\n  })\n  \n  output$plot &lt;- renderPlot({\n    hist(data(),\n      breaks = 40,\n      xlim = c(-2, 2),\n      ylim = c(0, 1),\n      lty = \"blank\",\n      xlab = \"value\",\n      freq = FALSE,\n      main = \"\"\n    )\n    \n    x &lt;- seq(from = -2, to = 2, length.out = 500)\n    y &lt;- dnorm(x)\n    lines(x, y, lwd=1.5)\n    \n    lwd &lt;- 5\n    abline(v=0, col=\"red\", lwd=lwd, lty=2)\n    abline(v=mean(data()), col=\"blue\", lwd=lwd, lty=1)\n\n    legend(legend = c(\"Normal\", \"Mean\", \"Sample mean\"),\n      col = c(\"black\", \"red\", \"blue\"),\n      lty = c(1, 2, 1),\n      lwd = c(1, lwd, lwd),\n      x = 1,\n      y = 0.9\n    )\n  }, res=140)\n}\n\n# Create Shiny app ----\nshinyApp(ui = ui, server = server)\nThe following code simulates \\((x,y)\\)-pairs of random variables.\n\n# Unset random seed\nset.seed(Sys.time())\n\n# Specify means and correlation coefficient\nn &lt;- 50             # number of points\nmu &lt;- c(1.25, 2.5)  # mean vector (mu_x, mu_y)\nsigma_x &lt;- 1        # standard deviation x\nsigma_y &lt;- 3        # standard deviation y\nrho &lt;- 0.8          # correlation coefficient\n\n# Define variance-covariance matrix\nsigma &lt;- matrix(\n  c(sigma_x^2,\n    rho*sigma_x*sigma_y,\n    rho*sigma_x*sigma_y,\n    sigma_y^2),\n  nrow = 2)\n\n# Simulate bivariate normal data\nmvn_data &lt;- MASS::mvrnorm(n, mu, sigma) %&gt;% \n  data.frame() %&gt;% \n  rename(x = X1, y = X2)\n\nThe following table illustrates some of the simulated values. The mean of the \\(x\\) values is \\(\\bar x = 1.252\\). The mean of the \\(y\\) values is \\(\\bar y =2.548\\). We will soon use the values \\((x-\\bar x)\\), \\((x-\\bar x)^2\\), \\((y-\\bar y)\\), \\((y-\\bar y)^2\\), and \\((x-\\bar x)(y-\\bar y)\\). For convenience, they are included in the table below.\n\n\n\n\n\ni\nx\ny\nx-mean(x)\n(x-mean(x))^2\ny-mean(y)\n(y-mean(y))^2\n(x-mean(x))(y-mean(y))\n\n\n\n\n1\n1.626\n6.923\n0.374\n0.14\n4.375\n19.139\n1.635\n\n\n2\n-1.237\n-5.53\n-2.49\n6.199\n-8.078\n65.252\n20.112\n\n\n3\n0.787\n1.496\n-0.466\n0.217\n-1.052\n1.107\n0.49\n\n\n4\n0.509\n0.533\n-0.743\n0.552\n-2.015\n4.061\n1.497\n\n\n5\n1.081\n3.023\n-0.171\n0.029\n0.474\n0.225\n-0.081\n\n\n6\n1.737\n8.001\n0.485\n0.235\n5.453\n29.735\n2.642\n\n\n:\n:\n:\n:\n:\n:\n:\n:\n\n\n:\n:\n:\n:\n:\n:\n:\n:\n\n\n48\n1.848\n5.108\n0.596\n0.355\n2.559\n6.551\n1.525\n\n\n49\n1.274\n1.731\n0.022\n0\n-0.818\n0.668\n-0.018\n\n\n50\n2.169\n3.357\n0.917\n0.841\n0.808\n0.653\n0.741\n\n\nsum\n62.623\n127.418\n0\n48.467\n0\n546.559\n138.758\n\n\n\n\n\n\n\nThe simulated values are plotted below, with vertical lines drawn at \\(x = \\bar x\\) and \\(y = \\bar y\\). The first simulated point \\((i=1)\\) is circled.\n\n\n\n\n\n\n\n\n\nIf the quantity \\((x-\\bar x)(y-\\bar y)\\) is greater than zero, the points are colored blue. Otherwise, they are colored orange.\n\nWhat color are the points if \\((x-\\bar x)\\) and \\((y-\\bar y)\\) have the same sign?\nWhat color are the points if \\((x-\\bar x)\\) and \\((y-\\bar y)\\) have different signs?\n\nTo compute the sample covariance, we divide the sum of the \\((x - \\bar x)(y - \\bar y)\\) values by \\(n-1\\):\n\\[\nCov(x,y)\n=\n\\frac{\\sum\\limits_{i=1}^n (x - \\bar x)(y - \\bar y)}{n-1}\n=\n\\frac{138.758}{50 - 1}\n=\n2.832\n\\]\nYou can think of this as an “average” of the \\((x - \\bar x)(y - \\bar y)\\) values. The only difference is that we divide by \\(n-1\\) instead of \\(n\\).\n\nIf there are more blue points than orange points, what should the sign of the sample covariance be? Why?\nWhat does the sample covariance tell us?\n\nThe sample covariance is related to the sample standard deviation of \\(x\\) and \\(y\\) and the sample correlation coefficient between \\(x\\) and \\(y\\).\nThe sample standard deviations are:\n\\[\n\\begin{align*}\ns_x &= \\sqrt{ \\frac{\\sum\\limits_{i=1}^n (x - \\bar x)^2}{n-1} }\n=\n\\sqrt{\n\\frac{\n48.467\n}{\n50-1\n}\n}\n=\n0.995\n\\\\\ns_y &= \\sqrt{ \\frac{\\sum\\limits_{i=1}^n (y - \\bar y)^2}{n-1} }\n=\n\\sqrt{\n\\frac{\n546.559\n}{\n50-1\n}\n}\n=\n3.34\n\\end{align*}\n\\]\nThe sample correlation coefficient is: \\[\nr = \\frac{\\sum\\limits_{i=1}^n (x - \\bar x)(y - \\bar y)}{\\sqrt{\\sum\\limits_{i=1}^n (x - \\bar x)^2} \\sqrt{\\sum\\limits_{i=1}^n (y - \\bar y)^2}}\n=\n\\frac{\n138.758\n}{\n\\sqrt{ 48.467}\n\\sqrt{ 546.559}\n}\n=\n0.853\n\\]\n\nWhat do you get if you multiply the equations for \\(r\\), \\(s_x\\), and \\(s_y\\) together? \\[\n\\begin{align*}\n  r \\cdot s_x \\cdot s_y\n  &=\n  \\frac{\\sum\\limits_{i=1}^n (x - \\bar x)(y - \\bar y)}{\\sqrt{\\sum\\limits_{i=1}^n (x - \\bar x)^2} \\sqrt{\\sum\\limits_{i=1}^n (y - \\bar y)^2}}\n  \\cdot\n  \\sqrt{ \\frac{\\sum\\limits_{i=1}^n (x - \\bar x)^2}{n-1} }\n  \\cdot\n  \\sqrt{ \\frac{\\sum\\limits_{i=1}^n (y - \\bar y)^2}{n-1} }\n  \\\\\n  ~&~\\\\\n  &=\\\\\n  ~&~\\\\\n\\end{align*}\n\\]\nUse the numerical values above to confirm your result. Any discrepancy is due to roundoff error."
  },
  {
    "objectID": "class_activities/chapter_2_lesson_1_class_activity.html#team-activity-computational-practice-15-mins",
    "href": "class_activities/chapter_2_lesson_1_class_activity.html#team-activity-computational-practice-15-mins",
    "title": "Chapter 2: Correlation",
    "section": "Team Activity: Computational Practice (15 mins)",
    "text": "Team Activity: Computational Practice (15 mins)\nWith your assigned partner, compute the following values for the \\(n=6\\) values given in the table below:\n\n\\(\\bar x =\\)\n\\(\\bar y =\\)\n\\(s_x =\\)\n\\(s_y =\\)\n\\(r =\\)\n\\(Cov(x,y) =\\)\n\n\n\n\n\n\ni\nx\ny\nx-mean(x)\n(x-mean(x))^2\ny-mean(y)\n(y-mean(y))^2\n(x-mean(x))(y-mean(y))\n\n\n\n\n1\n-2.1\n2.8\n-1.9\n3.61\n1\n1\n-1.9\n\n\n2\n-0.2\n2.2\n\n\n\n\n\n\n\n3\n0.8\n0.9\n\n\n\n\n\n\n\n4\n0.4\n2\n\n\n\n\n\n\n\n5\n2.3\n-1\n\n\n\n\n\n\n\n6\n-2.4\n3.9\n\n\n\n\n\n\n\nsum\n-1.2\n10.8"
  },
  {
    "objectID": "class_activities/chapter_2_lesson_1_class_activity.html#recap-5-min",
    "href": "class_activities/chapter_2_lesson_1_class_activity.html#recap-5-min",
    "title": "Chapter 2: Correlation",
    "section": "Recap (5 min)",
    "text": "Recap (5 min)\nWorking with your partner, prepare to explain the following concepts to the class:\n\nVariance\nStandard deviation\nCorrelation\nCovariance"
  },
  {
    "objectID": "data/tools for obtaining data/download_closing_price_data_for_any_stock.html",
    "href": "data/tools for obtaining data/download_closing_price_data_for_any_stock.html",
    "title": "Testing Code",
    "section": "",
    "text": "This report provides a comprehensive analysis of Netflix (NFLX) stock prices from January 16, 2020, to January 17, 2024. The analysis utilizes various R packages for time series analysis, visualization, and forecasting.\nPackages Used:\n\ntsibble: for handling time series data.\nfable: for time series modeling and forecasting.\nfeasts: for feature extraction and seasonal decomposition.\ntsibbledata: for accessing example time series data.\nfable.prophet: for time series forecasting with Prophet models.\ntidyverse: for data manipulation and visualization.\npatchwork: for combining multiple plots.\nrio: for importing and exporting data.\narrow: for efficient data frame handling.\ntidyquant: for financial analysis.\nlubridate: for date and time manipulation.\ndygraphs: for interactive time series plots.\nriem: for interactive exploratory modeling.\nscales: for controlling plot scales.\nplotly: for interactive visualizations."
  },
  {
    "objectID": "data/tools for obtaining data/download_closing_price_data_for_any_stock.html#overview",
    "href": "data/tools for obtaining data/download_closing_price_data_for_any_stock.html#overview",
    "title": "Testing Code",
    "section": "",
    "text": "This report provides a comprehensive analysis of Netflix (NFLX) stock prices from January 16, 2020, to January 17, 2024. The analysis utilizes various R packages for time series analysis, visualization, and forecasting.\nPackages Used:\n\ntsibble: for handling time series data.\nfable: for time series modeling and forecasting.\nfeasts: for feature extraction and seasonal decomposition.\ntsibbledata: for accessing example time series data.\nfable.prophet: for time series forecasting with Prophet models.\ntidyverse: for data manipulation and visualization.\npatchwork: for combining multiple plots.\nrio: for importing and exporting data.\narrow: for efficient data frame handling.\ntidyquant: for financial analysis.\nlubridate: for date and time manipulation.\ndygraphs: for interactive time series plots.\nriem: for interactive exploratory modeling.\nscales: for controlling plot scales.\nplotly: for interactive visualizations."
  },
  {
    "objectID": "data/tools for obtaining data/download_closing_price_data_for_any_stock.html#data-retrieval",
    "href": "data/tools for obtaining data/download_closing_price_data_for_any_stock.html#data-retrieval",
    "title": "Testing Code",
    "section": "Data Retrieval:",
    "text": "Data Retrieval:\nThe stock prices of x can be fetched from the specified date range using the tq_get function.\n\n# Set symbol and date range\nsymbol &lt;- \"MCD\"\ndate_start &lt;- \"2021-01-01\"\ndate_end &lt;- \"2024-01-01\"\n\n# Fetch stock prices\ndf_stock &lt;- tq_get(symbol, from = date_start, to = date_end, get = \"stock.prices\")\n\n\n## Data Transformation:\n# The retrieved data was transformed into a tibble format and further converted into a # tsibble, a specialized time series tibble, for easier handling and analysis.\n\n# Transform data into tibble\ndf_tsibble &lt;- df_stock %&gt;%\n  mutate(\n    dates = date, \n    value = adjusted\n  ) %&gt;%\n  select(dates, value) %&gt;%\n  as_tsibble(index = dates, key = NULL) %&gt;% \n  arrange(dates)\n\n## Time Series Plot:\n# A time series plot was generated using plot_ly to visualize the trend in Netflix stock prices over the specified period. The plot provides insights into the historical performance of the stock chosen.\n\n# Generate time series plot using plot_ly\nplot_ly(df_tsibble, x = ~dates, y = ~value, type = 'scatter', mode = 'lines') %&gt;%\n  layout(\n    xaxis = list(title = \"Date\"),\n    yaxis = list(title = \"Value\"),\n    title = paste0(\"Time Plot of \", symbol, \" Daily Closing Price\")\n  )\n\n\n\n\n\n\ndf_tsibble &lt;- df_tsibble |&gt;\n  mutate(diff = value - lag(value))\n\n# Generate time series plot using plot_ly\nplot_ly(df_tsibble, x = ~dates, y = ~diff, type = 'scatter', mode = 'lines') %&gt;%\n  layout(\n    xaxis = list(title = \"Date\"),\n    yaxis = list(title = \"Value\"),\n    title = paste0(\"Difference of \", symbol, \" Daily Closing Price\")\n  )\n\n\n\n\n\n\nautoplot(df_tsibble, .vars = value) +  \n  labs(\n    x = \"\",\n    y = \"Closign Price point\",\n    title = \"Time Series Plot\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\n\ndf_tsibble &lt;- df_tsibble |&gt;\n  mutate(month = month(dates))\nplot_ly(df_tsibble, x = ~factor(month), y = ~value, type = 'box') %&gt;%\n  layout(\n    xaxis = list(title = \"\"),\n    yaxis = list(title = \"Value\"),\n    title = \"Boxplot of Time Series Data by Month\"\n  )\n\n\n\n\n\n\nggplot(df_tsibble, aes(x = factor(month), y = value)) +\n    geom_boxplot() +\n  labs(\n    x = \"Month Number\",\n    y = \"CLosing Price\",\n    title = \"Boxplots of Closing Price Stock by Month\"\n  ) +\n  theme(plot.title = element_text(hjust = 0.5))"
  },
  {
    "objectID": "outcomes.html",
    "href": "outcomes.html",
    "title": "Course Outcomes",
    "section": "",
    "text": "Introduce the course structure and syllabus\n\n\nGet to know each other\nDescribe key concepts in time series analysis\nExplore an example time series interactively\n\n\n\n\n\n\n\nUse technical language to describe the main features of time series data\n\n\nDefine time series analysis\nDefine time series\nDefine sampling interval\nDefine serial dependence or autocorrelation\nDefine a time series trend\nDefine seasonal variation\nDefine cycle\nDifferentiate between deterministic and stochastic trends\n\n\n\n\nPlot time series data to visualize trends, seasonal patterns, and potential outliers\n\n\nPlot a “ts” object\nPlot the estimated trend of a time series by computing the mean across one full period\n\n\n\n\n\n\n\nDecompose time series into trends, seasonal variation, and residuals\n\n\nDefine smoothing\nCompute the centered moving average for a time series\nEstimate the trend component using moving averages\n\n\n\n\nPlot time series data to visualize trends, seasonal patterns, and potential outliers\n\n\nPlot the estimated trend of a time series using a moving average\nMake box plots to examine seasonality\nInterpret the trend and seasonal pattern observed in a time series\n\n\n\n\n\n\n\nUse R to describe key features of time series data\n\n\nImport CSV data and convert to tsibble format\n\n\n\n\nDecompose time series into trends, seasonal variation, and residuals\n\n\nImplement additive decomposition\nExplain how to remove seasonal variation using an estimate for seasonal component of a time series\nCompute the estimators of seasonal variation for an additive model\nCalculate the random component for an additive model\nCompute a seasonally-adjusted time series based on an additive model\n\n\n\n\n\n\n\nDecompose time series into trends, seasonal variation, and residuals\n\n\nExplain the differences between additive and multiplicative models\nImplement multiplicative decomposition\nCompute the estimators of seasonal variation for a multiplicative model\nCalculate the random component for a multiplicative model\nCompute a seasonally-adjusted time series based on a multiplicative model"
  },
  {
    "objectID": "outcomes.html#chapter-1",
    "href": "outcomes.html#chapter-1",
    "title": "Course Outcomes",
    "section": "",
    "text": "Introduce the course structure and syllabus\n\n\nGet to know each other\nDescribe key concepts in time series analysis\nExplore an example time series interactively\n\n\n\n\n\n\n\nUse technical language to describe the main features of time series data\n\n\nDefine time series analysis\nDefine time series\nDefine sampling interval\nDefine serial dependence or autocorrelation\nDefine a time series trend\nDefine seasonal variation\nDefine cycle\nDifferentiate between deterministic and stochastic trends\n\n\n\n\nPlot time series data to visualize trends, seasonal patterns, and potential outliers\n\n\nPlot a “ts” object\nPlot the estimated trend of a time series by computing the mean across one full period\n\n\n\n\n\n\n\nDecompose time series into trends, seasonal variation, and residuals\n\n\nDefine smoothing\nCompute the centered moving average for a time series\nEstimate the trend component using moving averages\n\n\n\n\nPlot time series data to visualize trends, seasonal patterns, and potential outliers\n\n\nPlot the estimated trend of a time series using a moving average\nMake box plots to examine seasonality\nInterpret the trend and seasonal pattern observed in a time series\n\n\n\n\n\n\n\nUse R to describe key features of time series data\n\n\nImport CSV data and convert to tsibble format\n\n\n\n\nDecompose time series into trends, seasonal variation, and residuals\n\n\nImplement additive decomposition\nExplain how to remove seasonal variation using an estimate for seasonal component of a time series\nCompute the estimators of seasonal variation for an additive model\nCalculate the random component for an additive model\nCompute a seasonally-adjusted time series based on an additive model\n\n\n\n\n\n\n\nDecompose time series into trends, seasonal variation, and residuals\n\n\nExplain the differences between additive and multiplicative models\nImplement multiplicative decomposition\nCompute the estimators of seasonal variation for a multiplicative model\nCalculate the random component for a multiplicative model\nCompute a seasonally-adjusted time series based on a multiplicative model"
  },
  {
    "objectID": "outcomes.html#chapter-2",
    "href": "outcomes.html#chapter-2",
    "title": "Course Outcomes",
    "section": "Chapter 2",
    "text": "Chapter 2\n\nLesson 1\n\n\nCompute the key statistics used to describe the linear relationship between two variables\n\n\nCompute the sample mean\nCompute the sample variance\nCompute the sample standard deviation\nCompute the sample covariance\nCompute the sample correlation coefficient\nExplain sample covariance using a scatter plot\n\n\n\n\nInterpret the key statistics used to describe sample data\n\n\nInterpret the sample mean\nInterpret the sample variance\nInterpret the sample standard deviation\nInterpret the sample covariance\nInterpret the sample correlation coefficient\n\n\n\n\nLesson 2\n\n\nDefine key terms in time series analysis\n\n\nDefine the ensemble of a time series\nDefine the expected value (or mean function) of a time series model\nDefine the sample estimate of the population mean of a time series\nDefine the variance function of a time series model\nState the constant variance estimator for a time series model\nExplain the stationarity assumption\nExplain the stationary variance assumption\nDefine lag\nDefine autocorrelation\nDefine the second-order stationary time series\nExplain the autocovariance function in Equation (2.11)\nExplain the lag k autocorrelation function in Equation (2.12)\nDefine the autocovariance function, acvf\nDefine the sample autocorrelation function, acf\n\n\n\n\nCalculate sample estimates of autocovariance and autocorrelation functions from time series data\n\n\nDefine the sample autocovariance function, c_k\nDefine the sample autocorrelation function, r_k\n\n\n\n\nLesson 3\n\n\nExplain the theoretical implications of autocorrelation for the estimation of time series statistics\n\n\nExplain how positive autocorrelation leads to underestimation of variance in short time series\nExplain how negative autocorrelation can improve efficiency of sample mean estimate\n\n\n\n\nInterpret correlograms to identify significant lags, correlations, trends, and seasonality\n\n\nCreate a correlogram\nInterpret a correlogram\nDefine a sampling distribution\nState the sampling distribution of rk\nExplain the concept of a confidence interval\nConduct a single hypothesis test using a correlogram\nDescribe the problems associated with multiple hypothesis testing in a correlogram\nDifferentiate statistical and practical significance\nDiagnose non-stationarity using a correlogram"
  },
  {
    "objectID": "outcomes.html#chapter-3",
    "href": "outcomes.html#chapter-3",
    "title": "Course Outcomes",
    "section": "Chapter 3",
    "text": "Chapter 3\n\nLesson 1\n\n\nExplain the purpose and limitations of forecasting\n\n\nDefine lead time\nDefine forecasting\nDifferentiate causation from correlation\n\n\n\n\nExplain why there is not one correct model to describe a time series\n\n\nExplain why there can be several suitable models for a given time series\n\n\n\n\nUse cross-correlation analysis to quantify lead/lag relationships\n\n\nExplain forecasting by leading indicators\nDefine the population k-lag ccvf\nDefine the population k-lag ccf\nDefine the sample k-lag ccvf\nDefine the sample k-lag ccf\nEstimate an ccf for two time series\nInterpret whether a variable is a leading indicator using a cross-correlogram\n\n\n\n\nEvaluate the limitations of forecasting models based on past trends\n\n\nExplain how unexpected future events may invalidate forecast trends\nAvoid over-extrapolation of fitted trends beyond reasonable time horizons\n\n\n\n\nLesson 2\n\n\nImplement simple exponential smoothing to estimate local mean levels\n\n\nExplain forecasting by extrapolation\nState the assumptions of exponential smoothing\nDefine exponential weighted moving average (EWMA)\nState the exponential smoothing forecasting equation\nState the EWMA in geometric series form (in terms of x_t only Eq 3.18)\nExplain the EWMA intuitively\nDefine the one-step-ahead prediction error (1PE)\nState the SS1PE used to estimate the smoothing parameter of a EWMA\nIndicate when the EWMA smoothing parameter is optimally set as 1/n\n\n\n\n\nLesson 3\n\n\nImplement the Holt-Winter method to forecast time series\n\n\nJustify the need for the Holt-Winters method\nDescribe how to obtain initial parameters for the Holt-Winters algorithm\nExplain the Holt-Winters update equations for additive decomposition models\nExplain the purpose of the parameters \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\)\nInterpret the coefficient estimates \\(a_t\\), \\(b_t\\), and \\(s_t\\) of the Holt-Winters algorithm\nExplain the Holt-Winters forecasting equation for additive decomposition models, Equation (3.22)\n\n\n\n\nLesson 4\n\n\nImplement the Holt-Winter method to forecast time series\n\n\nCompute the Holt-Winters estimate by hand\nUse HoltWinters() to forecast additive model time series\nPlot the Holt-Winters decomposition of a time series (see Fig 3.10)\nPlot the Holt-Winters fitted values versus the original time series (see Fig 3.11)\nSuperimpose plots of the Holt-Winters predictions with the time series realizations (see Fig 3.13)\n\n\n\n\nLesson 5\n\n\nImplement the Holt-Winter method to forecast time series\n\n\nExplain the Holt-Winters method equations for multiplicative decomposition models\nExplain the purpose of the paramters \\(\\alpha\\), \\(\\beta\\), and \\(\\gamma\\)\nInterpret the coefficient estimates \\(a_t\\), \\(b_t\\), and \\(s_t\\) of the Holt-Winters smoothing algorithm\nExplain the Holt-Winters forecasting equation for multiplicative decomposition models, Equation (3.23)\nUse HoltWinters() to forecast multiplicative model time series\nPlot the Holt-Winters decomposition of a TS (see Fig 3.10)\nPlot the Holt-Winters fitted values versus the original time series (see Fig 3.11)\nSuperimpose plots of the Holt-Winters predictions with the time series realizations (see Fig 3.13)"
  },
  {
    "objectID": "outcomes.html#chapter-4",
    "href": "outcomes.html#chapter-4",
    "title": "Course Outcomes",
    "section": "Chapter 4",
    "text": "Chapter 4\n\nLesson 1\n\n\nCharacterize the properties of discrete white noise\n\n\nDefine Residual error\nDefine discrete white noise (DWN)\nDefine Gaussian white noise\nSimulate Gaussian white noise with R\nPlot DWN simulation results\nState DWN second order properties\nExplain how to estimate (or fit) a DWN process\nState the assumptions needed to categorize residual error series as white noise\n\n\n\n\nCharacterize the properties of a random walk\n\n\nDefine a random walk\n\n\n\n\nSimulate realizations from basic time series models in R\n\n\nSimulate a random walk\nPlot a random walk\n\n\n\n\nLesson 2\n\n\nCharacterize the properties of a random walk\n\n\nDefine the second order properties of a random walk\nDefine the backward shift operator\nUse the backward shift operator to state a random walk as a sequence of white noise realizations\nDefine a random walk with drift\n\n\n\n\nSimulate realizations from basic time series models in R\n\n\nSimulate a random walk\nPlot a random walk\n\n\n\n\nFit time series models to data and interpret fitted parameters\n\n\nMotive the need for differencing in time series analysis\nDefine the difference operator\nExplain the relationship between the difference operator and the backward shift operator\nTest whether a series is a random walk using first differences\nExplain how to estimate a random walk with increasing slope using Holt-Winters\nEstimate the drift parameter of a random walk\n\n\n\n\nLesson 3\n\n\nCharacterize the properties of an \\(AR(p)\\) stochastic process\n\n\nDefine an \\(AR(p)\\) stochastic process\nExpress an \\(AR(p)\\) process using the backward shift operator\nState an \\(AR(p)\\) forecast (or prediction) function\nIdentify stationarity of an \\(AR(p)\\) process using the backward shift operator\nDetermine the stationarity of an \\(AR(p)\\) process using a characteristic equation\n\n\n\n\nCheck model adequacy using diagnostic plots like correlograms of residuals\n\n\nCharacterize a random walk’s second order characteristics using a correlogram\nDefine partial autocorrelations\nExplain how to use a partial correlogram to decide what model would be suitable to estimate an \\(AR(p)\\) process\nDemonstrate the use of partial correlogram via simulation\n\n\n\n\nLesson 4\n\n\nFit time series models to data and interpret fitted parameters\n\n\nFit an \\(AR(p)\\) model to simulated data\nExplain the difference between parameters of the data generating process and estimates\nCalculate confidence intervals for AR coefficient estimates\nInterpret AR coefficient estimates in the context of the source and nature of historical data\n\n\n\n\nCheck model adequacy using diagnostic plots like correlograms of residuals\n\n\nCompare AR fitted models to an underlying data generating process\nExplain the limitations of stochastic model fitting as evidence in favor or against real world arguments."
  },
  {
    "objectID": "outcomes.html#chapter-5",
    "href": "outcomes.html#chapter-5",
    "title": "Course Outcomes",
    "section": "Chapter 5",
    "text": "Chapter 5\n\nLesson 1\n\n\nExplain the difference between stochastic and deterministic trends in time series\n\n\nDescribe deterministic trends as smooth, predictable changes over time\nDefine stochastic trends as random, unpredictable fluctuations\nExplain the different treatment of stochastic and deterministic trends when forecasting\n\n\n\n\nFit linear regression models to time series data\n\n\nDefine a linear time series model\nExplain why ordinary linear regression systematically underestimates of the standard error of parameter estimates when the error terms are autocorrelated\nApply generalized least squares GLS in R to estimate linear regression model parameters\nExplain how to estimate the autocorrelation input for the GLS algorithm\nCompare GLS and OLS standard error estimates to evaluate autocorrelation bias\nIdentify an appropriate function to model the trend in a given time series\nRepresent seasonal factors in a regression model using indicator variables\nFit a linear model for a simulated time series with linear time trend and \\(AR(p)\\) error\nUse acf and pacf to test for autocorrelation in the residuals\nEstimate a seasonal indicator model using GLS\nForecast using a fitted GLS model with seasonal indicator variables\n\n\n\n\nApply differencing to nonstationary time series\n\n\nTransform a non-stationary linear to a stationary process using differencing\nState how to remove a polynomial trend of order \\(m\\)\n\n\n\n\nSimulate time series\n\n\nSimulate a time series with a linear time trend and a \\(AR(p)\\) error\n\n\n\n\nLesson 2\n\n\nFit linear regression models to time series data\n\n\nDescribe a Fourier series\nExplain how a few terms in a Fourier series can be used to fit a seasonal component\nMotivate the use of the harmonic seasonal model\nRepresent seasonal factors using harmonic seasonal terms\n\n\n\n\nLesson 3\n\n\nFit linear regression models to time series data\n\n\nState the additive model with harmonic seasonal component\nSimulate a time series with harmonic seasonal components\nIdentify an appropriate function to model the trend in a given time series\nIdentify a parsimonious set of harmonic terms for use in a regression model\nFit the additive model with harmonic seasonal component to real-world data\nEvaluate residuals using a correlogram and partial correlogram to ensure they meet the assumptions\n\n\n\n\nApply model selection criteria\n\n\nUse AIC to aid in model selection\n\n\n\n\nLesson 4\n\n\nApply logarithmic transformations to time series\n\n\nExplain when to use a log-transformation\nEstimate a harmonic seasonal model using GLS with a log-transformed series\nExplain how to use logarithms to linearize certain non-linear trends\n\n\n\n\nApply non-linear models to time series\n\n\nExplain when to use non-linear models\nSimulate a time series with an exponential trend\nFit a time series model with an exponential trend\n\n\n\n\n\nLesson 5\n\n\nApply logarithmic transformations to time series\n\n\nApply a log-transformation to a multiplicative time series\n\n\n\n\nApply the bias correction factor for inverse transformations\n\n\nState the bias correction procedure for log-transform estimates \nExplain when to use the bias correction factor\nUse the bias correction factor for a log-transform model\nForecast using the inverse-transform and bias correction of a log-transformed model"
  },
  {
    "objectID": "outcomes.html#chapter-6",
    "href": "outcomes.html#chapter-6",
    "title": "Course Outcomes",
    "section": "Chapter 6",
    "text": "Chapter 6\n\nLesson 1\n\n\nCharacterize the properties of moving average (MA) models\n\n\nDefine a moving average (MA) process\nWrite an MA(q) model in terms of the backward shift operator\nState the mean and variance of an MA(q) process\nExplain the autocorrelation function of an MA(q) process\nDefine an invertible MA process\n\n\n\n\nFit time series models to data and interpret fitted parameters\n\n\nDetermine an appropriate MA(q) model to fit to a time series based on the ACF plot\nFit an MA(q) model to data in R using the arima() function\nAssess model fit by examining residual diagnostic plots\nInterpret the fitted MA coefficients\n\n\n\n\nLesson 2\n\n\nDefine autoregressive moving average (ARMA) models\n\n\nWrite the equation for an ARMA(p,q) model\nExpress an ARMA model in terms of the backward shift operators for the AR and MA components\nState facts about ARMA processes related to stationarity, invertibility, special cases, parsimony, and parameter redundancy\nUse ACF and PACF plots to determine if an AR, MA or ARMA model is appropriate for a time series\n\n\n\n\nApply an iterative time series modeling process\n\n\nFit a regression model to capture trend and seasonality\nExamine residual diagnostic plots to assess autocorrelation\nFit an ARMA model to the residuals if needed\nCheck the residuals of the ARMA model for white noise\nForecast the original series by combining the regression and ARMA model forecasts"
  },
  {
    "objectID": "outcomes.html#chapter-7",
    "href": "outcomes.html#chapter-7",
    "title": "Course Outcomes",
    "section": "Chapter 7",
    "text": "Chapter 7\n\nLesson 1\n\n\nExplain the concept of non-stationarity in time series\n\n\nDefine non-stationarity and its implications for time series analysis\nIdentify non-stationary behavior in time series plots\n\n\n\n\nApply differencing to remove non-stationarity\n\n\nExplain the concept of differencing and its role in removing non-stationarity\nUse differencing to transform non-stationary time series into stationary ones\nInterpret the results of differencing on time series plots and ACF/PACF\n\n\n\n\nIdentify integrated models and ARIMA notation\n\n\nDefine integrated models and their relationship to differencing\nUnderstand the ARIMA notation and its components (p, d, q)\nRecognize the role of the ‘d’ parameter in ARIMA models\n\n\n\n\nLesson 2\n\n\nIdentify seasonal ARIMA models\n\n\nDefine seasonal ARIMA models and their notation (p, d, q)(P, D, Q)[m]\nIdentify the need for seasonal ARIMA models in time series with seasonal patterns\n\n\n\n\nApply the fitting procedure for seasonal ARIMA models\n\n\nDescribe the steps involved in fitting seasonal ARIMA models\nDetermine the appropriate order of differencing (d and D) based on ACF/PACF plots\nSelect the order of AR and MA terms (p, q, P, Q) using ACF/PACF plots and model selection criteria\n\n\n\n\nFit seasonal ARIMA models to time series data using R\n\n\nUse R to fit seasonal ARIMA models\nInterpret the output for the ARIMA models, including coefficients and model diagnostics\nForecast future values using the fitted seasonal ARIMA model"
  },
  {
    "objectID": "resources/lubridate/lubridate.html",
    "href": "resources/lubridate/lubridate.html",
    "title": "Lubridate",
    "section": "",
    "text": "Conversion specification\nDescription\nExample\n\n\n%a\nAbbreviated weekday\nSun, Thu\n\n\n%A\nFull weekday\nSunday, Thursday\n\n\n%b or %h\nAbbreviated month\nMay, Jul\n\n\n%B\nFull month\nMay, July\n\n\n%d\nDay of the month\n01-31\n27, 07\n\n\n%j\nDay of the year\n001-366\n148, 188\n\n\n%m\nMonth\n01-12\n05, 07\n\n\n%U\nWeek\n01-53\nwith Sunday as first day of the week\n22, 27\n\n\n%w\nWeekday\n0-6\nSunday is 0\n0, 4\n\n\n%W\nWeek\n00-53\nwith Monday as first day of the week\n21, 27\n\n\n%x\nDate, locale-specific\n\n\n\n%y\nYear without century\n00-99\n84, 05\n\n\n%Y\nYear with century\non input:\n00 to 68 prefixed by 20\n69 to 99 prefixed by 19\n1984, 2005\n\n\n%C\nCentury\n19, 20\n\n\n%D\nDate formatted %m/%d/%y\n05/27/84, 07/07/05\n\n\n%u\nWeekday\n1-7\nMonday is 1\n7, 4\n\n\n%n\nNewline on output or\nArbitrary whitespace on input\n\n\n\n%t\nTab on output or\nArbitrary whitespace on input"
  },
  {
    "objectID": "resources/tidyverts/tidyverts.html",
    "href": "resources/tidyverts/tidyverts.html",
    "title": "Tidyverts",
    "section": "",
    "text": "Tidyverse Conversion\n\n\nTidyverts Documentation\n\ntsibble\nfable\nfeasts\ntsibbledata\nfable.prophet\nfable.binary\ntsibbletalk"
  },
  {
    "objectID": "chapter_5_lesson_4.html#class-activity-anti-log-transformation-and-bias-correction-on-simulated-data-10-min",
    "href": "chapter_5_lesson_4.html#class-activity-anti-log-transformation-and-bias-correction-on-simulated-data-10-min",
    "title": "Transformations, Forecasting adn Bias Correction",
    "section": "Class Activity: Anti-Log Transformation and Bias Correction on Simulated Data (10 min)",
    "text": "Class Activity: Anti-Log Transformation and Bias Correction on Simulated Data (10 min)\n\nForecasts for a Simulated Time Series\nWe can use the forecast() function to predict future values of this time series. Table 2 displays the output of the forecast() command. Note that the column labeled x_t (i.e. \\(x_t\\)), representing the time series is populated with information tied to a normal distribution. The mean and standard deviation specified are the estimated parameters for the distribution of the predicted values of \\(\\log(x_t)\\). If you raise \\(e\\) to the power of the mean, you get the values in the .mean column.\n\n\nShow the code\n# Fit model (OLS)\nsim_reduced_linear_lm1 &lt;- sim_ts |&gt;\n  model(sim_reduced_linear1 = TSLM(log(x_t) ~ std_t + \n        sin1 + cos1 + sin2 + cos2 + sin3 + cos3))\n\n# Compute forecast\nn_years_forecast &lt;- 5\nn_months_forecast &lt;- 12 * n_years_forecast\n\nnew_dat &lt;- tibble(t = n_months:(n_months + n_months_forecast )) |&gt;\n  mutate(\n    dates = seq(max(dates_seq), length.out=n_months_forecast + 1, by=\"1 month\") \n  ) |&gt;\n  mutate(\n    std_t = (t - mean(pull(sim_ts, t))) / sd(pull(sim_ts, t)),\n    sin1 = sin(2 * pi * 1 * t / 12),\n    cos1 = cos(2 * pi * 1 * t / 12),\n    sin2 = sin(2 * pi * 2 * t / 12),\n    cos2 = cos(2 * pi * 2 * t / 12),\n    sin3 = sin(2 * pi * 3 * t / 12),\n    cos3 = cos(2 * pi * 3 * t / 12),\n    sin4 = sin(2 * pi * 4 * t / 12),\n    cos4 = cos(2 * pi * 4 * t / 12),\n    sin5 = sin(2 * pi * 5 * t / 12),\n    cos5 = cos(2 * pi * 5 * t / 12),\n    cos6 = cos(2 * pi * 6 * t / 12)\n  ) |&gt;\n  as_tsibble(index = dates)\n\nsim_reduced_linear_lm1 |&gt; \n  forecast(new_data = new_dat)\n\n\n\n\n\n\nTable 2: Output of the forecast() command for the simulated time series\n\n\n\n\n\n\n.model\ndates\nx_t\n.mean\nt\nstd_t\nsin1\ncos1\nsin2\ncos2\nsin3\ncos3\n\n\n\n\n\nsim_reduced_linear1\n2024-12-01\nt(N(3.7, 0.00055))\n40.714\n108\n1.708\n0\n1\n0\n1\n0\n1\n...\n\n\nsim_reduced_linear1\n2025-01-01\nt(N(3.8, 0.00056))\n43.133\n109\n1.74\n0.5\n0.866\n0.866\n0.5\n1\n0\n...\n\n\nsim_reduced_linear1\n2025-02-01\nt(N(3.7, 0.00056))\n41.625\n110\n1.772\n0.866\n0.5\n0.866\n-0.5\n0\n-1\n...\n\n\nsim_reduced_linear1\n2025-03-01\nt(N(3.7, 0.00056))\n39.159\n111\n1.804\n1\n0\n0\n-1\n-1\n0\n...\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\nsim_reduced_linear1\n2029-11-01\nt(N(4.5, 6e-04))\n90.307\n167\n3.592\n-0.5\n0.866\n-0.866\n0.5\n-1\n0\n...\n\n\nsim_reduced_linear1\n2029-12-01\nt(N(4.6, 6e-04))\n101.132\n168\n3.624\n0\n1\n0\n1\n0\n1\n...\n\n\n\n\n\n\n\n\n\n\nFigure 8 illustrates the forecasted values for the time series.\n\n\nShow the code\nsim_forecast_plot_regular &lt;- sim_reduced_linear_lm1 |&gt; \n  forecast(new_data = new_dat) |&gt;\n  autoplot(sim_ts, level = 95) +\n  labs(\n    x = \"Month\",\n    y = \"x_t\",\n    title = \"Simulated Time Series\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"inset\") +\n  theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n\nsim_forecast_plot_logged &lt;- sim_reduced_linear_lm1 |&gt; \n  forecast(new_data = new_dat) |&gt;\n  autoplot(sim_ts, level = 95) +\n  scale_y_continuous(trans = \"log\", labels = trans_format(\"log\")) +\n  labs(\n    x = \"Month\",\n    y = \"log(x_t)\",\n    title = \"Logarithm of Simulated Time Series\"\n  ) +\n  theme_minimal() +\n  theme(legend.position = \"inset\")  +\n  theme(\n    plot.title = element_text(hjust = 0.5)\n  )\n\nsim_forecast_plot_regular\n\n\n\n\n\n\n\n\nFigure 8: Forecasted values of the time series with 95% confidence bands\n\n\n\n\n\n\n\nBias Correction\nThe forecasts presented above were computed by raising \\(e\\) to the power of the predicted log-values. Unfortunately, this introduces bias in the forecasted means. This bias tends to be large if the regression model does not fit the data closely.\nThe textbook points out that the bias correction should only be applied for means, not for simulated values. This means that if you are simulating transformed values, and you apply the inverse of your original transformation, the resulting values are appropriate.\nWhen we apply the inverse transform to the residual series, we introduce a bias.  We can account for this bias applying one of two adjustments to our mean values. The theory behind this transformations is alluded to in the textbook, but is not essential.\nThere are two common patterns observed in the residual series: (1) Gaussian white noise or (2) Non-Normal values.\nWe can use the skewness statistic to assess the shape of the residual series. When the skewness is less than -1 or greater than 1, we say that the distribution is highly skewed. For skewness values between -1 and -0.5 or between 0.5 and 1, we say there is moderate skewness. If skewness lies between -0.5 and 0.5, the distribution is considered roughly symmetric.\n\n\n\n\nLog-Normal Correction\n\nNormally-Distributed Residual Series\nIf the residual series follows a normal distribution, we multiply the means of the forecasted values \\(\\hat x_t\\) by the factor \\(e^{\\frac{1}{2} \\sigma^2}\\):\n\\[\n  \\hat x_t' = e^{\\frac{1}{2} \\sigma^2} \\cdot \\hat x_t\n\\]\nwhere \\(\\left\\{ \\hat x_t: t = 1, \\ldots, n \\right\\}\\) gives the values of the forecasted series, and \\(\\left\\{ \\hat x_t': t = 1, \\ldots, n \\right\\}\\) is the adjusted forecasted values.\n\n\n\n\n\n\nEmperical Correction\n\nNon-Normally Distributed Residual Series\nIf the residual series lacks normality , then we can adjust the forecasts \\(\\left\\{ \\hat x_t \\right\\}\\) as follows:\n\\[\n  \\hat x_t' = e^{\\widehat{\\log x_t}} \\sum_{t=1}^{n} \\frac{e^{z_t}}{n}\n\\]\nwhere \\(\\left\\{ \\widehat{\\log x_t}: t = 1, \\ldots, n \\right\\}\\) is the forecasted series obtained by fitting the log-regression model.\n\\(\\left\\{ z_t \\right\\}\\) is the residual series from this fitted model in the log-transformed units.\n\n\n\n\n\nThe code given below can be used to compute the corrected mean values for the simulated data.\n\n\nShow the code\nsim_model_values &lt;- sim_reduced_linear_lm1 |&gt;\n  glance()\n\nsim_model_check &lt;- sim_model_values |&gt;\n  mutate(\n    sigma = sqrt(sigma2),\n    lognorm_cf = exp((1/2) * sigma2),\n    empirical_cf = sim_reduced_linear_lm1 |&gt;\n      residuals() |&gt;\n      pull(.resid) |&gt;\n      exp() |&gt;\n      mean()) |&gt;\n  select(.model, r_squared, sigma2, sigma, lognorm_cf, empirical_cf)\n\nsim_pred &lt;- sim_reduced_linear_lm1 |&gt; \n  forecast(new_data = new_dat) |&gt;\n  mutate(.mean_correction = .mean * sim_model_check$empirical_cf) |&gt;\n  select(t, x_t, .mean, .mean_correction)\n\nsim_model_check\n\n\n# A tibble: 1 × 6\n  .model              r_squared   sigma2  sigma lognorm_cf empirical_cf\n  &lt;chr&gt;                   &lt;dbl&gt;    &lt;dbl&gt;  &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n1 sim_reduced_linear1     0.998 0.000507 0.0225       1.00         1.00\n\n\nFrom this, we observe that for the simulated data, \\(R^2 = 0.998\\). This indicates that the model explains a high proportion of the variation in the data. The log-normal adjustment is \\(1.00025\\), and the emperical adjustment is \\(1.00023\\). Both of these values are extremely close to 1, so they will have a negligible impact on the predicted values.\nThis result does not generalize. In other situations, there can be a substantial effect of this bias on the predicted means.\n\n\nHistogram of residuals\nFigure 9 gives a histogram of the residuals and compute the skewness of the residual series.\n\n\nShow the code\nsim_resid_df &lt;- sim_reduced_linear_lm1 |&gt; \n  residuals() |&gt; \n  as_tibble() |&gt; \n  dplyr::select(.resid) |&gt;\n  rename(x = .resid) \n  \nsim_resid_df |&gt;\n  mutate(density = dnorm(x, mean(sim_resid_df$x), sd(sim_resid_df$x))) |&gt;\n  ggplot(aes(x = x)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"#56B4E9\", binwidth = 0.01) +\n    geom_line(aes(x = x, y = density)) +\n    theme_bw() +\n    labs(\n      x = \"Values\",\n      y = \"Frequency\",\n      title = \"Histogram of Residuals from the Reduced Linear Model\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\nFigure 9: Histogram of the values in the residual series based on the model with a linear trend and seasonal Fourier terms where i≤3\n\n\n\n\n\nWe can use the command skewness(sim_resid_df$x) to compute the skewness of these residuals: -0.135. This number is close to zero (specifically between -0.5 and 0.5,) so we conclude that the residual series is approximately normally distributed. We can apply the log-normal correction to our mean forecast values.",
    "crumbs": [
      "Lesson 4",
      "Transformations, Forecasting adn Bias Correction"
    ]
  },
  {
    "objectID": "chapter_5_lesson_4.html#class-activity-apple-revenue-10-min",
    "href": "chapter_5_lesson_4.html#class-activity-apple-revenue-10-min",
    "title": "Transformations, Forecasting adn Bias Correction",
    "section": "Class Activity: Apple Revenue (10 min)",
    "text": "Class Activity: Apple Revenue (10 min)\nWe take another look at the quarterly revenue reported by Apple Inc. from Q1 of 2005 through Q1 of 2012\n\nVisualizing the Time Series\nFigure 10 gives the time plot illustrating the quarterly revenue reported by Apple from the first quarter of 2005 through the first quarter of 2012.\n\n\nShow the code\napple_raw &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/apple_revenue.csv\") |&gt;\n  mutate(dates = round_date(mdy(date), unit = \"quarter\")) |&gt;\n  arrange(dates)\n\napple_ts &lt;- apple_raw |&gt;\n  filter(dates &lt;= my(\"Jan 2012\")) |&gt;\n  dplyr::select(dates, revenue_billions) |&gt;\n  mutate(t = 1:n()) |&gt;\n  mutate(std_t = (t - mean(t)) / sd(t)) |&gt;\n  mutate(\n    sin1 = sin(2 * pi * 1 * t / 4),\n    cos1 = cos(2 * pi * 1 * t / 4),\n    cos2 = cos(2 * pi * 2 * t / 4)\n  ) |&gt;\n  as_tsibble(index = dates)\n\napple_plot_regular &lt;- apple_ts |&gt;\n  autoplot(.vars = revenue_billions) +\n  stat_smooth(method = \"lm\", \n              formula = y ~ x, \n              geom = \"smooth\",\n              se = FALSE,\n              color = \"#E69F00\",\n              linetype = \"dotted\") +\n    labs(\n      x = \"Quarter\",\n      y = \"Revenue (Billions USD)\",\n      title = \"Apple Revenue (in Billions USD)\"\n    ) +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5))\n\napple_plot_transformed &lt;- apple_ts |&gt;\n  autoplot(.vars = log(revenue_billions)) +\n  stat_smooth(method = \"lm\", \n              formula = y ~ x, \n              geom = \"smooth\",\n              se = FALSE,\n              color = \"#E69F00\",\n              linetype = \"dotted\") +\n    labs(\n      x = \"Quarter\",\n      y = \"Logarithm of Revenue\",\n      title = \"Logarithm of Apple Revenue\"\n    ) +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5))\n\napple_plot_regular | apple_plot_transformed\n\n\n\n\n\n\n\n\nFigure 10: Apple quarterly revenue figures (in billions of U.S. dollars) from Q1 of 2005 to Q1 of 2012; the figure on the left presents the revenue in dollars and the figure on the right gives the logarithm of the quarterly revenue; a simple linear regression line is given for reference\n\n\n\n\n\n\n\nFinding a Suitable Model\nWe start by fitting a cubic trend to the logarithm of the quarterly revenues. The full model is fitted here:\n\n\nShow the code\n# Cubic model with standardized time variable\n\napple_full_cubic_lm &lt;- apple_ts |&gt;\n  model(apple_full_cubic = TSLM(log(revenue_billions) ~ std_t + I(std_t^2) + I(std_t^3) +\n    sin1 + cos1 + cos2 )) # Note sin2 is omitted\napple_full_cubic_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 7 × 7\n  .model           term        estimate std.error statistic  p.value sig  \n  &lt;chr&gt;            &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;\n1 apple_full_cubic (Intercept)   1.83      0.0572    32.1   5.67e-20 TRUE \n2 apple_full_cubic std_t         1.01      0.0972    10.4   6.41e-10 TRUE \n3 apple_full_cubic I(std_t^2)   -0.0158    0.0445    -0.355 7.26e- 1 FALSE\n4 apple_full_cubic I(std_t^3)    0.0866    0.0516     1.68  1.07e- 1 FALSE\n5 apple_full_cubic sin1          0.217     0.0533     4.08  4.96e- 4 TRUE \n6 apple_full_cubic cos1          0.0839    0.0552     1.52  1.43e- 1 FALSE\n7 apple_full_cubic cos2         -0.0981    0.0382    -2.57  1.74e- 2 TRUE \n\n\nThe quadratic and cubic trend terms are not statistically signficant. We now eliminate the cubic term and fit a full model with a quadratic trend.\n\n\nShow the code\napple_full_quad_lm &lt;- apple_ts |&gt;\n  model(apple_full_quad = TSLM(log(revenue_billions) ~ std_t + I(std_t^2) + \n    sin1 + cos1 + cos2 )) # Note sin2 is omitted\napple_full_quad_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 6 × 7\n  .model          term        estimate std.error statistic  p.value sig  \n  &lt;chr&gt;           &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;\n1 apple_full_quad (Intercept)   1.83      0.0594    30.9   3.12e-20 TRUE \n2 apple_full_quad std_t         1.16      0.0403    28.7   1.64e-19 TRUE \n3 apple_full_quad I(std_t^2)   -0.0158    0.0462    -0.341 7.36e- 1 FALSE\n4 apple_full_quad sin1          0.217     0.0554     3.93  6.73e- 4 TRUE \n5 apple_full_quad cos1          0.0934    0.0570     1.64  1.15e- 1 FALSE\n6 apple_full_quad cos2         -0.0981    0.0396    -2.48  2.11e- 2 TRUE \n\n\nThe quadratic trend term is not statistically significant. Nevertheless, we will still fit a reduced model with a quadratic trend but we will omit the non-signficant seasonal Fourier term, cos1.\n\n\nShow the code\n# Quadratic trend with standardized time variable\n\napple_reduced_quad_lm1 &lt;- apple_ts |&gt;\n  model(apple_reduced_quad1 = TSLM(log(revenue_billions) ~ std_t + I(std_t^2) +\n    sin1 + cos2 )) # Note sin2 is omitted\napple_reduced_quad_lm1 |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 5 × 7\n  .model              term        estimate std.error statistic  p.value sig  \n  &lt;chr&gt;               &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;\n1 apple_reduced_quad1 (Intercept)   1.83      0.0614    29.9   1.71e-20 TRUE \n2 apple_reduced_quad1 std_t         1.16      0.0415    28.0   7.94e-20 TRUE \n3 apple_reduced_quad1 I(std_t^2)   -0.0158    0.0478    -0.330 7.44e- 1 FALSE\n4 apple_reduced_quad1 sin1          0.217     0.0573     3.80  8.79e- 4 TRUE \n5 apple_reduced_quad1 cos2         -0.0981    0.0410    -2.39  2.49e- 2 TRUE \n\n\nThe quadratic trend term is still not statistically significant. We will fit a full model with a linear trend.\n\n\nShow the code\n# Linear trend with standardized time variable\n\napple_full_linear_lm &lt;- apple_ts |&gt;\n  model(apple_full_linear = TSLM(log(revenue_billions) ~ std_t + \n    sin1 + cos1 + cos2 )) # Note sin2 is omitted\napple_full_linear_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 5 × 7\n  .model            term        estimate std.error statistic  p.value sig  \n  &lt;chr&gt;             &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;\n1 apple_full_linear (Intercept)   1.82      0.0388     46.9  4.04e-25 TRUE \n2 apple_full_linear std_t         1.16      0.0396     29.2  2.81e-20 TRUE \n3 apple_full_linear sin1          0.215     0.0540      3.99 5.42e- 4 TRUE \n4 apple_full_linear cos1          0.0934    0.0559      1.67 1.08e- 1 FALSE\n5 apple_full_linear cos2         -0.0972    0.0388     -2.50 1.95e- 2 TRUE \n\n\nThe coefficient on the cos1 seasonal Fourier term is not statistically significant. We now fit a reduced model that only contains the significant terms from the full model with a linear trend.\n\n\nShow the code\n# Linear trend with standardized time variable\n\napple_reduced_linear_lm1 &lt;- apple_ts |&gt;\n  model(apple_reduced_linear1 = TSLM(log(revenue_billions) ~ std_t + \n    sin1 + cos2 )) # Note sin2 is omitted\napple_reduced_linear_lm1 |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 4 × 7\n  .model                term        estimate std.error statistic  p.value sig  \n  &lt;chr&gt;                 &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;\n1 apple_reduced_linear1 (Intercept)   1.82      0.0402     45.3  1.60e-25 TRUE \n2 apple_reduced_linear1 std_t         1.16      0.0408     28.5  1.42e-20 TRUE \n3 apple_reduced_linear1 sin1          0.215     0.0559      3.85 7.21e- 4 TRUE \n4 apple_reduced_linear1 cos2         -0.0972    0.0402     -2.42 2.32e- 2 TRUE \n\n\nAll the terms are statistically significant in this model. We now compare the models we have fitted using the AIC, AICc, and BIC criterion.\n\n\nShow the code\nmodel_combined &lt;- apple_ts |&gt;\n  model(\n    apple_full_cubic = TSLM(log(revenue_billions) ~ std_t + I(std_t^2) + I(std_t^3) +\n                    sin1 + cos1 + cos2),\n    apple_full_quad = TSLM(log(revenue_billions) ~ std_t + I(std_t^2) + \n                    sin1 + cos1 + cos2),\n    apple_reduced_quad1 = TSLM(log(revenue_billions) ~ std_t + I(std_t^2) + \n                    sin1 + cos2),\n    apple_full_linear = TSLM(log(revenue_billions) ~ std_t +\n                    sin1 + cos1 + cos2),\n    apple_reduced_linear1 = TSLM(log(revenue_billions) ~ std_t + \n                    sin1 + cos2 )\n  )\n\nglance(model_combined) |&gt;\n  select(.model, AIC, AICc, BIC)\n\n\n\n\n\n\nTable 3: Comparison of the AIC, AICc, and BIC values for the models fitted to the logarithm of the simulated time series.\n\n\n\n\n\n\nModel\nAIC\nAICc\nBIC\n\n\n\n\napple_full_cubic\n-84\n-76.8\n-73.1\n\n\napple_full_quad\n-82.5\n-77.2\n-73\n\n\napple_reduced_quad1\n-81.3\n-77.5\n-73.1\n\n\napple_full_linear\n**-84.4**\n-80.6\n-76.2\n\n\napple_reduced_linear1\n-83.2\n**-80.6**\n**-76.4**\n\n\n\n\n\n\n\n\n\n\nWe will apply the apple_reduced_linear1 model.\n\n\nUsing the Residuals to Determine the Appropriate Correction\nThe residuals of this model are illustrated in Figure 11.\n\n\nShow the code\napple_resid_df &lt;- model_combined |&gt; \n  dplyr::select(apple_reduced_linear1) |&gt;\n  residuals() |&gt; \n  as_tibble() |&gt; \n  dplyr::select(.resid) |&gt;\n  rename(x = .resid) \n  \napple_resid_df |&gt;\n  mutate(density = dnorm(x, mean(apple_resid_df$x), sd(apple_resid_df$x))) |&gt;\n  ggplot(aes(x = x)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"#56B4E9\", binwidth = 0.05) +\n    geom_line(aes(x = x, y = density)) +\n    theme_bw() +\n    labs(\n      x = \"Values\",\n      y = \"Frequency\",\n      title = \"Histogram of Residuals from the Reduced Model with a Linear Trend\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\nFigure 11: Histogram of the residuals from the reduced model with a linear trend component\n\n\n\n\n\nUsing the command skewness(apple_resid_df$x), we compute the skewness of these residuals as: -0.799. This number is not close to zero (it is between -1 and -0.5) indicating moderate skewness. We would therefore apply the empirical correction to our mean forecast values.\n\n\nApplying the Correction Factor\nTable 4 summarizes some of the corrected mean values. Note that in this particular case, the corrected values are very close to the uncorrected values.\n\n\nShow the code\napple_model_values &lt;- model_combined |&gt; \n  dplyr::select(apple_reduced_linear1) |&gt;\n  glance()\n\napple_model_check &lt;- apple_model_values |&gt;\n  mutate(\n    sigma = sqrt(sigma2),\n    lognorm_cf = exp((1/2) * sigma2),\n    empirical_cf = apple_reduced_linear_lm1 |&gt;\n      residuals() |&gt;\n      pull(.resid) |&gt;\n      exp() |&gt;\n      mean()) |&gt;\n  select(.model, r_squared, sigma2, sigma, lognorm_cf, empirical_cf)\n\napple_pred &lt;- model_combined |&gt; \n  dplyr::select(apple_reduced_linear1) |&gt;\n  forecast(new_data = apple_ts) |&gt;\n  mutate(.mean_correction = .mean * apple_model_check$empirical_cf) |&gt;\n  select(t, revenue_billions, .mean, .mean_correction)\n\napple_model_check\n\n\n# A tibble: 1 × 6\n  .model                r_squared sigma2 sigma lognorm_cf empirical_cf\n  &lt;chr&gt;                     &lt;dbl&gt;  &lt;dbl&gt; &lt;dbl&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n1 apple_reduced_linear1     0.971 0.0466 0.216       1.02         1.02\n\n\n\n\nShow the code\napple_pred &lt;- model_combined |&gt; \n  dplyr::select(apple_reduced_linear1) |&gt;\n  forecast(new_data = apple_ts) |&gt;\n  mutate(.mean_correction = .mean * apple_model_check$empirical_cf) |&gt;\n  select(t, revenue_billions, .mean, .mean_correction)\n\n\n\n\n\n\nTable 4: Fitted values for the model representing Apple’s quarterly revenue\n\n\n\n\n\n\nt\nrevenue_billions\n.mean\n.mean_correction\ndates\n\n\n\n\n1\nt(N(0.22, 0.057))\n1.284\n1.309\n2005-01-01\n\n\n2\nt(N(-0.051, 0.054))\n0.975\n0.994\n2005-04-01\n\n\n3\nt(N(0.064, 0.057))\n1.096\n1.118\n2005-07-01\n\n\n4\nt(N(0.22, 0.053))\n1.281\n1.306\n2005-10-01\n\n\n⋮\n⋮\n⋮\n⋮\n⋮\n\n\n28\nt(N(3.5, 0.054))\n33.885\n34.542\n2011-10-01\n\n\n29\nt(N(4, 0.057))\n58.588\n59.725\n2012-01-01\n\n\n\n\n\n\n\n\n\n\n\n\nPlotting the Fitted Values\nThese fitted values are illustrated in Figure 12.\n\n\nShow the code\napple_ts |&gt;\n  autoplot(.vars = revenue_billions) +\n  geom_line(data = apple_pred, aes(x = dates, y = .mean_correction), color = \"#56B4E9\") +\n    labs(\n      x = \"Quarter\",\n      y = \"Revenue (Billions USD)\",\n      title = \"Apple Revenue in Billions of U.S. Dollars\"\n    ) +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFigure 12: Apple Inc.’s quarterly revenue in billions of U.S. dollars through first quarter of 2012 (in black) and the fitted regression model (in blue)\n\n\n\n\n\nThis time series was used as an example. We are obviously not interested in forecasting future values using this model. However, this is an excellent example of real-world exponential growth in a time series with a seasonal component. Limiting factors prevent exponential growth from being sustainable in the long run. After 2012, the Apple quarterly revenues follow a different, but very impressive, model. This is illustrated in Figure 13.\n\n\nShow the code\napple_raw |&gt;\n  dplyr::select(dates, revenue_billions) |&gt;\n  as_tsibble(index = dates) |&gt;\n  autoplot(.vars = revenue_billions) +\n  geom_line(\n    data = apple_raw |&gt; filter(dates &gt;= my(\"Jan 2012\")), \n    aes(x = dates, y = revenue_billions), \n    color = \"#D55E00\"\n  ) +\n  labs(\n    x = \"Quarter\",\n    y = \"Revenue (Billions USD)\",\n    title = \"Apple Revenue (in Billions USD)\"\n  ) +\n  theme_minimal() +\n  theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFigure 13: Apple Inc.’s quarterly revenue in billions of U.S. dollars; values beginning with the first quarter of 2012 are shown in orange",
    "crumbs": [
      "Lesson 4",
      "Transformations, Forecasting adn Bias Correction"
    ]
  },
  {
    "objectID": "chapter_5_lesson_4.html#choose-one-of-the-following-small-group-activities-25-min",
    "href": "chapter_5_lesson_4.html#choose-one-of-the-following-small-group-activities-25-min",
    "title": "Transformations, Forecasting adn Bias Correction",
    "section": "Choose One of the Following Small-Group Activities (25 min)",
    "text": "Choose One of the Following Small-Group Activities (25 min)\n\nSmall-Group Activity: Retail Sales (All Other General Merchandise Stores)\nThe code below downloads and gives the time plot for the total monthly sales in the United States for retail stores with the NAICS category 45299, “All Other General Merchandise Stores.” The time plot is given in Figure Figure 14.\n\n\nShow the code\n# Read in retail sales data for \"all other general merchandise stores\"\nretail_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/retail_by_business_type.parquet\") |&gt;\n  filter(naics == 45299) |&gt;\n  filter(as_date(month) &gt;= my(\"Jan 1998\")) |&gt;\n  mutate(t = 1:n()) |&gt;\n  mutate(std_t = (t - mean(t)) / sd(t)) |&gt;\n  mutate(\n    cos1 = cos(2 * pi * 1 * t / 12),\n    cos2 = cos(2 * pi * 2 * t / 12),\n    cos3 = cos(2 * pi * 3 * t / 12),\n    cos4 = cos(2 * pi * 4 * t / 12),\n    cos5 = cos(2 * pi * 5 * t / 12),\n    cos6 = cos(2 * pi * 6 * t / 12),\n    sin1 = sin(2 * pi * 1 * t / 12),\n    sin2 = sin(2 * pi * 2 * t / 12),\n    sin3 = sin(2 * pi * 3 * t / 12),\n    sin4 = sin(2 * pi * 4 * t / 12),\n    sin5 = sin(2 * pi * 5 * t / 12)\n  ) |&gt;\n  as_tsibble(index = month)\n\nretail_ts |&gt;\n  autoplot(.vars = sales_millions) +\n  stat_smooth(method = \"lm\", \n              formula = y ~ x, \n              geom = \"smooth\",\n              se = FALSE,\n              color = \"#E69F00\",\n              linetype = \"dotted\") +\n    labs(\n      x = \"Month\",\n      y = \"Sales (Millions of U.S. Dollars)\",\n      title = paste0(retail_ts$business[1], \" (\", retail_ts$naics[1], \")\")\n    ) +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFigure 14: Time plot of the total monthly retail sales for all other general merchandise stores (45299)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUse the retail sales data to do the following.\n\nSelect an appropriate fitted model using the AIC, AICc, or BIC critera.\nUse the residuals to determine the appropriate correction for the data.\nForecast the data for the next 5 years.\nApply the appropriate correction to the forecasted values.\nPlot the fitted (forecasted) values along with the time series.\n\n\n\n\n\nSmall-Group Activity: Industrial Electricity Consumption in Texas\nThese data represent the amount of electricity used each month for industrial applications in Texas.\n\n\nShow the code\nelec_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/electricity_tx.csv\") |&gt;\n  dplyr::select(-comments) |&gt;\n  mutate(month = my(month)) |&gt;\n  mutate(\n    t = 1:n(),\n    std_t = (t - mean(t)) / sd(t)\n  ) |&gt;\n  mutate(\n    cos1 = cos(2 * pi * 1 * t / 12),\n    cos2 = cos(2 * pi * 2 * t / 12),\n    cos3 = cos(2 * pi * 3 * t / 12),\n    cos4 = cos(2 * pi * 4 * t / 12),\n    cos5 = cos(2 * pi * 5 * t / 12),\n    cos6 = cos(2 * pi * 6 * t / 12),\n    sin1 = sin(2 * pi * 1 * t / 12),\n    sin2 = sin(2 * pi * 2 * t / 12),\n    sin3 = sin(2 * pi * 3 * t / 12),\n    sin4 = sin(2 * pi * 4 * t / 12),\n    sin5 = sin(2 * pi * 5 * t / 12)\n  ) |&gt;\n  as_tsibble(index = month)\n\nelec_plot_raw &lt;- elec_ts |&gt;\n    autoplot(.vars = megawatthours) +\n    labs(\n      x = \"Month\",\n      y = \"Megawatt-hours\",\n      title = \"Texas' Industrial Electricity Use\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\nelec_plot_log &lt;- elec_ts |&gt;\n    autoplot(.vars = log(megawatthours)) +\n    labs(\n      x = \"Month\",\n      y = \"log(Megwatt-hours)\",\n      title = \"Log of Texas' Industrial Electricity Use\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\nelec_plot_raw | elec_plot_log\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUse the Texas industrial electricity consumption data to do the following.\n\nSelect an appropriate fitted model using the AIC, AICc, or BIC critera.\nUse the residuals to determine the appropriate correction for the data.\nForecast the data for the next 5 years.\nApply the appropriate correction to the forecasted values.\nPlot the fitted (forecasted) values along with the time series.",
    "crumbs": [
      "Lesson 4",
      "Transformations, Forecasting adn Bias Correction"
    ]
  },
  {
    "objectID": "chapter_5_lesson_4.html#homework-preview-5-min-1",
    "href": "chapter_5_lesson_4.html#homework-preview-5-min-1",
    "title": "Transformations, Forecasting adn Bias Correction",
    "section": "Homework Preview (5 min)",
    "text": "Homework Preview (5 min)\n\nReview upcoming homework assignment\nClarify questions\n\n\n\n\n\n\n\nDownload Homework\n\n\n\n homework_5_4.qmd \n\n\n\nSmall-Group Activity\n\n\nSmall-Group Activity: Retail Sales Code\n\n\n\n\n\n\nCheck Your Understanding Code\n\n\n\n\n\n\n\n\n\nFigures\n\n\n\n\n\nFigure 15 gives the total sales (in millions of U.S. dollars) for the category “all other general merchandise stores (45299),” beginning with January 1998.\n\n\nShow the code\n# Read in retail sales data for \"all other general merchandise stores\"\nretail_ts &lt;- rio::import(\"https://byuistats.github.io/timeseries/data/retail_by_business_type.parquet\") |&gt;\n  filter(naics == 45299) |&gt;\n  mutate(t = 1:n()) |&gt;\n  mutate(std_t = (t - mean(t)) / sd(t)) |&gt;\n  mutate(\n    cos1 = cos(2 * pi * 1 * t / 12),\n    cos2 = cos(2 * pi * 2 * t / 12),\n    cos3 = cos(2 * pi * 3 * t / 12),\n    cos4 = cos(2 * pi * 4 * t / 12),\n    cos5 = cos(2 * pi * 5 * t / 12),\n    cos6 = cos(2 * pi * 6 * t / 12),\n    sin1 = sin(2 * pi * 1 * t / 12),\n    sin2 = sin(2 * pi * 2 * t / 12),\n    sin3 = sin(2 * pi * 3 * t / 12),\n    sin4 = sin(2 * pi * 4 * t / 12),\n    sin5 = sin(2 * pi * 5 * t / 12)\n  ) |&gt;\n  filter(as_date(month) &gt;= my(\"Jan 1998\")) |&gt;\n  as_tsibble(index = month)\n\nretail_ts |&gt;\n  autoplot(.vars = sales_millions) +\n    labs(\n      x = \"Month\",\n      y = \"Sales (Millions of U.S. Dollars)\",\n      title = paste0(retail_ts$business[1], \" (\", retail_ts$naics[1], \")\")\n    ) +\n    theme_minimal() +\n    theme(plot.title = element_text(hjust = 0.5))\n\n\n\n\n\n\n\n\nFigure 15: Time plot of the total monthly retail sales for all other general merchandise stores (45299)\n\n\n\n\n\nFigure 16 shows the “All other general merchandise” retail sales data.\n\nShow the code\nplot_raw &lt;- retail_ts |&gt;\n    autoplot(.vars = sales_millions) +\n    labs(\n      x = \"Month\",\n      y = \"sales_millions\",\n      title = \"Other General Merchandise Sales\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\nplot_log &lt;- retail_ts |&gt;\n    autoplot(.vars = log(sales_millions)) +\n    labs(\n      x = \"Month\",\n      y = \"log(sales_millions)\",\n      title = \"Logarithm of Other Gen. Merch. Sales\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\nplot_raw | plot_log\n\n\n\n\n\n\n\nFigure 16: Time plot of the time series (left) and the natural logarithm of the time series (right)\n\n\n\n\n\n\n\n\n\n\n\n\n\nCubic Trend\n\n\n\n\n\n\n\nShow the code\n# Cubic model with standardized time variable\n\nfull_cubic_lm &lt;- retail_ts |&gt;\n  model(full_cubic = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + I(std_t^3) +\n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n    + sin4 + cos4 + sin5 + cos5 + cos6)) # Note sin6 is omitted\nfull_cubic_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 15 × 7\n   .model     term        estimate std.error statistic   p.value sig  \n   &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 full_cubic (Intercept)  8.21      0.00391  2101.    0         TRUE \n 2 full_cubic std_t        0.445     0.00621    71.8   1.58e-184 TRUE \n 3 full_cubic I(std_t^2)   0.0224    0.00704     3.19  1.59e-  3 TRUE \n 4 full_cubic I(std_t^3)  -0.00244   0.00573    -0.426 6.70e-  1 FALSE\n 5 full_cubic sin1        -0.0232    0.00333    -6.96  2.30e- 11 TRUE \n 6 full_cubic cos1         0.0354    0.00332    10.6   1.71e- 22 TRUE \n 7 full_cubic sin2        -0.0692    0.00332   -20.8   3.31e- 59 TRUE \n 8 full_cubic cos2         0.0785    0.00332    23.6   3.68e- 69 TRUE \n 9 full_cubic sin3        -0.0434    0.00332   -13.1   6.81e- 31 TRUE \n10 full_cubic cos3         0.0699    0.00332    21.0   5.82e- 60 TRUE \n11 full_cubic sin4        -0.0288    0.00332    -8.68  3.06e- 16 TRUE \n12 full_cubic cos4         0.0627    0.00332    18.9   3.93e- 52 TRUE \n13 full_cubic sin5         0.0118    0.00332     3.56  4.35e-  4 TRUE \n14 full_cubic cos5         0.0636    0.00332    19.2   3.92e- 53 TRUE \n15 full_cubic cos6         0.0251    0.00235    10.7   1.17e- 22 TRUE \n\n\nNote that each of the Fourier terms is statistically significant.\n\n\n\n\n\n\n\n\n\n\nQuadratic Trend\n\n\n\n\n\n\n\nShow the code\nfull_quad_lm &lt;- retail_ts |&gt;\n  model(full_quad = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + \n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n    + sin4 + cos4 + sin5 + cos5 + cos6 )) # Note sin6 is omitted\nfull_quad_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 14 × 7\n   .model    term        estimate std.error statistic   p.value sig  \n   &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 full_quad (Intercept)   8.22     0.00334   2460.   0         TRUE \n 2 full_quad std_t         0.443    0.00398    111.   1.01e-237 TRUE \n 3 full_quad I(std_t^2)    0.0200   0.00404      4.94 1.30e-  6 TRUE \n 4 full_quad sin1         -0.0231   0.00332     -6.96 2.31e- 11 TRUE \n 5 full_quad cos1          0.0353   0.00332     10.7  1.54e- 22 TRUE \n 6 full_quad sin2         -0.0692   0.00332    -20.9  2.28e- 59 TRUE \n 7 full_quad cos2          0.0785   0.00332     23.7  2.33e- 69 TRUE \n 8 full_quad sin3         -0.0434   0.00332    -13.1  5.77e- 31 TRUE \n 9 full_quad cos3          0.0699   0.00332     21.1  3.96e- 60 TRUE \n10 full_quad sin4         -0.0288   0.00332     -8.69 2.83e- 16 TRUE \n11 full_quad cos4          0.0627   0.00332     18.9  2.84e- 52 TRUE \n12 full_quad sin5          0.0118   0.00332      3.57 4.25e-  4 TRUE \n13 full_quad cos5          0.0636   0.00332     19.2  2.81e- 53 TRUE \n14 full_quad cos6          0.0251   0.00235     10.7  1.04e- 22 TRUE \n\n\n\n\n\n\n\n\n\n\n\n\nLinear Trend\n\n\n\n\n\n\n\nShow the code\nfull_linear_lm &lt;- retail_ts |&gt;\n  model(full_quadratic = TSLM(log(sales_millions) ~ std_t +\n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n    + sin4 + cos4 + sin5 + cos5 + cos6 )) # Note sin6 is omitted\nfull_linear_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 13 × 7\n   .model         term        estimate std.error statistic   p.value sig  \n   &lt;chr&gt;          &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 full_quadratic (Intercept)   8.23     0.00264   3114.   0         TRUE \n 2 full_quadratic std_t         0.457    0.00303    151.   2.86e-275 TRUE \n 3 full_quadratic sin1         -0.0231   0.00345     -6.69 1.16e- 10 TRUE \n 4 full_quadratic cos1          0.0354   0.00345     10.3  3.26e- 21 TRUE \n 5 full_quadratic sin2         -0.0692   0.00345    -20.1  1.59e- 56 TRUE \n 6 full_quadratic cos2          0.0785   0.00345     22.8  2.95e- 66 TRUE \n 7 full_quadratic sin3         -0.0434   0.00345    -12.6  3.44e- 29 TRUE \n 8 full_quadratic cos3          0.0699   0.00345     20.3  2.87e- 57 TRUE \n 9 full_quadratic sin4         -0.0288   0.00345     -8.36 2.81e- 15 TRUE \n10 full_quadratic cos4          0.0627   0.00345     18.2  1.18e- 49 TRUE \n11 full_quadratic sin5          0.0118   0.00345      3.43 6.95e-  4 TRUE \n12 full_quadratic cos5          0.0636   0.00345     18.4  1.26e- 50 TRUE \n13 full_quadratic cos6          0.0251   0.00244     10.3  2.38e- 21 TRUE \n\n\n\n\n\n\n\n\n\n\n\n\nModel Comparison\n\n\n\n\n\nWe will now compare the models we fitted above. #tbl-ModelComparison2 gives the AIC, AICc, and BIC of the models fitted above. In addition, other models with a reduced number of Fourier terms are included. For example, the model labeled reduced_quadratic_fourier_i5 includes linear and quadratic trend terms but also the Fourier terms where \\(i \\le 5\\).\n\n\nShow the code\nmodel_combined &lt;- retail_ts |&gt;\n  model(\n    full_cubic = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + I(std_t^3) +\n                    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n                  + sin4 + cos4 + sin5 + cos5 + cos6 ),\n    full_quadratic = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + \n                       sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n                     + sin4 + cos4 + sin5 + cos5 + cos6 ),\n    reduced_quadratic_fourier_i5 = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + \n                       sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n                     + sin4 + cos4 + sin5 + cos5),\n    reduced_quadratic_fourier_i4 = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + \n                       sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n                     + sin4 + cos4),\n    # reduced_quadratic_fourier_i3 = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + \n    #                    sin1 + cos1 + sin2 + cos2 + sin3 + cos3),\n    # reduced_quadratic_fourier_i2 = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + \n    #                    sin1 + cos1 + sin2 + cos2),\n    # reduced_quadratic_fourier_i1 = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + \n    #                    sin1 + cos1),\n    full_linear = TSLM(log(sales_millions) ~ std_t + \n                         sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n                       + sin4 + cos4 + sin5 + cos5 + cos6 ),\n    reduced_linear_fourier_i5 = TSLM(log(sales_millions) ~ std_t + \n                       sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n                     + sin4 + cos4 + sin5 + cos5),\n    reduced_linear_fourier_i4 = TSLM(log(sales_millions) ~ std_t + \n                       sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n                     + sin4 + cos4),\n    # reduced_linear_fourier_i3 = TSLM(log(sales_millions) ~ std_t + \n    #                    sin1 + cos1 + sin2 + cos2 + sin3 + cos3),\n    # reduced_linear_fourier_i2 = TSLM(log(sales_millions) ~ std_t + \n    #                    sin1 + cos1 + sin2 + cos2),\n    # reduced_linear_fourier_i1 = TSLM(log(sales_millions) ~ std_t + \n    #                    sin1 + cos1)\n  )\n\nglance(model_combined) |&gt;\n  select(.model, AIC, AICc, BIC)\n\n\n\n\n\n\nTable 5: Comparison of the AIC, AICc, and BIC values for the models fitted to the logarithm of the retail sales time series.\n\n\n\n\n\n\nModel\nAIC\nAICc\nBIC\n\n\n\n\nfull_cubic\n-1904.6\n-1902.7\n-1845.4\n\n\nfull_quadratic\n**-1906.5**\n**-1904.8**\n**-1850.9**\n\n\nreduced_quadratic_fourier_i5\n-1807.4\n-1805.9\n-1755.5\n\n\nreduced_quadratic_fourier_i4\n-1611.1\n-1610\n-1566.6\n\n\nfull_linear\n-1883.8\n-1882.4\n-1832\n\n\nreduced_linear_fourier_i5\n-1791.6\n-1790.3\n-1743.5\n\n\nreduced_linear_fourier_i4\n-1603.8\n-1602.9\n-1563.1\n\n\n\n\n\n\n\n\n\n\nThe model with the lowest AIC, AICc, and BIC values is the full quadratic model. This can be written as:\n\\[\\begin{align*}\n  \\log(x_t) &= \\beta_0\n            + \\beta_1 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right)\n            + \\beta_2 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right)^2 \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_3 \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + \\beta_4 \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_5 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + \\beta_6 \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_7 \\sin \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right)\n            + \\beta_8 \\cos \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_9 \\sin \\left( \\frac{2\\pi \\cdot 4 t}{12} \\right)\n            + \\beta_{10} \\cos \\left( \\frac{2\\pi \\cdot 4 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_{11} \\sin \\left( \\frac{2\\pi \\cdot 5 t}{12} \\right)\n            + \\beta_{12} \\cos \\left( \\frac{2\\pi \\cdot 5 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            \\phantom{+ \\beta_{13} \\sin \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right)}\n            + \\beta_{13} \\cos \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right)\n      + z_t\n\\end{align*}\\]\n\n\n\n\n\n\n\n\n\n\nAutocorrelation of the Random Component\n\n\n\n\n\nWe check for autocorrelation in the random component to determine if using GLS is warranted.\nFigure 17 illustrates the ACF of the full model with a quadratic trend.\n\nfull_quad_ts &lt;- full_quad_lm |&gt;\n  residuals() \n\nacf(full_quad_ts$.resid, plot=TRUE, lag.max = 25)\n\n\n\n\n\n\n\nFigure 17: ACF of the full model with a quadratic trend\n\n\n\n\n\nWe observe evidence of autocorrelation in the random terms. In fact, there is something happening on an annual\nFigure 18 illustrates the PACF of the reduced model 1 with linear trend.\n\n\nShow the code\nalphas_quad &lt;- pacf(full_quad_ts$.resid, plot=FALSE, lag.max = 25) \npacf(full_quad_ts$.resid, plot=TRUE, lag.max = 25)\n\n\n\n\n\n\n\n\nFigure 18: PACF of the full model with a quadratic trend\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nApplying Genearlized Least Squares, GLS\n\n\n\n\n\nRecall that in Chapter 5, Lesson 1, we fitted a linear regression model using the value of the partial autocorrelation function for \\(k=1\\). This helps account for the autocorrelation in the residuals.\nWe will use the PACF when \\(k=1\\) to apply the GLS algorithm. The first few partial autocorrelation values are:\n\n\nShow the code\npacf(full_quad_ts$.resid, plot=FALSE, lag.max = 10)\n\n\n\nPartial autocorrelations of series 'full_quad_ts$.resid', by lag\n\n     1      2      3      4      5      6      7      8      9     10 \n 0.330  0.129 -0.023  0.062 -0.031  0.019 -0.035  0.044 -0.100  0.048 \n\n\nThe partial autocorrelation when \\(k=1\\) is approximately 0.33. We will use this value as we recompute the regression coefficients.\n\n\nShow the code\n# Load additional packages\npacman::p_load(tidymodels, multilevelmod,\n  nlme, broom.mixed)\n\ntemp_spec &lt;- linear_reg() |&gt;\n  set_engine(\"gls\", correlation = nlme::corAR1(0.497))\n\ntemp_gls &lt;- temp_spec |&gt;\n  fit(log(sales_millions) ~ std_t + sin1 + cos1 + sin2 + cos2 + sin3 + cos3, data = retail_ts)\n\ntidy(temp_gls) |&gt;\n  mutate(\n    lower = estimate + qnorm(0.025) * std.error,\n    upper = estimate + qnorm(0.975) * std.error\n  ) \n\n\n# A tibble: 8 × 7\n  term        estimate std.error statistic   p.value   lower   upper\n  &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n1 (Intercept)   8.23     0.00332   2479.   0          8.22    8.23  \n2 std_t         0.458    0.00381    120.   1.71e-250  0.450   0.465 \n3 sin1         -0.0231   0.00446     -5.18 4.17e-  7 -0.0318 -0.0143\n4 cos1          0.0355   0.00446      7.95 4.13e- 14  0.0267  0.0442\n5 sin2         -0.0692   0.00487    -14.2  3.22e- 35 -0.0787 -0.0597\n6 cos2          0.0787   0.00487     16.2  2.28e- 42  0.0691  0.0882\n7 sin3         -0.0434   0.00567     -7.65 2.86e- 13 -0.0545 -0.0323\n8 cos3          0.0701   0.00567     12.4  1.68e- 28  0.0590  0.0812\n\n\nFigure 19 illustrates the original time series (in black) and the fitted model (in blue). For reference, a dotted line illustrating the simple least squares line is plotted on this figure for reference. It helps highlight the exponential shape of the trend.\n\n\nShow the code\nforecast_df &lt;- full_quad_lm |&gt; \n  forecast(retail_ts) |&gt;  # computes the anti-log of the predicted values and returns them as .mean\n  as_tibble() |&gt; \n  dplyr::select(std_t, .mean) |&gt; \n  rename(pred = .mean)\n\nretail_ts |&gt;\n  left_join(forecast_df, by = \"std_t\") |&gt;\n  as_tsibble(index = month) |&gt;\n  autoplot(.vars = sales_millions) +\n  geom_smooth(method = \"lm\", formula = 'y ~ x', se = FALSE, color = \"#E69F00\", linewidth = 0.5, linetype = \"dotted\") +\n  geom_line(aes(y = pred), color = \"#56B4E9\", alpha = 0.75) +\n    labs(\n      x = \"Month\",\n      y = \"Simulated Time Series\",\n      title = \"Time Plot of Simulated Time Series with an Exponential Trend\",\n      subtitle = \"Predicted values based on the full cubic model are given in blue\"\n    ) +\n    theme_minimal() +\n    theme(\n      plot.title = element_text(hjust = 0.5),\n      plot.subtitle = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\nFigure 19: Time plot of the time series (left) and the natural logarithm of the time series (right)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUse the retail sales data to do the following.\n\nSelect an appropriate fitted model using the AIC, AICc, or BIC critera.\nUse the residuals to determine the appropriate correction for the data.\nForecast the data for the next 5 years.\nApply the appropriate correction to the forecasted values.\nPlot the fitted (forecasted) values along with the time series.\n\n\n\n\n\nShow the code\nretail_full_quad_lm &lt;- retail_ts |&gt;\n  model(retail_full_quad = TSLM(log(sales_millions) ~ std_t + I(std_t^2) + \n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n    + sin4 + cos4 + sin5 + cos5 + cos6 )) # Note sin6 is omitted\nretail_full_quad_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 14 × 7\n   .model           term        estimate std.error statistic   p.value sig  \n   &lt;chr&gt;            &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 retail_full_quad (Intercept)   8.22     0.00334   2460.   0         TRUE \n 2 retail_full_quad std_t         0.443    0.00398    111.   1.01e-237 TRUE \n 3 retail_full_quad I(std_t^2)    0.0200   0.00404      4.94 1.30e-  6 TRUE \n 4 retail_full_quad sin1         -0.0231   0.00332     -6.96 2.31e- 11 TRUE \n 5 retail_full_quad cos1          0.0353   0.00332     10.7  1.54e- 22 TRUE \n 6 retail_full_quad sin2         -0.0692   0.00332    -20.9  2.28e- 59 TRUE \n 7 retail_full_quad cos2          0.0785   0.00332     23.7  2.33e- 69 TRUE \n 8 retail_full_quad sin3         -0.0434   0.00332    -13.1  5.77e- 31 TRUE \n 9 retail_full_quad cos3          0.0699   0.00332     21.1  3.96e- 60 TRUE \n10 retail_full_quad sin4         -0.0288   0.00332     -8.69 2.83e- 16 TRUE \n11 retail_full_quad cos4          0.0627   0.00332     18.9  2.84e- 52 TRUE \n12 retail_full_quad sin5          0.0118   0.00332      3.57 4.25e-  4 TRUE \n13 retail_full_quad cos5          0.0636   0.00332     19.2  2.81e- 53 TRUE \n14 retail_full_quad cos6          0.0251   0.00235     10.7  1.04e- 22 TRUE \n\n\nShow the code\nretail_resid_df &lt;- retail_full_quad_lm |&gt; \n  residuals() |&gt; \n  as_tibble() |&gt; \n  dplyr::select(.resid) |&gt;\n  rename(x = .resid) \n  \nretail_resid_df |&gt;\n  mutate(density = dnorm(x, mean(retail_resid_df$x), sd(retail_resid_df$x))) |&gt;\n  ggplot(aes(x = x)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"#56B4E9\", binwidth = 0.02) +\n    geom_line(aes(x = x, y = density)) +\n    theme_bw() +\n    labs(\n      x = \"Values\",\n      y = \"Frequency\",\n      title = \"Histogram of Residuals from the Full Quadratic Model\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nShow the code\nskewness(retail_resid_df$x)\n\n\n[1] 0.2205956\n\n\nThere is little skewness. We will use the log-normal correction factor.\n\nSmall-Group Activity: Texas Industrial Electricity Usage\n\n\n\n\n\n\n\n\nCheck Your Understanding\n\n\n\nUse the Texas industrial electricity consumption data to do the following.\n\nSelect an appropriate fitted model using the AIC, AICc, or BIC critera.\nUse the residuals to determine the appropriate correction for the data.\nForecast the data for the next 5 years.\nApply the appropriate correction to the forecasted values.\nPlot the fitted (forecasted) values along with the time series.\n\n\n\n\n\nShow the code\n# Cubic model with standardized time variable\n\nelec_full_cubic_lm &lt;- elec_ts |&gt;\n  model(elec_full_cubic = TSLM(log(megawatthours) ~ std_t + I(std_t^2) + I(std_t^3) +\n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 + sin4 + cos4 + sin5 + cos5 + cos6)) # Note sin6 is omitted\nelec_full_cubic_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 15 × 7\n   .model          term          estimate std.error statistic  p.value sig  \n   &lt;chr&gt;           &lt;chr&gt;            &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;\n 1 elec_full_cubic (Intercept) 16.1         0.00684 2349.     0        TRUE \n 2 elec_full_cubic std_t        0.124       0.0115    10.8    1.19e-20 TRUE \n 3 elec_full_cubic I(std_t^2)   0.0313      0.00513    6.10   8.50e- 9 TRUE \n 4 elec_full_cubic I(std_t^3)   0.0105      0.00589    1.79   7.55e- 2 FALSE\n 5 elec_full_cubic sin1        -0.0541      0.00648   -8.36   3.68e-14 TRUE \n 6 elec_full_cubic cos1        -0.0468      0.00645   -7.25   1.89e-11 TRUE \n 7 elec_full_cubic sin2        -0.0000864   0.00646   -0.0134 9.89e- 1 FALSE\n 8 elec_full_cubic cos2        -0.000831    0.00645   -0.129  8.98e- 1 FALSE\n 9 elec_full_cubic sin3         0.00103     0.00645    0.160  8.73e- 1 FALSE\n10 elec_full_cubic cos3         0.0131      0.00645    2.03   4.46e- 2 TRUE \n11 elec_full_cubic sin4         0.0121      0.00645    1.87   6.29e- 2 FALSE\n12 elec_full_cubic cos4         0.00272     0.00645    0.421  6.74e- 1 FALSE\n13 elec_full_cubic sin5         0.0220      0.00645    3.41   8.36e- 4 TRUE \n14 elec_full_cubic cos5        -0.000505    0.00645   -0.0783 9.38e- 1 FALSE\n15 elec_full_cubic cos6        -0.00172     0.00456   -0.378  7.06e- 1 FALSE\n\n\nShow the code\n# Quadratic model with standardized time variable\n\nelec_full_quadratic_lm &lt;- elec_ts |&gt;\n  model(elec_full_cubic = TSLM(log(megawatthours) ~ std_t + I(std_t^2) + \n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 + sin4 + cos4 + sin5 + cos5 + cos6)) # Note sin6 is omitted\nelec_full_quadratic_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 14 × 7\n   .model          term         estimate std.error statistic  p.value sig  \n   &lt;chr&gt;           &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;\n 1 elec_full_cubic (Intercept) 16.1        0.00689 2333.     0        TRUE \n 2 elec_full_cubic std_t        0.143      0.00462   31.0    5.06e-68 TRUE \n 3 elec_full_cubic I(std_t^2)   0.0313     0.00517    6.05   1.04e- 8 TRUE \n 4 elec_full_cubic sin1        -0.0551     0.00650   -8.47   1.79e-14 TRUE \n 5 elec_full_cubic cos1        -0.0465     0.00650   -7.16   3.03e-11 TRUE \n 6 elec_full_cubic sin2        -0.000536   0.00650   -0.0825 9.34e- 1 FALSE\n 7 elec_full_cubic cos2        -0.000571   0.00650   -0.0880 9.30e- 1 FALSE\n 8 elec_full_cubic sin3         0.000775   0.00650    0.119  9.05e- 1 FALSE\n 9 elec_full_cubic cos3         0.0133     0.00650    2.05   4.19e- 2 TRUE \n10 elec_full_cubic sin4         0.0119     0.00649    1.84   6.80e- 2 FALSE\n11 elec_full_cubic cos4         0.00298    0.00650    0.458  6.48e- 1 FALSE\n12 elec_full_cubic sin5         0.0219     0.00649    3.37   9.39e- 4 TRUE \n13 elec_full_cubic cos5        -0.000245   0.00650   -0.0378 9.70e- 1 FALSE\n14 elec_full_cubic cos6        -0.00159    0.00459   -0.347  7.29e- 1 FALSE\n\n\nShow the code\nelec_resid_df &lt;- elec_full_quadratic_lm |&gt; \n  residuals() |&gt; \n  as_tibble() |&gt; \n  dplyr::select(.resid) |&gt;\n  rename(x = .resid) \n  \nelec_resid_df |&gt;\n  mutate(density = dnorm(x, mean(elec_resid_df$x), sd(elec_resid_df$x))) |&gt;\n  ggplot(aes(x = x)) +\n    geom_histogram(aes(y = after_stat(density)),\n        color = \"white\", fill = \"#56B4E9\", binwidth = 0.02) +\n    geom_line(aes(x = x, y = density)) +\n    theme_bw() +\n    labs(\n      x = \"Values\",\n      y = \"Frequency\",\n      title = \"Histogram of Residuals from the Full Quadratic Model\"\n    ) +\n    theme(\n      plot.title = element_text(hjust = 0.5)\n    )\n\n\n\n\n\n\n\n\n\nShow the code\nskewness(elec_resid_df$x)\n\n\n[1] -0.9556189\n\n\nThere is moderate negative skewness. We will use the emperical correction factor.",
    "crumbs": [
      "Lesson 4",
      "Transformations, Forecasting adn Bias Correction"
    ]
  },
  {
    "objectID": "chapter_5_lesson_4.html#cubic-model",
    "href": "chapter_5_lesson_4.html#cubic-model",
    "title": "Transformations, Forecasting adn Bias Correction",
    "section": "Cubic Model",
    "text": "Cubic Model\nAfter taking the (natural) logarithm of \\(x_t\\), we fit a cubic model to the log-transformed time series.\n\nFull Cubic Model\n\\[\\begin{align*}\n  \\log(x_t) &= \\beta_0\n            + \\beta_1 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right)\n            + \\beta_2 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right)^2\n            + \\beta_3 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right)^3 \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_4 \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + \\beta_5 \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_6 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + \\beta_7 \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_8 \\sin \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right)\n            + \\beta_9 \\cos \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_{10} \\sin \\left( \\frac{2\\pi \\cdot 4 t}{12} \\right)\n            + \\beta_{11} \\cos \\left( \\frac{2\\pi \\cdot 4 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_{12} \\sin \\left( \\frac{2\\pi \\cdot 5 t}{12} \\right)\n            + \\beta_{13} \\cos \\left( \\frac{2\\pi \\cdot 5 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            \\phantom{+ \\beta_{15} \\sin \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right)}\n            + \\beta_{14} \\cos \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right)\n      + z_t\n\\end{align*}\\]\n\n\nShow the code\n# Cubic model with standardized time variable\n\nfull_cubic_lm &lt;- sim_ts |&gt;\n  model(full_cubic = TSLM(log(x_t) ~ std_t + I(std_t^2) + I(std_t^3) +\n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n    + sin4 + cos4 + sin5 + cos5 + cos6 )) # Note sin6 is omitted\nfull_cubic_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 15 × 7\n   .model     term         estimate std.error statistic   p.value sig  \n   &lt;chr&gt;      &lt;chr&gt;           &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 full_cubic (Intercept)  2.82       0.00327   865.    2.08e-183 TRUE \n 2 full_cubic std_t        0.473      0.00550    86.0   1.90e- 90 TRUE \n 3 full_cubic I(std_t^2)  -0.00488    0.00246    -1.98  5.02e-  2 FALSE\n 4 full_cubic I(std_t^3)   0.000860   0.00285     0.302 7.64e-  1 FALSE\n 5 full_cubic sin1         0.0328     0.00312    10.5   1.68e- 17 TRUE \n 6 full_cubic cos1         0.0389     0.00308    12.6   7.52e- 22 TRUE \n 7 full_cubic sin2         0.0496     0.00309    16.1   1.33e- 28 TRUE \n 8 full_cubic cos2         0.0297     0.00308     9.65  1.10e- 15 TRUE \n 9 full_cubic sin3         0.0111     0.00308     3.60  5.15e-  4 TRUE \n10 full_cubic cos3         0.00765    0.00308     2.48  1.49e-  2 TRUE \n11 full_cubic sin4         0.00155    0.00308     0.504 6.15e-  1 FALSE\n12 full_cubic cos4         0.00185    0.00308     0.600 5.50e-  1 FALSE\n13 full_cubic sin5         0.00169    0.00308     0.550 5.84e-  1 FALSE\n14 full_cubic cos5         0.00186    0.00308     0.602 5.48e-  1 FALSE\n15 full_cubic cos6         0.00184    0.00218     0.845 4.00e-  1 FALSE\n\n\nNote that neither the quadratic nor the cubic terms are statistically significant in this model. We will eliminate the cubic term and fit a model with a quadratic trend.",
    "crumbs": [
      "Lesson 4",
      "Transformations, Forecasting adn Bias Correction"
    ]
  },
  {
    "objectID": "chapter_5_lesson_4.html#quadratic-model",
    "href": "chapter_5_lesson_4.html#quadratic-model",
    "title": "Transformations, Forecasting adn Bias Correction",
    "section": "Quadratic Model",
    "text": "Quadratic Model\nWe now fit a quadratic model to the log-transformed time series.\n\nFull Quadratic Model\nThe full model with a quadratic trend is written as:\n\\[\\begin{align*}\n  \\log(x_t) &= \\beta_0\n            + \\beta_1 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right)\n            + \\beta_2 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right)^2 \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_3 \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + \\beta_4 \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_5 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + \\beta_6 \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_7 \\sin \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right)\n            + \\beta_8 \\cos \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_9 \\sin \\left( \\frac{2\\pi \\cdot 4 t}{12} \\right)\n            + \\beta_{10} \\cos \\left( \\frac{2\\pi \\cdot 4 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_{11} \\sin \\left( \\frac{2\\pi \\cdot 5 t}{12} \\right)\n            + \\beta_{12} \\cos \\left( \\frac{2\\pi \\cdot 5 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n           \\phantom{+ \\beta_{14} \\sin \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right)}\n            + \\beta_{13} \\cos \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right)\n      + z_t\n\\end{align*}\\]\n\n\nShow the code\nfull_quad_lm &lt;- sim_ts |&gt;\n  model(full_quad = TSLM(log(x_t) ~ std_t + I(std_t^2) + \n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n    + sin4 + cos4 + sin5 + cos5 + cos6 )) # Note sin6 is omitted\nfull_quad_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 14 × 7\n   .model    term        estimate std.error statistic   p.value sig  \n   &lt;chr&gt;     &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 full_quad (Intercept)  2.82      0.00325   869.    2.41e-185 TRUE \n 2 full_quad std_t        0.475     0.00219   217.    1.02e-128 TRUE \n 3 full_quad I(std_t^2)  -0.00488   0.00244    -1.99  4.90e-  2 TRUE \n 4 full_quad sin1         0.0326    0.00307    10.6   9.09e- 18 TRUE \n 5 full_quad cos1         0.0389    0.00306    12.7   4.29e- 22 TRUE \n 6 full_quad sin2         0.0496    0.00307    16.2   6.71e- 29 TRUE \n 7 full_quad cos2         0.0298    0.00306     9.72  7.30e- 16 TRUE \n 8 full_quad sin3         0.0111    0.00306     3.61  4.97e-  4 TRUE \n 9 full_quad cos3         0.00768   0.00306     2.51  1.39e-  2 TRUE \n10 full_quad sin4         0.00153   0.00306     0.500 6.18e-  1 FALSE\n11 full_quad cos4         0.00188   0.00306     0.614 5.41e-  1 FALSE\n12 full_quad sin5         0.00168   0.00306     0.550 5.84e-  1 FALSE\n13 full_quad cos5         0.00189   0.00306     0.616 5.39e-  1 FALSE\n14 full_quad cos6         0.00186   0.00217     0.857 3.93e-  1 FALSE\n\n\nNow, note that the quadratic term is statistically significant. It was not significant when the cubic term was included in the model, but it is now.\n\n\nReduced Quadratic Trend: Model 1\nThis model omits all the Fourier (sine and cosine) terms that are not significant in the previous model.\n\n\nShow the code\nreduced_quadratic_lm1 &lt;- sim_ts |&gt;\n  model(reduced_quadratic1 = TSLM(log(x_t) ~ std_t + I(std_t^2) + \n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3))\nreduced_quadratic_lm1 |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 9 × 7\n  .model             term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;              &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_quadratic1 (Intercept)  2.82      0.00320    882.   1.18e-194 TRUE \n2 reduced_quadratic1 std_t        0.475     0.00216    220.   4.66e-135 TRUE \n3 reduced_quadratic1 I(std_t^2)  -0.00487   0.00241     -2.02 4.57e-  2 TRUE \n4 reduced_quadratic1 sin1         0.0326    0.00303     10.8  2.18e- 18 TRUE \n5 reduced_quadratic1 cos1         0.0389    0.00302     12.9  6.79e- 23 TRUE \n6 reduced_quadratic1 sin2         0.0496    0.00302     16.4  5.12e- 30 TRUE \n7 reduced_quadratic1 cos2         0.0298    0.00302      9.87 2.15e- 16 TRUE \n8 reduced_quadratic1 sin3         0.0111    0.00302      3.66 4.02e-  4 TRUE \n9 reduced_quadratic1 cos3         0.00768   0.00302      2.54 1.25e-  2 TRUE \n\n\nAll the terms are statistically significant in this model.\n\n\nReduced Quadratic Trend: Model 2\nThis model only includes the Fourier series terms where \\(i=1\\) or \\(i=2\\).\n\n\nShow the code\nreduced_quadratic_lm2 &lt;- sim_ts |&gt;\n  model(reduced_quadratic2 = TSLM(log(x_t) ~ std_t + I(std_t^2) + \n    sin1 + cos1 + sin2 + cos2))\nreduced_quadratic_lm2 |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 7 × 7\n  .model             term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;              &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_quadratic2 (Intercept)  2.82      0.00347    813.   1.54e-194 TRUE \n2 reduced_quadratic2 std_t        0.475     0.00234    203.   9.18e-134 TRUE \n3 reduced_quadratic2 I(std_t^2)  -0.00486   0.00261     -1.86 6.57e-  2 FALSE\n4 reduced_quadratic2 sin1         0.0326    0.00329      9.93 1.25e- 16 TRUE \n5 reduced_quadratic2 cos1         0.0389    0.00327     11.9  6.98e- 21 TRUE \n6 reduced_quadratic2 sin2         0.0496    0.00328     15.1  1.03e- 27 TRUE \n7 reduced_quadratic2 cos2         0.0298    0.00327      9.09 8.89e- 15 TRUE \n\n\nAs we would expect, all the terms are statistically significant. (They were all significant in the previous model, so it is not surprising that they are still significant.)\n\n\nReduced Quadratic Trend: Model 3\nThis model is reduced to include only the Fourier series terms for \\(i=1\\).\n\n\nShow the code\nreduced_quadratic_lm3 &lt;- sim_ts |&gt;\n  model(reduced_quadratic3 = TSLM(log(x_t) ~ std_t + I(std_t^2) + \n    sin1 + cos1)) \nreduced_quadratic_lm3 |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 5 × 7\n  .model             term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;              &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_quadratic3 (Intercept)  2.82      0.00695   406.    7.13e-167 TRUE \n2 reduced_quadratic3 std_t        0.474     0.00467   101.    5.16e-105 TRUE \n3 reduced_quadratic3 I(std_t^2)  -0.00475   0.00523    -0.908 3.66e-  1 FALSE\n4 reduced_quadratic3 sin1         0.0325    0.00658     4.95  2.97e-  6 TRUE \n5 reduced_quadratic3 cos1         0.0389    0.00656     5.94  3.97e-  8 TRUE \n\n\nAll the terms in this parsimonious model are statistically significant.",
    "crumbs": [
      "Lesson 4",
      "Transformations, Forecasting adn Bias Correction"
    ]
  },
  {
    "objectID": "chapter_5_lesson_4.html#linear-model",
    "href": "chapter_5_lesson_4.html#linear-model",
    "title": "Transformations, Forecasting adn Bias Correction",
    "section": "Linear Model",
    "text": "Linear Model\nEven though the quadratic terms were statistically significant, there is no visual indication that there is a quadratic trend in the time series after taking the logarithm. Hence, we will now fit a linear model to the log-transformed time series. We want to be able to compare the fit of models with a linear trend to the models with quadratic trends.\n\nFull Linear Model\nFirst, we fit a full model with a linear trend. We can express this model as:\n\\[\\begin{align*}\n  \\log(x_t) &= \\beta_0\n            + \\beta_1 \\left( \\frac{t - \\mu_t}{\\sigma_t} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_2 \\sin \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right)\n            + \\beta_3 \\cos \\left( \\frac{2\\pi \\cdot 1 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_4 \\sin \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right)\n            + \\beta_5 \\cos \\left( \\frac{2\\pi \\cdot 2 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_6 \\sin \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right)\n            + \\beta_7 \\cos \\left( \\frac{2\\pi \\cdot 3 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_8 \\sin \\left( \\frac{2\\pi \\cdot 4 t}{12} \\right)\n            + \\beta_9 \\cos \\left( \\frac{2\\pi \\cdot 4 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            + \\beta_{10} \\sin \\left( \\frac{2\\pi \\cdot 5 t}{12} \\right)\n            + \\beta_{11} \\cos \\left( \\frac{2\\pi \\cdot 5 t}{12} \\right) \\\\\n      & ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n            \\phantom{+ \\beta_{13} \\sin \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right)}\n            + \\beta_{12} \\cos \\left( \\frac{2\\pi \\cdot 6 t}{12} \\right)\n      + z_t\n\\end{align*}\\]\n\n\nShow the code\nfull_linear_lm &lt;- sim_ts |&gt;\n  model(full_linear = TSLM(log(x_t) ~ std_t + \n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3 \n    + sin4 + cos4 + sin5 + cos5 + cos6 )) # Note sin6 is omitted\nfull_linear_lm |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 13 × 7\n   .model      term        estimate std.error statistic   p.value sig  \n   &lt;chr&gt;       &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n 1 full_linear (Intercept)  2.82      0.00220  1281.    4.18e-203 TRUE \n 2 full_linear std_t        0.475     0.00222   214.    3.22e-129 TRUE \n 3 full_linear sin1         0.0326    0.00312    10.4   1.83e- 17 TRUE \n 4 full_linear cos1         0.0388    0.00311    12.5   9.93e- 22 TRUE \n 5 full_linear sin2         0.0496    0.00311    15.9   1.47e- 28 TRUE \n 6 full_linear cos2         0.0298    0.00311     9.56  1.41e- 15 TRUE \n 7 full_linear sin3         0.0110    0.00311     3.55  5.99e-  4 TRUE \n 8 full_linear cos3         0.00767   0.00311     2.47  1.54e-  2 TRUE \n 9 full_linear sin4         0.00153   0.00311     0.492 6.24e-  1 FALSE\n10 full_linear cos4         0.00188   0.00311     0.604 5.47e-  1 FALSE\n11 full_linear sin5         0.00168   0.00311     0.541 5.90e-  1 FALSE\n12 full_linear cos5         0.00189   0.00311     0.607 5.45e-  1 FALSE\n13 full_linear cos6         0.00186   0.00220     0.844 4.01e-  1 FALSE\n\n\n\n\nReduced Linear Trend: Model 1\nThis model excludes the terms that were not significant in the full model with a linear trend.\n\n\nShow the code\nreduced_linear_lm1 &lt;- sim_ts |&gt;\n  model(reduced_linear1 = TSLM(log(x_t) ~ std_t + \n    sin1 + cos1 + sin2 + cos2 + sin3 + cos3))\nreduced_linear_lm1 |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05)\n\n\n# A tibble: 8 × 7\n  .model          term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;           &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_linear1 (Intercept)  2.82      0.00217   1301.   2.91e-213 TRUE \n2 reduced_linear1 std_t        0.475     0.00219    217.   1.58e-135 TRUE \n3 reduced_linear1 sin1         0.0326    0.00307     10.6  4.53e- 18 TRUE \n4 reduced_linear1 cos1         0.0388    0.00306     12.7  1.63e- 22 TRUE \n5 reduced_linear1 sin2         0.0496    0.00307     16.2  1.18e- 29 TRUE \n6 reduced_linear1 cos2         0.0298    0.00306      9.71 4.25e- 16 TRUE \n7 reduced_linear1 sin3         0.0111    0.00306      3.61 4.86e-  4 TRUE \n8 reduced_linear1 cos3         0.00767   0.00306      2.50 1.39e-  2 TRUE \n\n\nAll of the terms are significant in this model.\n\n\nReduced Linear Trend: Model 2\nWe reduce the model to see if a more parsimonious model will suffice. This model contains a linear trend and the Fourier series terms associated with \\(i=1\\) and \\(i=2\\).\n\n\nShow the code\nreduced_linear_lm2 &lt;- sim_ts |&gt;\n  model(reduced_linear2 = TSLM(log(x_t) ~ std_t + \n    sin1 + cos1 + sin2 + cos2))\nreduced_linear_lm2 |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 6 × 7\n  .model          term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;           &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_linear2 (Intercept)   2.82     0.00234   1203.   1.37e-213 TRUE \n2 reduced_linear2 std_t         0.475    0.00237    201.   2.51e-134 TRUE \n3 reduced_linear2 sin1          0.0326   0.00332      9.81 2.14e- 16 TRUE \n4 reduced_linear2 cos1          0.0388   0.00331     11.7  1.35e- 20 TRUE \n5 reduced_linear2 sin2          0.0496   0.00332     14.9  1.88e- 27 TRUE \n6 reduced_linear2 cos2          0.0298   0.00331      8.98 1.46e- 14 TRUE \n\n\nAll the terms in this model are statistically significant.\n\n\nReduced Linear Trend: Model 3\nFinally, we fit a more reduced model that includes a linear trend and only the Fourier terms associated with \\(i=1\\).\n\n\nShow the code\nreduced_linear_lm3 &lt;- sim_ts |&gt;\n  model(reduced_linear3 = TSLM(log(x_t) ~ std_t + \n    sin1 + cos1)) \nreduced_linear_lm3 |&gt;\n  tidy() |&gt;\n  mutate(sig = p.value &lt; 0.05) \n\n\n# A tibble: 4 × 7\n  .model          term        estimate std.error statistic   p.value sig  \n  &lt;chr&gt;           &lt;chr&gt;          &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;\n1 reduced_linear3 (Intercept)   2.82     0.00463    609.   1.56e-186 TRUE \n2 reduced_linear3 std_t         0.474    0.00467    101.   7.71e-106 TRUE \n3 reduced_linear3 sin1          0.0325   0.00657      4.95 2.92e-  6 TRUE \n4 reduced_linear3 cos1          0.0389   0.00655      5.93 3.98e-  8 TRUE \n\n\nEach of these terms is significant.",
    "crumbs": [
      "Lesson 4",
      "Transformations, Forecasting adn Bias Correction"
    ]
  },
  {
    "objectID": "chapter_1_lesson_2.html#comparison-of-deterministic-and-stochastic-trends-in-time-series-10-min",
    "href": "chapter_1_lesson_2.html#comparison-of-deterministic-and-stochastic-trends-in-time-series-10-min",
    "title": "Plots Trends, and Seasonal Variation",
    "section": "Comparison of Deterministic and Stochastic Trends in Time Series (10 min)",
    "text": "Comparison of Deterministic and Stochastic Trends in Time Series (10 min)\n\nTime Series with a Stochastic Trend\nThe following app illustrates a few realizations of a stochastic trend.\n \n\nIf a stochastic time series displays an upward trend, can we conclude that trend will continue in the same direction? Why or why not?\n\n\n\nTime Series with a Deterministic Trend\nThe figure below illustrates realizations of a deterministic trend. The data fluctuate around a sine curve.",
    "crumbs": [
      "Lesson 2",
      "Plots Trends, and Seasonal Variation"
    ]
  }
]