---
title: "Introduction to Non-stationary Models and Differencing"
subtitle: "Chapter 7: Lesson 1"
format: html
editor: source
sidebar: false
---

```{r}
#| include: false
source("common_functions.R")
```

```{=html}
<script type="text/javascript">
 function showhide(id) {
    var e = document.getElementById(id);
    e.style.display = (e.style.display == 'block') ? 'none' : 'block';
 }
 
 function openTab(evt, tabName) {
    var i, tabcontent, tablinks;
    tabcontent = document.getElementsByClassName("tabcontent");
    for (i = 0; i < tabcontent.length; i++) {
        tabcontent[i].style.display = "none";
    }
    tablinks = document.getElementsByClassName("tablinks");
    for (i = 0; i < tablinks.length; i++) {
        tablinks[i].className = tablinks[i].className.replace(" active", "");
    }
    document.getElementById(tabName).style.display = "block";
    evt.currentTarget.className += " active";
 }    
</script>
```


## Learning Outcomes

{{< include outcomes/_chapter_7_lesson_1_outcomes.qmd >}}




## Preparation

-   Read Sections 7.1-7.2
-   Read [Prof. Frenzel's Blog Post](https://prof-frenzel.medium.com/kb-time-series-analysis-part-4-autoregressive-models-ed824838bd4c)


## Learning Journal Exchange (10 min)

-   Review another student's journal

-   What would you add to your learning journal after reading another student's?

-   What would you recommend the other student add to their learning journal?

-   Sign the Learning Journal review sheet for your peer


## Class Activity: Non-seasonal ARIMA Models (15 min)

### Effect of Differencing


In [Chapter 4 Lesson 2](https://byuistats.github.io/timeseries/chapter_4_lesson_2.html#McDonalds), 
we found that if we compute the first difference of the price of McDonald's stock from July 2020 through December 2023, the differences can be modeled as white noise.
Sometimes differencing can remove trends.

Consider the case of a random walk and a linear trend with white noise errors

#### Random Walk

<!-- Check Your Understanding -->

::: {.callout-tip icon=false title="Check Your Understanding"}

Consider the random walk

$$
  x_t = x_{t-1} + w_t
$$

where $\{w_t\}$ is a white noise process. 

-   What is the model for the first differences of this time series?

<!-- White noise -->

:::


#### Linear Trend with White Noise Errors

<!-- Check Your Understanding -->

::: {.callout-tip icon=false title="Check Your Understanding"}

Consider a time series with a linear trend and white noise errors.

$$
  x_t = a + bt + w_t
$$

where $\{w_t\}$ is a white noise process. 

-   What is the model for the first differences of this time series?
<!-- $\nabla x_t = x_t - x_{t-1} = w_t$, which is white noise. -->
-   What is the model obtained by subtracting $a+bt$ from this series?
<!-- $\nabla x_t = x_t - x_{t-1} = b + w_t - w_{t-1}$, which is an $MA(1)$ process -->
-   What are some potential concerns of using differencing to eliminate a deterministic trend?
<!-- It can lead to an unnecessarily complicated model. -->

:::


### Fitting an ARIMA Model when the Difference Model has a Non-Zero Mean

(See the last sentence in the first paragraph on page 138.)

### Differencing a Time Series or the Logarithm of a Time Series

If the difference of a time series demonstrates an increasing trend, taking the logarithm before differencing can eliminate the increasing variation in the differences. As an example, consider the Austrailian electricity production series given in the book.

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| output: false

pacman::p_load("tsibble", "fable", "feasts",
    "tsibbledata", "fable.prophet",
    "tidyverse", "patchwork")
cbe <- read_table("https://byuistats.github.io/timeseries/data/cbe.dat") |>
  select(elec) |>
  mutate(
    date = seq(
      ymd("1958-01-01"),
      by = "1 months",
      length.out = n()),
      year_month = tsibble::yearmonth(date)) |>
  as_tsibble(index = year_month)

cbe |>
  mutate(
    `Diff series` = elec - lag(elec),
    `Diff log-series` = log(elec) - lag(log(elec))) |>
  pivot_longer(
    cols = all_of(c("elec", "Diff series", "Diff log-series"))) |>
  mutate(name = factor(name, levels =c("elec","Diff series", "Diff log-series"))) |>
  ggplot(aes(x = date, y = value)) +
  geom_line() +
  facet_wrap(~name, ncol = 1, scales = "free", strip.position = "left") +
  labs(x = "Time", y = "") +
  scale_x_date(breaks = "5 years", date_labels = "%Y") +
  theme_bw()
```

```{r}
#| label: fig-BooksFigure7Dot1
#| fig-cap: "Plot of Australian electricity production, first differences, and first differences of the logarithm of the series"
#| echo: false

cbe |>
  mutate(
    `Diff series` = elec - lag(elec),
    `Diff log-series` = log(elec) - lag(log(elec))) |>
  pivot_longer(
    cols = all_of(c("elec", "Diff series", "Diff log-series"))) |>
  mutate(name = factor(name, levels =c("elec","Diff series", "Diff log-series"))) |>
  ggplot(aes(x = date, y = value)) +
  geom_line() +
  facet_wrap(~name, ncol = 1, scales = "free", strip.position = "left") +
  labs(x = "Time", y = "") +
  scale_x_date(breaks = "5 years", date_labels = "%Y") +
  theme_bw()
```



### Integrated Time Series of Order d

::: {.callout-note icon=false title="Definition of an Integrated Series of Order $d$, $I(d)$"}

We say that a time series is **integrated of order d** if the $d^{th}$ difference of $\{x_t\}$ is a white noise process $\{w_t\}$. Expressed differently, we write this as ${\nabla^d x_t = w_t}$. We denote an integrated time series of order $d$ as $I(d)$.

:::

Recall that $\nabla^d \equiv \left( 1 - \mathbf{B} \right)^d$.
So, either of the following can be used to indicate an integrated time series of order $d$:

\begin{align*}
  \nabla^d x_t &= w_t \\
  ~\\
  \left( 1 - \mathbf{B} \right)^d x_t &= w_t 
\end{align*}


#### Special Case: $I(1)$

<!-- Check Your Understanding -->

::: {.callout-tip icon=false title="Check Your Understanding"}

-   What model is given by the special case $I(1)$?
<!-- Random walk -->

:::


### Second-Order Differencing and Lagged Differences

A linear trend can be removed by first-order differencing. A curved trend can sometimes be eliminated by second order differencing. 

In some cases, a lagged difference is more appropriate. For example, if you have monthly data and need to remove additive seasonal effects, you may want to take a difference with a lag of 12. This subtracts sequential January observations from each other. This models the year-over-year growth. 

Notice that taking a lag 12 difference 

$$  
  \left( 1 - \mathbf{B}^{12} \right) x_t  = x_t - x_{t-12}
$$

is very different from taking the twelfth differences 

\begin{align*}
  \nabla^{12} x_t 
    &= \left( 1 - \mathbf{B} \right)^{12} x_t \\
    &= x_t - 12 x_{t-1} + 66 x_{t-2} - 220 x_{t-3} + 495 x_{t-4} - 792 x_{t-5} + 924 x_{t-6} \\
    & ~~~~~~~~~~~~~~~~~~~ - 792 x_{t-7} + 495 x_{t-8} - 220 x_{t-9} + 66 x_{t-10} - 12 x_{t-11} + x_{t-12}
\end{align*}


### ARIMA Process

#### ARIMA 

::: {.callout-note icon=false title="Definition of an ARIMA Process"}

A time series is said to follow an **$ARIMA(p,d,q)$ process** if the $d^{th}$ differences of the time series follow an $ARMA(p,q)$ process.

:::

Suppose we let $y_t = \left( 1 - \mathbf{B} \right)^d x_t$. The series $\{y_t\}$ follows an $ARMA(p,q)$ process if
$\theta_p \left(\mathbf{B} \right) y_t = \phi_q \left(\mathbf{B} \right)w_t$.

Substituting, we find that $\{x_t\}$ follows an $ARIMA(p,d,q)$ process if

$$
    \theta_p \left(\mathbf{B} \right) \left( 1 - \mathbf{B} \right)^d x_t = \phi_q \left(\mathbf{B} \right) w_t
$$

where $\theta_p \left(\mathbf{B} \right)$ and $\phi_q \left(\mathbf{B} \right)$ are polynomials of orders $p$ and $q$, respectively.


#### Special Case: $IMA(d,q)$ Process

::: {.callout-note icon=false title="Definition of an IMA Process"}

A time series $\{x_t\}$ follows an **$IMA(d,q)$ process** if it can be expressed as:

$$
    \left( 1 - \mathbf{B} \right)^d x_t = \phi_q \left(\mathbf{B} \right) w_t
$$

Note that $IMA(d,q) \equiv ARIMA(0,d,q)$.

:::

<!-- Check Your Understanding -->

::: {.callout-tip icon=false title="Check Your Understanding"}

-   Solve for $x_t$ in an $IMA(1,1)$ process.
<!-- $x_t = x_{t-1} + w_t + \beta w_{t-1}$ -->

:::

#### Special Case: $ARI(p,d)$ Process

::: {.callout-note icon=false title="Definition of an ARI Process"}

A time series $\{x_t\}$ follows an **$ARI(p,d)$ process** if it can be expressed as:

$$
    \theta_p \left(\mathbf{B} \right) \left( 1 - \mathbf{B} \right)^d x_t = w_t
$$

Note that $ARI(p,d) \equiv ARIMA(p,d,0)$.

:::

<!-- Check Your Understanding -->

::: {.callout-tip icon=false title="Check Your Understanding"}

-   Solve for $x_t$ in an $ARI(1,1)$ process.
<!-- $x_t = \alpha x_{t-1} + x_{t-1} - \alpha x_{t-2} + w_t $ -->

:::


### Simulating an ARIMA Process

We can simulate data from the ARIMA process

$$
  x_t = 0.5 x_{t-1} + x_{t-1} - 0.5 x_{t-2} + w_t + 0.3 w_{t-1}
$$

using the following R code.

```{r}
set.seed(1)
n <- 10000
x <- rnorm(n)
w <- rnorm(n)
for (i in 3:n) {
  x[i] <- 0.5 * x[i - 1] + x[i - 1] - 0.5 * x[i - 2] + w[i] + 0.3 * w[i - 1]
}
arima(x, order = c(1, 1, 1))
```

This is an ARIMA(1,1,1) process with parameters $\alpha = 0.5$ and $\beta = 0.3$.


<!-- Check Your Understanding -->

::: {.callout-tip icon=false title="Check Your Understanding"}

-   Modify the code above to simulate from an $ARIMA(2,1,2)$ process with parameters $\alpha_1 = 0.5$, $\alpha_2 = 0.2$, $\beta_1 = 0.4$, and $\beta_2 = 0.1$.

<!-- $$(1 - \alpha_1 B - \alpha_2 B^2) (1-B) x_t = (1 + \beta_1 B + \beta_2 B^2) w_t$$ -->

:::



## Class Activity: Fitting an ARIMA Process - Exchange Rates (10 min)

The data file [exchange_rates.parquet](https://byuistats.github.io/timeseries/data/exchange_rates.parquet) gives the exchange rates for foreign currencies. The daily-observed values in the time series are the amount in the foreign currency equivalent to one U. S. dollar. We will consider the exchange rates to convert one dollar into Euros.

```{r}
#| label: tbl-exchangeRatesTS
#| tbl-cap: "Select values of the time series representing the exchange rate to convert US$1 into Euros"
#| code-fold: true
#| code-summary: "Show the code"

exchange_ts <- rio::import("data/exchange_rates.parquet") |>
  filter(currency == "USD.EUR") |>
  as_tsibble(index = date) |>
  na.omit()

exchange_ts |>
  display_partial_table(6,3)
```


```{r}
#| label: fig-exchangeRateTimePlot
#| fig-cap: "Time plot of the exchange rate to convert US$1 into Euros"
#| code-fold: true
#| code-summary: "Show the code"
 
exchange_ts |>
  autoplot(.vars = rate) + labs(title = exchange_ts$currency[1])
```


```{r}
#| label: fig-exchangeRateACFpacf
#| fig-cap: "Correlogram and partial correlogram for the time series representing the exchange rate to convert US$1 into Euros"
#| code-fold: true
#| warning: false
#| code-summary: "Show the code"

acf_plot <- exchange_ts |> select(rate) |> ACF() |> autoplot(var = .resid)

pacf_plot <- exchange_ts |> select(rate) |> PACF() |> autoplot(var = .resid)

acf_plot | pacf_plot

```



```{r}
# Fit the ARIMA Model

exchange_model <- exchange_ts |>
  model(
    auto = ARIMA(rate ~ 1 + pdq(0:2,0:1,0:2) + PDQ(0, 0, 0)),
    
    a000 = ARIMA(rate ~ 1 + pdq(0,0,0) + PDQ(0, 0, 0)),
    a001 = ARIMA(rate ~ 1 + pdq(0,0,1) + PDQ(0, 0, 0)),
    a002 = ARIMA(rate ~ 1 + pdq(0,0,2) + PDQ(0, 0, 0)),
    a100 = ARIMA(rate ~ 1 + pdq(1,0,0) + PDQ(0, 0, 0)),
    a101 = ARIMA(rate ~ 1 + pdq(1,0,1) + PDQ(0, 0, 0)),
    a102 = ARIMA(rate ~ 1 + pdq(1,0,2) + PDQ(0, 0, 0)),
    a200 = ARIMA(rate ~ 1 + pdq(2,0,0) + PDQ(0, 0, 0)),
    a201 = ARIMA(rate ~ 1 + pdq(2,0,1) + PDQ(0, 0, 0)),
    a202 = ARIMA(rate ~ 1 + pdq(2,0,2) + PDQ(0, 0, 0)),
    
    a011 = ARIMA(rate ~ 1 + pdq(0,1,1) + PDQ(0, 0, 0)),
    a012 = ARIMA(rate ~ 1 + pdq(0,1,2) + PDQ(0, 0, 0)),
    a110 = ARIMA(rate ~ 1 + pdq(1,1,0) + PDQ(0, 0, 0)),
    a111 = ARIMA(rate ~ 1 + pdq(1,1,1) + PDQ(0, 0, 0)),
    a112 = ARIMA(rate ~ 1 + pdq(1,1,2) + PDQ(0, 0, 0)),
    a210 = ARIMA(rate ~ 1 + pdq(2,1,0) + PDQ(0, 0, 0)),
    a211 = ARIMA(rate ~ 1 + pdq(2,1,1) + PDQ(0, 0, 0)),
    a212 = ARIMA(rate ~ 1 + pdq(2,1,2) + PDQ(0, 0, 0))
    )
```

Here is one way to determine which model is selected by the "auto" process.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

exchange_model |>
  select(auto)
```

We now examine all the fitted models to determine the value of the residual mean squared error (`sigma2`), log-likelihood, AIC, AICc, and BIC. For the log-likelihood, larger values are preferable. For all other measures, smaller values are preferred.

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| output: false

exchange_model |>
  glance()
```


```{r}
#| label: tbl-exchangeRateSeveralFittedModels
#| tbl-cap: "Values used in the model selection process for the time series representing the exchange rate to convert US$1 into Euros"
#| echo: false

exchange_model |>
  glance() |>
  display_arima_models_1()
```



```{r}
exchange_model |>
  glance() |>
  display_arima_models_1()
```

Suppose we choose to apply the "auto" model, which is $ARIMA(1,1,1)$.
The model parameters are summarized here:

```{r}
#| code-fold: true
#| code-summary: "Show the code"
#| output: false

exchange_model |>
  select(auto) |>
  coefficients()
```

```{r}
#| echo: false

exchange_model |>
  select(auto) |>
  coefficients() |>
  display_table()
```


The following plots give the acf and pacf of the residuals from this model.

```{r}
#| label: fig-exchangeRateResidACFpacf
#| fig-cap: "Correlogram and Partial Correlogram for the residuals from the ARIMA(1,1,1) model for the daily exchange rates to convert US$1 into Euros"
#| code-fold: true
#| code-summary: "Show the code"
#| warning: false

model_resid <- exchange_model |>
  select(auto) |>
  residuals()

acf_plot <- model_resid |> ACF() |> autoplot(var = .resid)

pacf_plot <- model_resid |> PACF() |> autoplot(var = .resid)

acf_plot | pacf_plot
```

Here is a histogram of the residuals from our model.

```{r}
#| label: fig-exchangeRateResidHistogram
#| fig-cap: "Histogram of the residuals from the ARIMA(1,1,1) model for the daily exchange rates to convert US$1 into Euros"
#| code-fold: true
#| code-summary: "Show the code"

ggplotly(model_resid |>
  mutate(density = dnorm(.resid, mean(model_resid$.resid), sd(model_resid$.resid))) |>
  ggplot(aes(x = .resid)) +
    geom_histogram(aes(y = after_stat(density)),
        color = "white", fill = "#56B4E9", binwidth = 0.001) +
    geom_line(aes(x = .resid, y = density)) +
    theme_bw() +
    labs(
      x = "Values",
      y = "Frequency",
      title = "Histogram of Residuals"
    ) +
    theme(
      plot.title = element_text(hjust = 0.5)
    ))
```



<!-- Check Your Understanding -->

::: {.callout-tip icon=false title="Check Your Understanding"}

-   Write the fitted model:
$$
  x_t = ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~
$$
-   Does the model provide an appropriate fit for the data?

:::

Here is a forecast for the next 7 days based on our model.

```{r}
exchange_model <- exchange_ts |> model(ARIMA(rate))

temps_forecast <- exchange_model |> forecast(h = "7 days")



temps_forecast |>
  autoplot(exchange_ts, level = 95) +
  geom_line(aes(y = .fitted, color = "Fitted"),
    data = augment(exchange_model)) +
  scale_color_discrete(name = "") +
  labs(
    x = "Date",
    y = "Exchange Rate",
    title = "Exchange Rate for Converting US$1 to Euros",
    subtitle = "7-Day Forecast Based on our AR(1,1,1) Model"
  ) +
  theme_minimal() +
  theme(
    plot.title = element_text(hjust = 0.5),
    plot.subtitle = element_text(hjust = 0.5)
  )
```






## Small-Group Activity: Fitting an ARIMA Process - Microsoft Stock Prices (20 min)

A time series given the daily closing price for Microsoft (MSFT) stock is given below. To handle the gaps in the data, we define a new variable, `t`, which gives the observation number.

```{r}
#| code-fold: true
#| code-summary: "Show the code"

# Set symbol and date range
symbol <- "MSFT"                # Abercrombie & Fitch stock trading symbol
date_start <- "2020-01-01"
date_end <- "2024-03-28"

# Fetch stock prices
df_stock <- tq_get(symbol, from = date_start, to = date_end, get = "stock.prices")

# Transform data into tsibble
stock_ts <- df_stock |>
  mutate(
    dates = date, 
    value = close
  ) |>
  dplyr::select(dates, value) |>
  as_tibble() |> 
  arrange(dates) |>
  mutate(t = 1:n()) |>
  as_tsibble(index = t, key = NULL)
```


<!-- Check Your Understanding -->

::: {.callout-tip icon=false title="Check Your Understanding"}

Using the daily closing prices of Microsoft stock, do the following.

-   Make a time plot of the data.
-   Create a correlogram and partial correlogram of the stock prices.
-   Fit candidate $ARIMA(p,d,q)$ models to the data.
-   Choose the "best" model, and justify your selection.
-   Generate a correlagram and partial correlogram of the residuals from your chosen model.
-   Make a histogram of the residuals from your model.
-   Did your your model account for the  the time series?
-   Predict the value 15 trading days in the future.

:::


```{r}
#| include: false
#| echo: false
#| eval: false

# Generate time series plot using plot_ly
plot_ly(stock_ts, x = ~dates, y = ~value, type = 'scatter', mode = 'lines') |>
  layout(
    xaxis = list(title = "Date"),
    yaxis = list(title = "Value"),
    title = paste0("Time Plot of ", symbol, " Daily Closing Price (", format(ymd(date_start), "%d %b %Y"), " - ", format(ymd(date_end), "%d %b %Y"),")")
  )

stock_model <- stock_ts |>
  model(
    auto = ARIMA(value ~ 1 + pdq(0:2,0:1,0:2) + PDQ(0, 0, 0)),
    
    a000 = ARIMA(value ~ 1 + pdq(0,0,0) + PDQ(0, 0, 0)),
    a001 = ARIMA(value ~ 1 + pdq(0,0,1) + PDQ(0, 0, 0)),
    a002 = ARIMA(value ~ 1 + pdq(0,0,2) + PDQ(0, 0, 0)),
    a100 = ARIMA(value ~ 1 + pdq(1,0,0) + PDQ(0, 0, 0)),
    a101 = ARIMA(value ~ 1 + pdq(1,0,1) + PDQ(0, 0, 0)),
    a102 = ARIMA(value ~ 1 + pdq(1,0,2) + PDQ(0, 0, 0)),
    a200 = ARIMA(value ~ 1 + pdq(2,0,0) + PDQ(0, 0, 0)),
    a201 = ARIMA(value ~ 1 + pdq(2,0,1) + PDQ(0, 0, 0)),
    a202 = ARIMA(value ~ 1 + pdq(2,0,2) + PDQ(0, 0, 0)),
    
    a011 = ARIMA(value ~ 1 + pdq(0,1,1) + PDQ(0, 0, 0)),
    a012 = ARIMA(value ~ 1 + pdq(0,1,2) + PDQ(0, 0, 0)),
    a110 = ARIMA(value ~ 1 + pdq(1,1,0) + PDQ(0, 0, 0)),
    a111 = ARIMA(value ~ 1 + pdq(1,1,1) + PDQ(0, 0, 0)),
    a112 = ARIMA(value ~ 1 + pdq(1,1,2) + PDQ(0, 0, 0)),
    a210 = ARIMA(value ~ 1 + pdq(2,1,0) + PDQ(0, 0, 0)),
    a211 = ARIMA(value ~ 1 + pdq(2,1,1) + PDQ(0, 0, 0)),
    a212 = ARIMA(value ~ 1 + pdq(2,1,2) + PDQ(0, 0, 0))
    ) 

stock_model |>
  glance() 

stock_model |>
  select(auto)

#########

stock_model |>
  glance() |>
  display_arima_models()

####

model_resid <- stock_model |>
  select(auto) |>
  residuals()

acf(model_resid$.resid, main = "ACF of Residuals from ARIMA Model")

pacf(model_resid$.resid, main = "ACF of Residuals from ARIMA Model")


ggplotly(model_resid |>
  mutate(density = dnorm(.resid, mean(model_resid$.resid), sd(model_resid$.resid))) |>
  ggplot(aes(x = .resid)) +
    geom_histogram(aes(y = after_stat(density)),
        color = "white", fill = "#56B4E9", binwidth = 2) +
    geom_line(aes(x = .resid, y = density)) +
    theme_bw() +
    labs(
      x = "Values",
      y = "Frequency",
      title = "Histogram of Residuals"
    ) +
    theme(
      plot.title = element_text(hjust = 0.5)
    ))

######

stock_ts$diff = stock_ts$value - lag(stock_ts$value) 


ggplotly(stock_ts |>
  na.omit() |>
  autoplot(.vars = diff) + 
    labs(
      title = paste("Time Plot of Differences in Daily", symbol, "Stock Prices"),
      subtitle = 
        paste0(
          format(ymd(date_start), "%d %b %Y"),
            " - ",
          format(ymd(date_end), "%d %b %Y")
          ),
      x = "Date",
      y = "Difference",
    ) +
    theme_minimal() +
    theme(
      plot.title = element_text(hjust = 0.5),
      plot.subtitle = element_text(hjust = 0.5)
    ))
```









## Homework Preview (5 min)

-   Review upcoming homework assignment
-   Clarify questions




::: {.callout-note icon=false}

## Download Homework

<a href="https://byuistats.github.io/timeseries/homework/homework_7_1.qmd" download="homework_7_1.qmd"> homework_7_1.qmd </a>

:::





<a href="javascript:showhide('Solutions1')"
style="font-size:.8em;">Class Activity</a>
  
::: {#Solutions1 style="display:none;"}
    

:::




<a href="javascript:showhide('Solutions2')"
style="font-size:.8em;">Class Activity</a>
  
::: {#Solutions2 style="display:none;"}
    

:::




<a href="javascript:showhide('Solutions3')"
style="font-size:.8em;">Class Activity</a>
  
::: {#Solutions3 style="display:none;"}
    

:::






# Great Resource

https://prof-frenzel.medium.com/kb-time-series-analysis-part-4-autoregressive-models-ed824838bd4c




# KB Time-Series Analysis ‚Äî Part 4 ‚Äî Autoregressive Models

Gear up for an in-depth exploration of Autoregressive Models, the benchmark for advanced time series forecasting. Today, we‚Äôll uncover their fundamental building blocks (AR and MA) and discuss their elegant integration into the ARIMA model, before we round it off with a hands-on R case study. Are you ready? Let‚Äôs go! 

### Need for Time Series Analysis

Time series analysis is fundamentally the study of data that captures how things evolve over time. Whether it‚Äôs predicting stock market trends in investment management, anticipating housing market shifts in real estate, or optimizing inventory in operations, the ability to forecast future events based on historical data is invaluable. This analysis often revolves around identifying patterns such as trends, which signify a general direction over a period, or seasonality, which reflects regular, predictable changes tied to seasonal factors. Understanding these basic concepts is the foundation for any professional looking to make informed predictions and decisions in their respective fields.


## Time Series Analysis Workflow

To make the most of this article, I suggest familiarizing yourself with my previous writings on the topic.

### Fundamentals of Autoregressive (AR) Models

The basic principle behind an AR model is its use of past data points to predict future values. Mathematically, this involves a model where the value of a variable at a certain time is expressed as a linear combination of its previous values. This formulation hinges on the assumption that historical patterns have a bearing on future trends.

To illustrate, consider a simple AR model, AR(1), where the prediction for a time series at time t is a function of its value at t-1, plus an error term. This error term accounts for randomness or unpredictability in the data. As the model complexity increases, so does the number of past values it considers. An AR(2) model, for instance, would incorporate the values at both t-1 and t-2 in its predictions.

In practice, the effectiveness of an AR model is largely dependent on the nature of the time series data at hand. For time series with strong historical dependence, AR models can be remarkably proficient at capturing the underlying patterns. However, they have limitations, particularly when dealing with non-stationary data, where statistical properties like the mean and variance change over time.

### Moving Average (MA) Models: Complementing AR Models

Unlike AR models that rely on past values of the variable itself, MA models use past error terms as predictors. These error terms are the differences between the actual values and the model‚Äôs predicted values. An MA model, essentially, tries to capture the ‚Äòshocks‚Äô or ‚Äòsurprises‚Äô in the time series data, which are then used to forecast future values.

The mathematical formulation of an MA model is straightforward. In an MA(1) model, for instance, the value at time t is a function of the average of the error terms at times t-1 and t. As with AR models, increasing the order of an MA model increases its complexity and the number of past error terms it considers.

AR models excel in scenarios where the value of a series is highly dependent on its previous values. In contrast, MA models are more useful for modeling series where the impact of a random shock or event dissipates over time. In the messy world of real-world data, there‚Äôs no single champion. Each model brings its own set of skills to the table, and their effectiveness hinges on the specific characteristics of the data we‚Äôre trying to understand.

One of the most powerful applications in time series analysis is combining AR and MA models, leading to the development of ARMA (Autoregressive Moving Average) models. ARMA models combine both the reliance of a series on its own past (as in AR) and the influence of random shocks (as in MA). In practice, ARMA models are widely used in economic and financial forecasting, where both historical trends and random events play significant roles in shaping future outcomes.

### Integrated Models: Tackling Non-Stationarity through Differencing

The effectiveness of AR and MA models hinges on the stationarity of the data being analyzed. Applying differencing can make them applicable to a wider range of data by removing trends and seasonal structures. The degree of differencing required can range from none to one or more. If the data is already stationary, no differencing is needed. However, if the data exhibits non-stationarity, such as a trend or seasonality, one or more differencing steps may be necessary to make the data stationary.

Practical examples of differencing in action can be observed in economic data analysis. Consider a time series representing a country‚Äôs quarterly GDP growth. This series might exhibit trends and seasonal patterns, making it non-stationary. By applying differencing, these elements are minimized, revealing a clearer picture of the underlying economic conditions and trends. Data transformation aligns the data with the requirements of forecasting models, thereby enabling more accurate predictions of future economic trends.

## Introducing ARIMA Models: The Ducati of Forecasting Models

ARIMA (Autoregressive Integrated Moving Average) models stand as a comprehensive tool, integrating the concepts of AR, MA, and differencing. ARIMA models are notable for their ability to model a wide range of time series data, including non-stationary series, by incorporating differencing directly into their structure.

In the ARIMA(p,d,q) framework, ‚Äòp‚Äô denotes the number of autoregressive terms, ‚Äòd‚Äô the number of nonseasonal differences needed to achieve stationarity, and ‚Äòq‚Äô the number of lagged forecast errors in the prediction equation.


### ARIMA Model and Parameters

The d parameter is the degree of differencing required to make the time series stationary. This involves subtracting the previous value from the current value d times. For example, a first-order difference (d=1) would be sufficient if the series has a stable mean but a varying variance. To determine the right d value, the Augmented Dickey-Fuller (ADF) test can be employed. This test checks for the presence of a unit root in the series, indicating non-stationarity. The number of differencing required to achieve stationarity, noted by the ADF test, will guide your appropriate choice for d.

The q parameter indicates the order of the moving average part. This specifies the number of lagged forecast errors that the model uses. A model with q=1 uses the previous forecast error in predicting future values (see MA(1) model). To determine the q values, you will need the Autocorrelation Function (ACF) plot. ACF measures the correlation between time series observations at different time lags. For an MA model, the plot will show significant correlations up to the lag equal to the order of the MA process, suggesting a suitable value for your q.

Lastly, the p parameter denotes the order of the autoregressive part, representing the number of lagged terms of the series included in the model. For instance, an ARIMA model with p=1 includes one lagged value of the series (see AR(1) model). To determine the appropriate p values, you can utilize tools like the Partial Autocorrelation Function (PACF). The PACF plot helps identify the extent of the lag in an autoregressive model by showing the correlation of the series with its own lagged values, controlling for the values of the time series at all shorter lags. The point where the PACF chart crosses the significance threshold for the first time indicates an appropriate value for p.

## How to Read ACF and PACF Plots

When interpreting Autocorrelation Function (ACF) and Partial Autocorrelation Function (PACF) plots, the goal is to identify the appropriate number of autoregressive (AR) terms (p) and moving average (MA) terms (q) for an ARIMA model. If you observe a tailing off in the ACF plot, this suggests an AR (Autoregressive) model is appropriate. In this case, the point where PACF cuts off will indicate the order ‚Äòp‚Äô for your AR(p) model. If the tailing off occurs in the PACF plot, it points towards an MA (Moving Average) model. Should both ACF and PACF plots show tailing off, this is indicative of an ARMA (Autoregressive Moving Average) model being a suitable choice.

### AR process

Example: AR(2) process

In an AR process, you‚Äôll typically notice the ACF diminishing gradually in a geometric pattern. Meanwhile, the PACF will show significance only at a limited number of initial lags. In our example, you observe two significant lags in the PACF, followed by a notable decrease in significance, it suggests an AR process of order 2, or AR(2). This means the current value in the series is influenced by its two immediate past values.

### MA process

Example: MA(1) process

Opposite to the AR process, an MA process is indicated by a PACF that declines geometrically while the ACF plot shows a few significant lags. In our example, you see the PACF changing somewhat gradually and the ACF presenting two significant lags before it diminishes, you‚Äôre likely looking at an MA(1) process. Note that ACF plots usually start at 0. This pattern suggests that autocorrelations after the initial lags are not significant and thus, don‚Äôt add valuable predictive information.

### ARMA process


Example: ARMA(1,1) process

Differentiating between AR and MA processes is usually straightforward, but real-world data often presents more complex scenarios. When both the ACF and PACF plots do not clearly show tailing off or cutting off it‚Äôs an indication to consider an ARMA model. The ARMA model combines elements of both AR and MA models, allowing for more flexibility in capturing the data‚Äôs characteristics.

To determine the AR part (p value) in an ARMA model, examine the PACF plot above. The spikes at specific lags suggest an AR(1) or AR(3) model. Similarly, for the MA part (q value), we see the spikes at lags 1 and 3 would indicate an MA(1) or MA(3).

Experimenting with various combinations of p and q, such as ARMA(1,1), ARMA(1,3), ARMA(3,1), and ARMA(3,3), is the most practical approach here. Through this experimentation, you assess each model‚Äôs performance using diagnostic tools and select the best model based on criteria like the RMSE or AIC (Akaike Information Criterion). In the end, it‚Äôs a process of trial and error to find the optimal ARMA model configuration for your specific data set.

## Types of ARIMA Models

As you‚Äôve likely gathered, it can be a bit of a maze with all the ARIMA models available. That‚Äôs why I‚Äôve put together a clear guide to the most common ones you‚Äôre likely to encounter, along with their practical use cases.

### 1: The White Noise Model ‚Äî ARIMA(0,0,0)

This model assumes that the time series is completely random. The values are uncorrelated, and each value is not influenced by past values. It‚Äôs often used as a null hypothesis against which more complex models are tested. In financial markets, for example, the white noise model can serve as a benchmark to show that a particular stock‚Äôs returns are purely random, suggesting there are no patterns to exploit for profit.

### 2: The Random Walk Model ‚Äî ARIMA(0,1,0)

The random walk model posits that the current observation is equal to the previous one plus a random error term. If you‚Äôre examining stock prices, this model would suggest that today‚Äôs price is yesterday‚Äôs price plus some random change, or ‚Äòdrift‚Äô, which is the average change over time. This model is commonly applied in financial time series where you expect a degree of continuity along with random variation.

### 3: The Simple Exponential Smoothing Model ‚Äî ARIMA(0,1,1)

This ARIMA model captures patterns in time series that lack a trend and are better smoothed over time. It‚Äôs akin to placing a weighted average on past errors to predict future values. Retail businesses might use this model for inventory forecasting where sales volume is steady and seasonal or trend-based changes are minimal.

### 4: The First-Order Autoregressive Model ‚Äî ARIMA(1,0,0)

Here, the present value is a function of the immediate previous value with a constant term added. This constant could represent a systematic, expected change in the data over time, which is not due to the past values. An ARIMA(1,0,0) might be used to forecast electricity demand where a stable, predictable pattern emerges over time.

### 5: The Mixed Model ‚Äî ARIMA(1,1,1)

This model integrates both AR and MA components. It works well for data that shows signs of both autoregressive and moving average properties. An example use case would be for analyzing and forecasting economic metrics like unemployment rates, where both past values and past forecast errors play a role in shaping current figures.

### 6: The Differenced First-Order Autoregressive Model ‚Äî ARIMA(1,1,0)

This model is a hybrid that accounts for changes in the mean level of a series over time. The ‚Äòdifferencing‚Äô aspect helps stabilize the mean, while the AR component models the autocorrelation. This might be used in weather forecasting, where temperature data could have a trend (seasonality) that needs differencing to reveal patterns in temperature changes over successive days.

### 7: The Exponential Smoothing with Growth Model ‚Äî ARIMA(0,1,1)

This model extends simple exponential smoothing by including a constant to capture trends, essentially smoothing with a ‚Äòdrift‚Äô. It‚Äôs suitable for data with noisy fluctuations around a slowly varying mean. Retailers might use it to forecast sales that have a slow growth or decline over time, where the most recent observations are weighted more heavily.

### 8: The Linear Exponential Smoothing Model ‚Äî ARIMA(0,2,1) or (0,2,2)

These ARIMA models add an additional layer of differencing, making them suitable for data that shows curvature or a change in the trend. They‚Äôre akin to fitting a linear trend to the data. An application could be in the context of tracking the progression of a new product‚Äôs sales, where you expect initial rapid growth that stabilizes over time.

### 9: The Damped-Trend Linear Exponential Smoothing Model ‚Äî ARIMA(1,1,2)

This model captures scenarios where trends are present but are expected to plateau or ‚Äòdampen‚Äô as time progresses. It‚Äôs particularly useful when forecasting long-term trends that are expected to slow down in the future, like the saturation of a new market after an initial phase of rapid growth.

### 10: The Seasonal Moving Average Model with Nonseasonal Smoothing ‚Äî ARIMA(0,1,1)(0,0,2)[12]

The ARIMA(0,1,1)(0,0,2)[12] model brings an intriguing complexity to forecasting. This model is essentially a nonseasonal ARIMA model with an added seasonal moving average component. The nonseasonal part, ARIMA(0,1,1), suggests that the data, once differenced, can be smoothed using one lag of the forecast errors ‚Äî indicative of a simple exponential smoothing approach. The seasonal component, (0,0,2)[12], adds a layer to account for seasonal effects at lags 12 and 24, which might be significant in a yearly cycle, hence the [12] indicating a seasonal period of 12.

A detailed use case for this model could involve monthly sales data for a retail company with prominent seasonal patterns, such as increased sales during holidays and a predictable off-season. The model would smooth out random fluctuations on a month-to-month basis while also capturing the impact of the sales cycles known to repeat yearly. The double seasonal moving average terms allow for modeling the nuanced pattern that might occur at the end of the year and then again in the subsequent year, acknowledging that such effects can have a two-year cycle. This is particularly useful when a single year‚Äôs data reflects an anomaly, ensuring the forecasting model is not overly influenced by one-off events.

## Data Science 101: don‚Äôt neglect model fitting and diagnostic checks!

### Model Fitting and Comparison: 

Once potential models are identified, they are fitted to the data, and their performance is evaluated using metrics like the Akaike Information Criterion (AIC) or Bayesian Information Criterion (BIC). Lower values of these metrics generally indicate a better model fit, providing a balance between model complexity and goodness of fit.

### Residual Analysis: 

After fitting the model, the residuals, or the differences between the observed values and the model‚Äôs predictions, are analyzed. This step is crucial as ideally, residuals should exhibit properties similar to white noise, indicating that the model has captured the underlying patterns effectively.

### Diagnostic Checks: 

The model undergoes various diagnostic checks, including tests for residual autocorrelation (like the Ljung-Box test), stationarity, and normality. These checks ensure that no significant information is left unexplained by the model.

### Cross-Validation: 

To assess the predictive performance of the model, time series cross-validation is employed. This involves fitting the model on a training dataset and then evaluating its forecasts on a validation dataset, providing a robust measure of its forecasting capabilities.

### Sensitivity Analysis: 

The model‚Äôs sensitivity to various parameters and underlying assumptions is tested. This analysis is essential for understanding how changes in the data or model assumptions might impact the forecasts, thereby assessing the robustness of the model.


## Case Study in R

Admittedly, I had my doubts about using a real dataset instead of a pre-packaged educational one. However, I‚Äôm now confident that working with real-world data is a great lesson for understanding the practical implications of auto.arima and our decision-making processes. Therefore, we will be looking at New home sales demand in the USA for the last 60 years, pulled from ‚Äòwww.huduser.gov', but you can also download it from my GitHub: üìÅ New Homes Sold Data.

I‚Äôll be utilizing the ‚Äòtseries‚Äô libraries, a more comprehensive time series package suitable for a broader range of tasks, including data manipulation, visualization, and forecasting. If you‚Äôre dealing with financial data like stock prediction, I suggest using the ‚Äòxts‚Äô library, which is specifically designed for high-frequency time series data, such as financial data. It‚Äôs more efficient than the ‚Äòtseries‚Äô package when working with large time series datasets.

I‚Äôve compiled a quick guide to the process, hoping it proves useful.


### ARIMA Coding Steps

```{r}
#| eval: false

# Load necessary libraries
library(forecast)
library(tseries)

# Data Loading-------------
data <- read.csv("NewHomesSold.csv", header = TRUE)
data$Month <- as.Date(as.yearmon(data$Month, "%m/%d/%Y"))
start_year <- year(data$Month[1])
data_ts <- ts(data$NewSold_US, start=c(start_year,1), frequency=12)

# Data Splitting for Forecast-------------
forecast_length <- 6
train_data <- ts(data_ts[1:(length(data_ts) - forecast_length)], start=c(start_year,1), frequency=12)
test_data <- ts(data_ts[(length(data_ts) - (forecast_length-1)):length(data_ts)], start=c(start_year + (length(data_ts) - forecast_length) %/% 12, (length(data_ts) - forecast_length) %% 12 + 1), frequency=12)

# Data Visualization-------------
plot(train_data, main="Training Data - New Homes Sold in the US", xlab="Year", ylab="Number of Homes Sold", col = 'blue')

# Time Series Decomposition-------------
decomposed_data <- stl(train_data, s.window="periodic")
plot(decomposed_data)
```

### New Home Sold

Decomposition: New Home Sold
The decomposition of the time series for new home sales in the USA reveals pronounced seasonality, potentially linked to cyclical factors like the fiscal calendar and climatic conditions that influence home buying behaviors. The trend, more indicative of the long-term market direction, reflecting the underlying growth or contraction within the housing market. It integrates the impact of nationwide economic policies, lending practices, demographic trends, or policy changes. The residuals, though seemingly random, are insightful, potentially indicating extraordinary events or data inconsistencies that call for additional further analysis.

```{r}
#| eval: false

# Stationarity Checks-------------
# ADF Test
adf.test(train_data, alternative="stationary")

# KPSS Test
kpss.test(train_data)

# AR/MA Process Identification-------------
acf(train_data, main = "ACF for Time Series")
pacf(train_data, main = "PACF for Time Series")
```

The interpretation of the ADF and KPSS test results present an interesting conflict. The ADF test, with a statistic of -2.2495 and a p-value of 0.4727, does not provide enough evidence to confirm stationarity, while the KPSS test contradicts this with a statistic of 0.91496 and a p-value of 0.01, suggesting non-stationarity. These conflicting results are a common challenge in time series analysis, prompting a closer look at the data and potentially, the application of differencing or transformation methods to achieve stationarity, a key assumption in many forecasting models.


ACF and PACF Plot ‚Äî New Home Sold
The ACF chart for the US new home sales time series displays a gradual decline in autocorrelation, but the correlation remains positive across many lags, indicative of a trend that may need to be addressed by differencing in the ARIMA model. The PACF chart reveals a sharp drop after the first lag and insignificant correlations afterward, suggesting the autoregressive part of our model should likely include one lag (p=1). Given these observations, I would lean towards an ARIMA(1,0,0) model, where p is 1, and q is likely to be 0, indicating no moving average term is necessary. The seasonal pattern in the ACF may also guide us to include seasonal terms in the ARIMA model, adjusting the q-value accordingly to model the observed persistence in autocorrelation.

```{r}
#| eval: false

# ARIMA Model Building-------------
# Auto ARIMA model
auto_arima_model <- auto.arima(train_data)
summary(auto_arima_model)
checkresiduals(auto_arima_model)

# Manual ARIMA model
manual_arima_model <- arima(train_data, order=c(1,0,0))
summary(manual_arima_model)
checkresiduals(manual_arima_model)

# Forecasting and Model Evaluation-------------
# Auto ARIMA Forecast
forecast_auto <- forecast(auto_arima_model, h=forecast_length)
plot(forecast_auto)
lines(test_data, col='red')

# Manual ARIMA Forecast
forecast_manual <- forecast(manual_arima_model, h=forecast_length)
plot(forecast_manual)
lines(test_data, col='blue')

# Calculating RMSE for Auto ARIMA (testing data)
rmse_auto <- round(sqrt(mean((forecast_auto$mean - test_data)^2)), 3)
# RMSE_Testing: 67.538

# Calculating RMSE for Manual ARIMA (testing data)
rmse_manual <- round(sqrt(mean((forecast_manual$mean - test_data)^2)), 3)
# RMSE_Testing: 72.629
```

The auto.arimafunction in R automatically selected an ARIMA(1,1,2)(1,0,2)[12] model, integrating both non-seasonal and seasonal components. This model, with its mix of autoregressive and moving average parameters, is reflective of the complex patterns in housing market data we discussed earlier. By comparison, the manually specified ARIMA(1,0,0) model is simpler, capturing only the immediate past value‚Äôs influence without adjusting for seasonality or additional lagged effects. This auto ARIMA model outperforms the manual ARIMA with an AIC score of 7615 versus 7672 ‚Äî lower is typically better in model selection. RMSE values tell us a similar story, the auto model‚Äôs RMSE of 46.7 is preferable to the manual‚Äôs 48.5, indicating closer fits to the historical data. However, the Ljung-Box test flags potential autocorrelation in the residuals for both models, with p-values suggesting that even the sophisticated auto model may not fully account for all the data‚Äôs complexity. Another reason for further analysis.


Forecast manual ARIMA

Forecast auto ARIMA
Important Notes
In our analysis, we opted for a six-month forecasting window because ARIMA models generally excel in short-term forecasting but may not perform as well for long-term predictions. This limitation is due to the model‚Äôs inherent design, which is more attuned to capturing short-term trends and patterns.
The KPSS and ADF tests, commonly used to check for stationarity in time series data, have their limitations. They might not always detect weak non-stationarity. These tests operate under specific assumptions and might miss subtle trends or other non-stationary elements in the data. This limitation should be considered when interpreting test results and deciding on the appropriate modeling approach.
The auto.arima() function's methodology sometimes leads to unexpected differencing. It follows a heuristic approach, selecting ARIMA parameters through a stepwise process. This can result in the inclusion of a differencing term even when it might not be strictly necessary. For a more rigorous analysis (computationally expensive!), consider changing the default parameters of the functionauto.arima(diff_train_data, stepwise=FALSE, approximation=FALSE, max.p=5, max.d=0, max.q=5, max.P=2, max.D=2, max.Q=2). This method may be more adept at identifying weak non-stationarity that KPSS and ADF tests miss. However, it's important to be vigilant about overfitting, particularly when the model starts to focus more on the noise rather than the underlying trend in the data. To mitigate this, a manual approach to model specification is often beneficial.
ARIMA models are not a one-size-fits-all solution for time series analysis. They may not be the best choice for data with strong seasonal patterns or non-linear characteristics. In such cases, exploring extensions like Seasonal ARIMA (SARIMA) or integrating ARIMA with machine learning techniques can be more effective.
I have taken a simple train-test split at the forecasting period. I recommend using alternative resampling methods to improve the robustness of your model. Options like Rolling forecast evaluation, Time series bootstrapping, or Time series cross-validation can provide a more comprehensive understanding of the model‚Äôs performance and its applicability to different data scenarios. This approach helps in mitigating the risk of overfitting and enhances the model‚Äôs generalizability.





Prof. Frenzel
Written by Prof. Frenzel